# Sección 12: Desplegando Microservicios en Contenedores Docker

---

## [Creando archivo Dockerfile eficiente para el Config Server](https://github.com/magadiflo/spring-microservices-in-action-2021/blob/main/04.welcome-to-docker.md#creando-imagen-de-docker-eficiente)

En la raíz del `config-server` creamos un archivo sin extensión llamado `Dockerfile`. Este archivo nos permitirá crear
la imagen de nuestro servidor de configuraciones.

Si bien es posible convertir un `jar` de `Spring Boot` en una imagen de `Docker` con solo unas pocas líneas en el
`Dockerfile`, usaremos la `función de capas` para crear una imagen de `Docker optimizada`. El comando `extract` se puede
utilizar para dividir fácilmente la aplicación en capas que se agregarán al `dockerfile`.

Agregamos el archivo `Dockerfile` en la raíz de nuestro microservicio para crear la `imagen docker optimizada` que
utiliza un `jarmode`.

El siguiente `Dockerfile` es un archivo `multi-stage` de tres etapas(`dependencies`, `builder`, `runner`).

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

EXPOSE 8888
CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

**Dónde**

- En la primera etapa estamos usando la imagen base del `eclipse-temurin:21-jdk-alpine` de java, esto es importante
  porque en esta etapa vamos a descargar las dependencias y compilar el `jar`, por eso necesitamos que sea un `JDK`.


- En las dos últimas etapas usamos como imagen base `eclipse-temurin:21-jre-alpine`, es decir, únicamente el entorno
  para ejecutar una aplicación java, aquí ya no necesitamos el `JDK`, sino solo el `JRE`, además que es más liviano.


- `RUN sed -i -e 's/\r$//' ./mvnw`, es un comando de `Linux` que utiliza `sed`, el editor de flujo, para eliminar los
  caracteres
  de retorno de carro `(\r)` al final de cada línea en el archivo `mvnw`.


- `RUN ./mvnw dependency:go-offline`, iniciamos la descarga de las dependencias de maven.


- `RUN java -Djarmode=layertools -jar app.jar extract`:
    - `Spring Boot 2.3` introdujo un modo especial para manejar los archivos `jar`, llamado `Layertools`. Esto permite
      que el `jar` se estructure en capas lógicas como `dependencies`, `spring-boot-loader`, `snapshot-dependencies`,
      y `application`.
    - `El objetivo de estas capas es optimizar la reconstrucción de imágenes Docker`. Al tener una estructura de capas,
      si una parte del código cambia, solo se necesita reconstruir las capas afectadas en lugar de todo el `jar`. Esto
      puede resultar en una construcción de `imágenes Docker` más `rápida` y `eficiente`.
    - Este comando utiliza el modo de herramientas de capa (`layertools`) que permite ejecutar operaciones específicas
      relacionadas con el manejo de capas.
    - El comando `extract` extrae capas del `jar` para la creación de la imagen. Este comando descompone el archivo
      `jar` de la siguiente manera:
        - `Dependencies`: Las dependencias de tiempo de ejecución.
        - `Spring-Boot-Loader`: La parte responsable de iniciar una aplicación Spring Boot.
        - `Snapshot-Dependencies`: Dependencias etiquetadas como "snapshot".
        - `Application`: El código específico de la aplicación, como tus clases y recursos.


- `COPY --from=builder /app/dependencies ./` (y los otros `COPY` de las capas del `jar`), copiamos las distintas
  capas que han sido extraídas por el comando descrito anteriormente. Las copiamos en una nueva etapa del
  `Dockerfile`.


- `EXPOSE 8888`, el comando `EXPOSE` en un `Dockerfile` sirve para `documentar` que un contenedor escucha en un puerto
  específico. No abre ni publica el puerto hacia el exterior por sí mismo, simplemente declara que tu aplicación dentro
  del contenedor usará ese puerto. Es básicamente `una forma de documentar` qué puerto utiliza la aplicación dentro del
  contenedor. Le dice a otros desarrolladores (y a herramientas como Docker Compose) que la app escucha en ese puerto.


- `CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]`, hasta este punto no estamos trabajando con un
  `jar`, sino más bien con el desempaquetado del `jar`, eso significa que no podemos usar el clásico
  `CMD ["java","-jar","app.jar"]` para ejecutar la aplicación cuando inicie el contenedor. El `JarLauncher` es parte del
  sistema de carga de `Spring Boot`. Este lanzador permite cargar la aplicación desde capas separadas, esencialmente
  emulando el comportamiento de un `jar` ejecutable pero permitiendo el uso de capas para optimizar la reconstrucción
  del contenedor. En resumen, en esta última etapa del `Dockerfile` estamos copiando el desempaquetado del `jar` de la
  etapa `builder` al directorio `/app` de esta última etapa `runner`. Eso significa, que el contenedor final, tendrá en
  el directorio `/app` todos los directorios y archivos del desempaquetado del `jar` y para poder iniciar la aplicación
  utilizaremos el `JarLauncher` ubicado en el directorio `org.springframework.boot.loader.launch`. Más adelante, cuando
  creemos un contendor ingresaremos dentro de él para ver el contenido del directorio `/app`. Nos daremos cuenta que
  este directorio contendrá los directorios y archivos del desempaquetado del `jar`.

## Construyendo imagen Docker del Config Server

Mediante la línea de comandos nos posicionamos en la raíz del `config-server` donde tenemos nuestro `Dockerfile`. Luego,
escribimos el siguiente comando que nos permitirá generar la `imagen docker` de este microservicio.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\infrastructure\config-server (feature/section-12)
$ docker image build -t config-server . 
````

**Dónde**

- `docker image build`, comando de Docker para construir una imagen a partir de un `Dockerfile`.


- `-t`, es una bandera que nos permite definir un `tag name` *(nombre y versión)* para la imagen. En nuestro caso, el
  nombre que le daremos a la imagen será `config-server`. Como no especificamos un `tag` explícito, `Docker` usará por
  defecto el tag `latest`. Si quisiéramos agregar un tag personalizado, lo haríamos así: `config-server:mi-version-tag`.


- `.`, el punto al final de la instrucción indica que Docker debe buscar el archivo `Dockerfile` en el directorio
  actual. En este caso, el directorio actual es la raíz del proyecto `/config-server`, donde estamos ubicados en la
  terminal. Justo allí se encuentra nuestro `Dockerfile`, así que le estamos diciendo a `Docker` que use ese contexto
  para construir la imagen.

Una vez explicado cada parte del comando procedemos a ejecutarlo.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\infrastructure\config-server (feature/section-12)
$ docker image build -t config-server .                                                                                                                
[+] Building 51.1s (24/24) FINISHED                                                                                                                    
 => [internal] load build definition from Dockerfile                                                                                                   
 => => transferring dockerfile: 782B                                                                                                                   
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jre-alpine                                                                       
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jdk-alpine                                                                       
 => [auth] library/eclipse-temurin:pull token for registry-1.docker.io                                                                                 
 => [internal] load .dockerignore                                                                                                                      
 => => transferring context: 2B                                                                                                                        
 => [dependencies 1/9] FROM docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0    
 => => resolve docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0                 
 => [builder 1/4] FROM docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd         
 => => resolve docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd                 
 => => sha256:cda86626eeb372589c3378d030f4522ba1b0c78ec58b1db87960fa4e5fcd3e34 2.28kB / 2.28kB                                                         
 => => sha256:e6744199aa66ab985e37e72924f1568a6751afa2c508c42a1b3b945f3a8850a7 126B / 126B                                                             
 => => sha256:74f6a226ed936757680facf9b217f62a2af16b663a69df8e4b3ece925e27ed2a 53.06MB / 53.06MB                                                       
 => => sha256:f6cd406c8d97cafcb893e824126c17fa19907b2bbc8d759931089e1be1e75750 16.18MB / 16.18MB                                                       
 => => extracting sha256:f6cd406c8d97cafcb893e824126c17fa19907b2bbc8d759931089e1be1e75750                                                              
 => => extracting sha256:74f6a226ed936757680facf9b217f62a2af16b663a69df8e4b3ece925e27ed2a                                                              
 => => extracting sha256:e6744199aa66ab985e37e72924f1568a6751afa2c508c42a1b3b945f3a8850a7                                                              
 => => extracting sha256:cda86626eeb372589c3378d030f4522ba1b0c78ec58b1db87960fa4e5fcd3e34                                                              
 => [internal] load build context                                                                                                                      
 => => transferring context: 13.95kB                                                                                                                   
 => CACHED [dependencies 2/9] WORKDIR /app                                                                                                             
 => [dependencies 3/9] COPY ./mvnw ./                                                                                                                  
 => [dependencies 4/9] COPY ./.mvn ./.mvn                                                                                                              
 => [dependencies 5/9] COPY ./pom.xml ./                                                                                                               
 => [dependencies 6/9] RUN sed -i -e 's/\r$//' ./mvnw                                                                                                  
 => [dependencies 7/9] RUN ./mvnw dependency:go-offline                                                                                                
 => [builder 2/4] WORKDIR /app                                                                                                                         
 => [dependencies 8/9] COPY ./src ./src                                                                                                                
 => [dependencies 9/9] RUN ./mvnw clean package -DskipTests                                                                                            
 => [builder 3/4] COPY --from=dependencies /app/target/*.jar ./app.jar                                                                                 
 => [builder 4/4] RUN java -Djarmode=layertools -jar app.jar extract                                                                                   
 => [runner 3/6] COPY --from=builder /app/dependencies ./                                                                                              
 => [runner 4/6] COPY --from=builder /app/spring-boot-loader ./                                                                                        
 => [runner 5/6] COPY --from=builder /app/snapshot-dependencies ./                                                                                     
 => [runner 6/6] COPY --from=builder /app/application ./                                                                                               
 => exporting to image                                                                                                                                 
 => => exporting layers                                                                                                                                
 => => exporting manifest sha256:ac034572e56e413bb510510d6e5013ef06c3fdcb2451a8d4202c6e45155b78ff                                                      
 => => exporting config sha256:49e50361b329d89f16d543879e5529c9db482271c8aa708184ffa23780ebdb60                                                        
 => => exporting attestation manifest sha256:c14bc17e6104177e6937e649ed3485c948cfd3dfc3e5d614df8eb7a1d6bcc961                                          
 => => exporting manifest list sha256:4deeb21c52dd73d36cb71af8a8d844d91beaf67cd21facc95fdc17c5c4f32439                                                 
 => => naming to docker.io/library/config-server:latest                                                                                                
 => => unpacking to docker.io/library/config-server:latest                                                                                             
                                                                                                                                                       
View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/ievhbmkf37mzrg72bqzg54dzm                                             
````

Finalizada la construcción, listamos las imágenes para ver el que acabamos de crear.

````bash
$ docker image ls
REPOSITORY      TAG       IMAGE ID       CREATED          SIZE
config-server   latest    4deeb21c52dd   21 minutes ago   366MB
````

Observamos que la imagen tiene el tag `latest`. Esto ocurre porque al construir la imagen con la opción
`-t config-server` no especificamos un tag explícito (por ejemplo, `config-server:1.0.0`).


> `Docker`, por convención, asigna automáticamente el `tag latest` cuando no se indica uno.
> Es una forma de marcar la versión más reciente por defecto, pero no significa que sea necesariamente la más nueva o
> recomendada — eso depende de la gestión de versiones que nosotros hagamos.

## Creando red en Docker

Creamos una red personalizada que luego asociaremos a los contenedores que forman parte de nuestro ecosistema de
microservicios.

````bash
$ docker network create microservices-project-net
4320f82dad9c8d95cf4e05cc5f6664a51d338df8b5b2c7b63d22e24275c212a6
````

**Dónde**

- `docker network create`, permite crear una nueva red personalizada en `Docker`.
- `microservices-project-net`, es el nombre que le damos a nuestra red.

Al crear una red personalizada:

- `Docker` usará por defecto el `driver bridge`, que permite la comunicación entre contenedores dentro de la misma red.
- Los contenedores conectados a esta red podrán resolverse entre sí por nombre, lo que simplifica mucho la comunicación
  entre microservicios (por ejemplo, `config-server` podrá ser accedido desde `eureka-server` solo usando su nombre de
  contenedor).
- Esta práctica `evita depender` de direcciones `IP dinámicas` y mejora la escalabilidad del proyecto.

> Es una buena práctica usar redes personalizadas en proyectos con múltiples contenedores, como una arquitectura de
> microservicios, ya que te da mayor control, aislamiento y facilidad para escalar.

Si listamos las redes, podemos verificar que está la que acabamos de crear.

````bash
$ docker network ls
NETWORK ID     NAME                        DRIVER    SCOPE
16b8d6f5da55   bridge                      bridge    local
8c77b18a87f1   host                        host      local
4320f82dad9c   microservices-project-net   bridge    local
fe251eccddcc   none                        null      local
````

Como se muestra en el resultado, `Docker` nos lista todas las redes disponibles actualmente:

- `bridge`, red predeterminada que `Docker` crea para permitir la comunicación entre contenedores si no se especifica
  una red personalizada.
- `host`, red especial donde el contenedor comparte directamente el stack de red del host (solo disponible en Linux).
- `none`, aisla completamente la red del contenedor (sin conectividad).
- `microservices-project-net`, es la red personalizada que acabamos de crear, usando el driver bridge.

Al crear redes personalizadas como `microservices-project-net`, ganamos ventajas clave como:

- Resolución de nombres entre contenedores.
- Mejor organización de servicios.
- Mayor seguridad y aislamiento del tráfico de red.

## Levantando contenedor a partir de la imagen Docker del Config Server

Una vez construida la `imagen Docker` del `Config Server`, podemos crear y ejecutar un contenedor a partir de ella
utilizando el siguiente comando.

````bash
$ docker container run -d -p 8888:8888 --name c-config-server --network microservices-project-net config-server
a59c00698b3fb1d3638c32a182e0110b50d3cb9deb1fc2252fc0d89a39031a3f
````

**Dónde**

- `docker container run`, comando para crear y ejecutar un contenedor a partir de una imagen.
- `-d`, ejecuta el contenedor en modo `detached` (segundo plano), permitiéndonos seguir usando la terminal.
- `-p 8888:8888`, especifica el mapeo de puertos en el formato HOST_PORT:CONTAINER_PORT. En este caso:
    - `8888 (izquierda)` es el puerto expuesto en nuestra máquina local.
    - `8888 (derecha)` es el puerto interno del contenedor en el que se ejecuta la aplicación Spring Boot.
- `--name c-config-server`, asigna un nombre identificador al contenedor.
- `--network microservices-project-net`, conecta el contenedor a la red personalizada que creamos anteriormente, lo que
  permite que se comunique fácilmente con otros contenedores de la misma red.
- `config-server`, es el nombre de la imagen `Docker` desde la cual se va a crear el contenedor.
    - Como no se especificó una versión (`tag`), Docker usará por defecto la etiqueta `latest`.
    - Si se desea usar una versión específica, se puede indicar así `config-server:1.0.0`.

Si ahora listamos los contenedores veremos que el contenedor `c-config-server` se está ejecutando correctamente, como lo
indica el estado `Up`. El mapeo de puertos `0.0.0.0:8888->8888/tcp` significa que cualquier solicitud hecha al puerto
`8888` de la máquina local será redirigida al puerto 8888 del contenedor. Esto nos permitirá acceder al `Config Server`
desde el navegador o cualquier cliente HTTP usando http://localhost:8888.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS          PORTS                    NAMES
a59c00698b3f   config-server   "/__cacert_entrypoin…"   23 seconds ago   Up 21 seconds   0.0.0.0:8888->8888/tcp   c-config-server
````

## Verificando que el contenedor está en la red docker

Después de levantar el contenedor, podemos `inspeccionar la red` que creamos para asegurarnos de que el contenedor está
efectivamente conectado a ella.

````bash
$ docker network inspect microservices-project-net
[
    {
        "Name": "microservices-project-net",
        "Id": "4320f82dad9c8d95cf4e05cc5f6664a51d338df8b5b2c7b63d22e24275c212a6",
        "Created": "2025-04-25T18:05:20.033232455Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv4": true,
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "172.18.0.0/16",
                    "Gateway": "172.18.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "a59c00698b3fb1d3638c32a182e0110b50d3cb9deb1fc2252fc0d89a39031a3f": {
                "Name": "c-config-server",
                "EndpointID": "fa81b04a669bb2440a5c71d275afcd5d5a71cc84fdf22c01ffc2c695e4bb43e8",
                "MacAddress": "c2:3b:21:ff:cb:18",
                "IPv4Address": "172.18.0.2/16",
                "IPv6Address": ""
            }
        },
        "Options": {},
        "Labels": {}
    }
]
````

En la salida del comando, dentro del atributo `Containers`, podemos ver que nuestro contenedor `c-config-server` está
registrado. Aparecen datos como:

- `Name: c-config-server` (el nombre que le dimos al contenedor).
- `IPv4Address: 172.18.0.2/16` (la IP asignada dentro de esta red).
- `MacAddress`, dirección física virtual asignada.

Este paso es útil para:

- Validar la conectividad entre contenedores cuando trabajamos en un entorno de microservicios.
- Usar nombres de contenedores como `hostnames` al momento de comunicar servicios entre sí dentro de la misma red
  Docker.

También podríamos `inspeccionar el contenedor` y verificar que la red a la que estamos conectados sea el que
definimos en la creación.

````bash
$ docker container inspect c-config-server
[
    {
        "HostConfig": {
            "NetworkMode": "microservices-project-net",
            ...
        },
        "NetworkSettings": {
            ...
            "Networks": {
                "microservices-project-net": {...}
            }
        },
        ...
    }
]
````

Como vemos el contenedor `c-config-server` está conectado a la red `microservices-project-net`, como se puede ver en
el resultado anterior.

## Logs del Config Server al iniciar el contenedor

Al iniciar el `contenedor Docker` del `Config Server`, es útil observar los `logs` para verificar que la aplicación se
ha levantado correctamente y que está leyendo los archivos de configuración como se espera.

Entonces, ejecutamos el siguiente comando para ver los logs del contenedor:

````bash
$ docker container logs c-config-server

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

2025-04-25T20:35:38.503Z  INFO 1 --- [config-server] [           main] d.m.c.s.app.ConfigServerApplication      : Starting ConfigServerApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app)
2025-04-25T20:35:38.506Z  INFO 1 --- [config-server] [           main] d.m.c.s.app.ConfigServerApplication      : No active profile set, falling back to 1 default profile: "default"
2025-04-25T20:35:39.888Z  INFO 1 --- [config-server] [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=209bd33f-ee6e-361d-bc59-b6fdc05076d4
2025-04-25T20:35:40.318Z  INFO 1 --- [config-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8888 (http)
2025-04-25T20:35:40.338Z  INFO 1 --- [config-server] [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-04-25T20:35:40.339Z  INFO 1 --- [config-server] [           main] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
2025-04-25T20:35:40.385Z  INFO 1 --- [config-server] [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-04-25T20:35:40.387Z  INFO 1 --- [config-server] [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1804 ms
2025-04-25T20:35:41.806Z  INFO 1 --- [config-server] [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint beneath base path '/actuator'
2025-04-25T20:35:41.891Z  INFO 1 --- [config-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8888 (http) with context path '/'
2025-04-25T20:35:41.913Z  INFO 1 --- [config-server] [           main] d.m.c.s.app.ConfigServerApplication      : Started ConfigServerApplication in 4.054 seconds (process running for 5.082)
2025-04-25T20:39:52.480Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-04-25T20:39:52.480Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-04-25T20:39:52.483Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 3 ms
2025-04-25T20:39:58.252Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T20:40:03.094Z  INFO 1 --- [config-server] [nio-8888-exec-3] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T20:40:40.633Z  INFO 1 --- [config-server] [nio-8888-exec-5] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service-dev.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T20:40:40.633Z  INFO 1 --- [config-server] [nio-8888-exec-5] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T21:34:38.249Z  INFO 1 --- [config-server] [nio-8888-exec-7] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service-prod.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T21:34:38.249Z  INFO 1 --- [config-server] [nio-8888-exec-7] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
````

Los `logs` del contenedor del `Config Server` confirman que la aplicación se ha iniciado correctamente usando `Java 21`,
desplegada en un contenedor con servidor embebido `Tomcat` escuchando en el puerto `8888`. Además, se evidencia que el
servidor ha cargado exitosamente los archivos de configuración (`item-service.yml`, `item-service-dev.yml` y
`item-service-prod.yml`) a partir de peticiones http realizadas para consultar estos archivos.
Esto indica que el `Config Server` está funcionando correctamente.

## Verificando el funcionamiento del Config Server

Podemos probar que el `Config Server` está corriendo correctamente y que tiene acceso al repositorio remoto de
configuración realizando peticiones http. En el siguiente ejemplo consultaremos por el archivo o perfil `dev`.

````bash
$ curl -v http://localhost:8888/item-service/dev | jq
>
< HTTP/1.1 200
< Content-Type: application/json
< Transfer-Encoding: chunked
< Date: Fri, 25 Apr 2025 20:40:40 GMT
<
{
  "name": "item-service",
  "profiles": [
    "dev"
  ],
  "label": null,
  "version": "6d1fd0fc178a112daa21f5705cd28023c55470f3",
  "state": "",
  "propertySources": [
    {
      "name": "https://github.com/magadiflo/config-server-repo.git/item-service-dev.yml",
      "source": {
        "custom.text": "item-service-dev.yml en config server con perfil dev (desde GitHub)",
        "custom.author.name": "Martín (desde GitHub)",
        "custom.author.email": "magadiflo@gmail.com (desde GitHub)"
      }
    },
    {
      "name": "https://github.com/magadiflo/config-server-repo.git/item-service.yml",
      "source": {
        "server.port": 8085,
        "custom.text": "item-service.yml en config server con perfil por defecto"
      }
    }
  ]
}
````

La petición anterior solicita el archivo de configuración para el servicio llamado `item-service` con el perfil `dev`.
Si todo está funcionando, obtendremos una respuesta `JSON` con las propiedades definidas en los archivos
`item-service.yml` y `item-service-dev.yml` del repositorio remoto, tal como se muestra en el resultado anterior.

## Renombrando una Imagen en Docker

En `Docker`, podemos `renombrar` o `etiquetar` imágenes existentes utilizando el comando `docker image tag`. Este
proceso es útil cuando queremos cambiar el nombre de una imagen o asignarle una nueva versión. A continuación, mostramos
cómo hacerlo:

1. `Listamos las imágenes actuales`. Primero, listamos las imágenes disponibles en la plataforma Docker con el siguiente
   comando.
    ````bash
    $ docker image ls
    REPOSITORY      TAG       IMAGE ID       CREATED       SIZE
    config-server   latest    4deeb21c52dd   4 hours ago   366MB
    ````
   Aquí, podemos ver que tenemos una imagen llamada `config-server` con la etiqueta `latest`.


3. `Renombrando la imagen`. Podemos renombrar la imagen utilizando el comando `docker image tag`. Existen dos maneras
   principales de hacerlo:
    - *Asignar un nombre y etiqueta diferente*
    ````bash
    $ docker image tag config-server:latest servidor-configuraciones:v1
    ````
   En este ejemplo, renombramos la imagen `config-server` con la etiqueta `latest` a `servidor-configuraciones` con la
   etiqueta `v1`.<br><br>

    - *Cambiar la versión de la imagen*
    ````bash
    $ docker image tag config-server:latest config-server:1.0.0
    ````
   Aquí, simplemente cambiamos la versión de `latest` a `1.0.0`.


3. `Verificando las Imágenes Renombradas`. Después de realizar los cambios, podemos listar nuevamente las imágenes para
   verificar que la operación se haya realizado correctamente.
    ````bash
    $ docker image ls
    REPOSITORY                 TAG       IMAGE ID       CREATED       SIZE
    config-server              1.0.0     4deeb21c52dd   4 hours ago   366MB
    config-server              latest    4deeb21c52dd   4 hours ago   366MB
    servidor-configuraciones   v1        4deeb21c52dd   4 hours ago   366MB
    ````

Como se observa, las tres imágenes (`config-server:1.0.0`, `config-server:latest`, y `servidor-configuraciones:v1`)
comparten el mismo `IMAGE ID`, ya que todas son basadas en la misma imagen base y no han sufrido modificaciones en su
contenido, solo han cambiado su nombre o versión.

## Eliminando imagen

Primero listamos las imágenes actuales.

````bash
$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED       SIZE
config-server              1.0.0     4deeb21c52dd   4 hours ago   366MB
config-server              latest    4deeb21c52dd   4 hours ago   366MB
servidor-configuraciones   v1        4deeb21c52dd   4 hours ago   366MB
````

Procedemos a eliminar la imagen `servidor-configuraciones` con tag `v1` utilizando el siguiente comando.

````bash
$ docker image rm servidor-configuraciones:v1
Untagged: servidor-configuraciones:v1
````

Volvemos a listar las imágenes y verificamos que la imagen fue eliminada.

````bash
$ docker image ls
REPOSITORY      TAG       IMAGE ID       CREATED       SIZE
config-server   1.0.0     4deeb21c52dd   4 hours ago   366MB
config-server   latest    4deeb21c52dd   4 hours ago   366MB
````

## Eliminando contenedor

Primero listamos los contenedores actuales.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE           COMMAND                  CREATED       STATUS       PORTS                    NAMES
a59c00698b3f   config-server   "/__cacert_entrypoin…"   2 hours ago   Up 2 hours   0.0.0.0:8888->8888/tcp   c-config-server
````

Como el contenedor se encuentra en ejecución (`Up`) primero debemos detenerlo.

````bash
$ docker container stop c-config-server
c-config-server
````

Ahora sí procedemos a eliminar el contenedor utilizando el siguiente comando.

````bash
$ docker container rm c-config-server
c-config-server
````

Si listamos nuevamente los contenedores veremos que la eliminación ha sido exitosa.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
````

## Explorando el contenido del contenedor del Config Server

Primero, listamos los contenedores que se encuentran en ejecución (o detenidos) en nuestra plataforma Docker. Esto nos
permite identificar el nombre del contenedor que deseamos inspeccionar.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS         PORTS                    NAMES
683472a028ac   config-server   "/__cacert_entrypoin…"   11 minutes ago   Up 5 minutes   0.0.0.0:8888->8888/tcp   c-config-server
````

En este caso, el contenedor que nos interesa tiene como nombre `c-config-server`.

A continuación, accedemos de forma interactiva al contenedor utilizando una `shell` mínima (`/bin/sh`).
Una vez dentro del contenedor, nos ubicamos en la ruta `/app`, donde se encuentra desplegada la aplicación. Listamos el
contenido del directorio para verificar los archivos y carpetas disponibles.

````bash
$ docker container exec -it c-config-server /bin/sh
/app # ls -l
total 12
drwxr-xr-x 1 root root 4096 Apr 25 17:35 BOOT-INF
drwxr-xr-x 4 root root 4096 Apr 25 17:35 META-INF
drwxr-xr-x 3 root root 4096 Apr 25 17:35 org
/app #
````

Estos directorios (`BOOT-INF`, `META-INF`, `org`) son característicos de una aplicación `Spring Boot` empacada como
ejecutable `.jar`, lo cual confirma que la aplicación se está ejecutando correctamente dentro del contenedor.

Para aclarar un poco sobre los directorios `BOOT-INF`, `META-INF` y `org`, estos directorios están presentes porque la
imagen `Docker` se ha construido usando el enfoque de capas de `Spring Boot` mediante `layertools`. Este enfoque permite
extraer y copiar de manera explícita las capas internas del `jar`, lo cual favorece la eficiencia en la construcción y
el cacheo de capas.

## Dockerfile para Eureka Server: Construcción de imagen y despliegue del contenedor

En la raíz del `discovery-server` creamos un archivo sin extensión llamado `Dockerfile`. Este archivo contendrá los
mismos comandos que usamos anteriormente en el `Dockerfile` del config server.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

EXPOSE 8761
CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

Mediante la línea de comandos nos posicionamos en la raíz del `discovery-server` donde tenemos nuestro `Dockerfile`.
Luego, ejecutamos el siguiente comando que nos permitirá generar la imagen docker de este microservicio.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\infrastructure\discovery-server (feature/section-12)
$ docker image build -t discovery-server .                                                                                                                
[+] Building 43.2s (24/24) FINISHED                                                                                                                       
 => [internal] load build definition from Dockerfile                                                                                                      
 => => transferring dockerfile: 782B                                                                                                                      
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jre-alpine                                                                          
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jdk-alpine                                                                          
 => [auth] library/eclipse-temurin:pull token for registry-1.docker.io                                                                                    
 => [internal] load .dockerignore                                                                                                                         
 => => transferring context: 2B                                                                                                                           
 => [builder 1/4] FROM docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd            
 => => resolve docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd                    
 => [dependencies 1/9] FROM docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0       
 => => resolve docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0                    
 => [internal] load build context                                                                                                                         
 => => transferring context: 15.89kB                                                                                                                      
 => CACHED [dependencies 2/9] WORKDIR /app                                                                                                                
 => CACHED [dependencies 3/9] COPY ./mvnw ./                                                                                                              
 => CACHED [dependencies 4/9] COPY ./.mvn ./.mvn                                                                                                          
 => [dependencies 5/9] COPY ./pom.xml ./                                                                                                                  
 => [dependencies 6/9] RUN sed -i -e 's/\r$//' ./mvnw                                                                                                     
 => [dependencies 7/9] RUN ./mvnw dependency:go-offline                                                                                                   
 => [dependencies 8/9] COPY ./src ./src                                                                                                                   
 => [dependencies 9/9] RUN ./mvnw clean package -DskipTests                                                                                               
 => CACHED [builder 2/4] WORKDIR /app                                                                                                                     
 => [builder 3/4] COPY --from=dependencies /app/target/*.jar ./app.jar                                                                                    
 => [builder 4/4] RUN java -Djarmode=layertools -jar app.jar extract                                                                                      
 => [runner 3/6] COPY --from=builder /app/dependencies ./                                                                                                 
 => [runner 4/6] COPY --from=builder /app/spring-boot-loader ./                                                                                           
 => [runner 5/6] COPY --from=builder /app/snapshot-dependencies ./                                                                                        
 => [runner 6/6] COPY --from=builder /app/application ./                                                                                                  
 => exporting to image                                                                                                                                    
 => => exporting layers                                                                                                                                   
 => => exporting manifest sha256:116aac5e68b4c476a72c57476d0a5d2a09314efd6640d2f564818f396b251ab1                                                         
 => => exporting config sha256:8874e0918b17917cb62ac0e1424516e04f1a5be46adbc3a8754f82cad7c10a4d                                                           
 => => exporting attestation manifest sha256:2e565cb5e43b32e8ab95ba5da4dd641ac8f4eba735568ef1bdd00453c152be3e                                             
 => => exporting manifest list sha256:90c55dd49e1283370a196a216b9d3111a0f90cc3919fad01c042d81f297842e0                                                    
 => => naming to docker.io/library/discovery-server:latest                                                                                                
 => => unpacking to docker.io/library/discovery-server:latest                                                                                             
                                                                                                                                                          
View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/3l7y2msljhy5kfbheb2zokjcq                                                
````

Finalizada la construcción, listamos las imágenes para ver el que acabamos de crear.

````bash
$ docker image ls
REPOSITORY         TAG       IMAGE ID       CREATED              SIZE
discovery-server   latest    90c55dd49e12   About a minute ago   391MB
config-server      latest    4deeb21c52dd   5 hours ago          366MB
````

## Levantando contenedor a partir de la imagen Docker del Discovery Server

Una vez construida la imagen Docker del `Discovery Server`, podemos crear y ejecutar un contenedor a partir de ella
utilizando el siguiente comando.

````bash
$ docker container run -d -p 8761:8761 --name c-discovery-server --network microservices-project-net discovery-server
efdda60f4dbb938dfc553c0fa192a5ef21fb5fa94ca1e010ee95a4725eec6eb6
````

**Importante**
> Como anteriormente hemos creado una red personalizada llamada `microservices-project-net` y la hemos asignado al
> contenedor `c-config-server`, es importante que a los nuevos contenedores también le asignemos esa misma red para que
> puedan comunicarse entre sí.

Si ahora listamos los contenedores podremos ver el que acabamos de crear y levantar.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS          PORTS                    NAMES
efdda60f4dbb   discovery-server   "/__cacert_entrypoin…"   4 minutes ago    Up 4 minutes    0.0.0.0:8761->8761/tcp   c-discovery-server
683472a028ac   config-server      "/__cacert_entrypoin…"   49 minutes ago   Up 44 minutes   0.0.0.0:8888->8888/tcp   c-config-server
````

Si hacemos un logs de contenedor `c-discovery-server` veremos que se está ejecutando correctamente.

````bash
docker container logs c-discovery-server

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

2025-04-25T22:58:39.927Z  INFO 1 --- [discovery-server] [           main] d.m.d.s.app.DiscoveryServerApplication   : Starting DiscoveryServerApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app)
2025-04-25T22:58:39.930Z  INFO 1 --- [discovery-server] [           main] d.m.d.s.app.DiscoveryServerApplication   : No active profile set, falling back to 1 default profile: "default"
2025-04-25T22:58:41.192Z  INFO 1 --- [discovery-server] [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=0111aded-c619-3f67-ab04-f0283131fe86
2025-04-25T22:58:41.590Z  INFO 1 --- [discovery-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8761 (http)
2025-04-25T22:58:41.607Z  INFO 1 --- [discovery-server] [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-04-25T22:58:41.608Z  INFO 1 --- [discovery-server] [           main] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
2025-04-25T22:58:41.655Z  INFO 1 --- [discovery-server] [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-04-25T22:58:41.656Z  INFO 1 --- [discovery-server] [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1661 ms
2025-04-25T22:58:42.740Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON encoding codec LegacyJacksonJson
2025-04-25T22:58:42.742Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON decoding codec LegacyJacksonJson
2025-04-25T22:58:42.938Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML encoding codec XStreamXml
2025-04-25T22:58:42.938Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML decoding codec XStreamXml
2025-04-25T22:58:43.112Z  INFO 1 --- [discovery-server] [           main] o.s.v.b.OptionalValidatorFactoryBean     : Failed to set up a Bean Validation provider: jakarta.validation.NoProviderFoundException: Unable to create a Configuration, because no Jakarta Bean Validation provider could be found. Add a provider like Hibernate Validator (RI) to your classpath.
2025-04-25T22:58:43.920Z  WARN 1 --- [discovery-server] [           main] iguration$LoadBalancerCaffeineWarnLogger : Spring Cloud LoadBalancer is currently working with the default cache. While this cache implementation is useful for development and tests, it's recommended to use Caffeine cache in production.You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
2025-04-25T22:58:43.948Z  INFO 1 --- [discovery-server] [           main] o.s.c.n.eureka.InstanceInfoFactory       : Setting initial instance status as: STARTING
2025-04-25T22:58:43.975Z  INFO 1 --- [discovery-server] [           main] com.netflix.discovery.DiscoveryClient    : Initializing Eureka in region us-east-1
2025-04-25T22:58:43.975Z  INFO 1 --- [discovery-server] [           main] com.netflix.discovery.DiscoveryClient    : Client configured to neither register nor query for data.
2025-04-25T22:58:43.977Z  INFO 1 --- [discovery-server] [           main] com.netflix.discovery.DiscoveryClient    : Discovery Client initialized at timestamp 1745621923977 with initial instances count: 0
2025-04-25T22:58:44.031Z  INFO 1 --- [discovery-server] [           main] c.n.eureka.DefaultEurekaServerContext    : Initializing ...
2025-04-25T22:58:44.033Z  WARN 1 --- [discovery-server] [           main] c.n.eureka.cluster.PeerEurekaNodes       : The replica size seems to be empty. Check the route 53 DNS Registry
2025-04-25T22:58:44.046Z  INFO 1 --- [discovery-server] [           main] c.n.e.registry.AbstractInstanceRegistry  : Finished initializing remote region registries. All known remote regions: []
2025-04-25T22:58:44.046Z  INFO 1 --- [discovery-server] [           main] c.n.eureka.DefaultEurekaServerContext    : Initialized
2025-04-25T22:58:44.067Z  INFO 1 --- [discovery-server] [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint beneath base path '/actuator'
2025-04-25T22:58:44.142Z  INFO 1 --- [discovery-server] [           main] o.s.c.n.e.s.EurekaServiceRegistry        : Registering application DISCOVERY-SERVER with eureka with status UP
2025-04-25T22:58:44.157Z  INFO 1 --- [discovery-server] [       Thread-9] o.s.c.n.e.server.EurekaServerBootstrap   : isAws returned false
2025-04-25T22:58:44.158Z  INFO 1 --- [discovery-server] [       Thread-9] o.s.c.n.e.server.EurekaServerBootstrap   : Initialized server context
2025-04-25T22:58:44.159Z  INFO 1 --- [discovery-server] [       Thread-9] c.n.e.r.PeerAwareInstanceRegistryImpl    : Got 1 instances from neighboring DS node
2025-04-25T22:58:44.159Z  INFO 1 --- [discovery-server] [       Thread-9] c.n.e.r.PeerAwareInstanceRegistryImpl    : Renew threshold is: 1
2025-04-25T22:58:44.159Z  INFO 1 --- [discovery-server] [       Thread-9] c.n.e.r.PeerAwareInstanceRegistryImpl    : Changing status to UP
2025-04-25T22:58:44.163Z  INFO 1 --- [discovery-server] [       Thread-9] e.s.EurekaServerInitializerConfiguration : Started Eureka Server
2025-04-25T22:58:44.187Z  INFO 1 --- [discovery-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8761 (http) with context path '/'
2025-04-25T22:58:44.188Z  INFO 1 --- [discovery-server] [           main] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 8761
2025-04-25T22:58:44.210Z  INFO 1 --- [discovery-server] [           main] d.m.d.s.app.DiscoveryServerApplication   : Started DiscoveryServerApplication in 4.808 seconds (process running for 5.492)
2025-04-25T22:59:44.160Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 0ms
2025-04-25T23:00:44.182Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1404ms
2025-04-25T23:01:44.182Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1520ms
2025-04-25T23:02:44.182Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1508ms
2025-04-25T23:03:44.186Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1383ms
````

Si ahora accedemos desde nuestro navegador hacia la dirección `http://localhost:8761/` veremos que nuestro servidor de
`eureka` se ha creado correctamente y está en ejecución.

![01.png](assets/section-12/01.png)

## Descargando imagen Docker para MySQL y PostgreSQL

Para descargar las imágenes `Docker` nos vamos a la página de [hub.docker](https://hub.docker.com/) donde se encuentran
las imágenes oficiales de los distintos productos de software.

En la imagen siguiente se muestra cómo es que buscamos la imagen de `MySQL` y el tag `8.0.41`. Una vez que encontremos
la versión que necesitamos copiamos el comando de descarga para tener la imagen en nuestro local.

![02.png](assets/section-12/02.png)

Utilizando el comando `docker pull` podemos descargar la imagen desde `docker hub`, tal como se muestra a continuación.

````bash
$ docker pull mysql:8.0.41-debian
8.0.41-debian: Pulling from library/mysql
80e697a2a38a: Pull complete
e2d15768547d: Pull complete
8a628cdd7ccc: Pull complete
3d80c213d73c: Pull complete
cdc6bb1849f7: Pull complete
fcc101123521: Pull complete
eccb44e37d22: Pull complete
dd34483b65e5: Pull complete
5bac1e6ff6c3: Pull complete
156e4bd92e5c: Pull complete
51f55966c0b7: Pull complete
f50c587f3059: Pull complete
Digest: sha256:b2252987e0ecdb820e96928948ac3bca1adcd2b4a2a2c7b0d7ea78f77a9dc6ac
Status: Downloaded newer image for mysql:8.0.41-debian
docker.io/library/mysql:8.0.41-debian
````

**Dónde**

- `docker pull`, comando utilizado para descargar una imagen desde un repositorio remoto (por defecto, `Docker Hub`).
- `mysql`, nombre de la `imagen` que se desea descargar.
- `8.0.41-debian`, `tag` que especifica la versión de la imagen, en este caso `MySQL 8.0.41` basada en `Debian`.

Haremos lo mismo para descargar la imagen de `PostgreSQL`.

````bash
$ docker pull postgres:17-alpine
17-alpine: Pulling from library/postgres
8f4971a5dfe7: Pull complete
215ba3ecdc26: Pull complete
d8d8fb695a5a: Pull complete
83efd74bc97e: Pull complete
4f18957d9158: Pull complete
c24c1ba610df: Pull complete
15ca4c67ed92: Pull complete
9e4acb9ca7d3: Pull complete
404c53e09e31: Pull complete
Digest: sha256:7062a2109c4b51f3c792c7ea01e83ed12ef9a980886e3b3d380a7d2e5f6ce3f5
Status: Downloaded newer image for postgres:17-alpine
docker.io/library/postgres:17-alpine
````

Finalizada las descargas, listamos las imágenes para ver que la imagen de `MySQL` y la de `PostgreSQL` fueron
descargadas correctamente.

````bash
$ docker image ls
REPOSITORY         TAG             IMAGE ID       CREATED        SIZE
discovery-server   latest          90c55dd49e12   2 days ago     391MB
config-server      latest          4deeb21c52dd   2 days ago     366MB
postgres           17-alpine       7062a2109c4b   2 months ago   398MB
mysql              8.0.41-debian   b2252987e0ec   3 months ago   812MB
````

**Nota**
> **No es estrictamente necesario descargar primero la imagen mediante `docker pull`.**
> Si se conoce el `nombre de la imagen` y el `tag`, se puede ejecutar directamente el comando `docker container run` o
> `docker run` para crear un contenedor. Si la imagen no se encuentra disponible en el entorno local, `Docker`
> automáticamente la descargará antes de crear el contenedor.

## Creando contenedor Docker de MySQL

Primero creamos el contenedor para `MySQL` ejecutando el siguiente comando.

````bash
$ docker container run -d -p 3307:3306 --name c-mysql -e MYSQL_DATABASE=db_product_service -e MYSQL_ROOT_PASSWORD=magadiflo --network microservices-project-net mysql:8.0.41-debian
a60e121908c8f16bf1544e1d5fbc4a3391a461d34abe663ef421a8f0a4766d37
````

**Donde**

- `docker container run`, comando utilizado para crear y ejecutar un nuevo contenedor a partir de una imagen.
- `-d`, ejecuta el contenedor en segundo plano (modo `detached`).
- `-p 3307:3306`, mapea el puerto `3306` del contenedor (`MySQL`) al puerto `3307` del host `(pc local)`.
- `--name c-mysql`, asigna el nombre `c-mysql` al contenedor para su identificación.
- `-e MYSQL_DATABASE=db_product_service`, define una variable de entorno para que al iniciar, el contenedor cree
  automáticamente una base de datos llamada `db_product_service`.
- `-e MYSQL_ROOT_PASSWORD=magadiflo`, define la contraseña del usuario `root` de `MySQL` dentro del contenedor.
- `--network microservices-project-net`, conecta el contenedor a una red personalizada de `Docker` llamada
  `microservices-project-net`, útil para la comunicación entre microservicios.

**Nota**

- `Creación automática de la imagen si no está en local`, si la imagen `mysql:8.0.41-debian` no está descargada en el
  entorno local, `Docker` la descargará automáticamente antes de crear el contenedor.


- `Importante sobre puertos`, se definió el puerto `3307` como puerto externo en el host `(pc local)` debido a que ya
  existe una instalación de `MySQL` localmente. Para evitar conflictos de puertos, se asignó un puerto diferente y
  disponible al contenedor. Con respecto al puerto del lado derecho de la instrucción `-p` (`3306`), corresponde al
  puerto interno del contenedor, que es el puerto por defecto donde el servicio de `MySQL` escucha
  `dentro del contenedor`.

Si listamos los contenedores vemos que el que acabamos de crear está levantado.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS                        PORTS                               NAMES
a60e121908c8   mysql:8.0.41-debian   "docker-entrypoint.s…"   26 minutes ago   Up 26 minutes                 33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server      "/__cacert_entrypoin…"   2 days ago       Exited (255) 57 minutes ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server         "/__cacert_entrypoin…"   2 days ago       Exited (255) 57 minutes ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Para verificar su funcionamiento, podemos ingresar dentro del contenedor de `MySQL` ejecutando el siguiente comando.

````bash
$ docker container exec -it c-mysql /bin/sh
# mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.41 MySQL Community Server - GPL

Copyright (c) 2000, 2025, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| db_product_service |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)

mysql>
````

**Dónde**

- `docker container exec`, comando que permite ejecutar una instrucción dentro de un contenedor en ejecución.
- `-i`, modo interactivo. Mantiene la sesión abierta para recibir entradas del usuario.
- `-t`, asocia un pseudo-TTY, proporcionando un terminal interactivo dentro del contenedor.
- `c-mysql`, nombre del contenedor donde se desea ejecutar el comando.
- `/bin/sh`, especifica el programa a ejecutar dentro del contenedor. En este caso, se abre una shell (sh) básica para
  interactuar con el sistema de archivos del contenedor.

Ahora, para acceder a la consola de `MySQL` desde ahí, debemos ejecutar este comando dentro del contenedor.

````bash
$ mysql -u root -p
````

**Dónde**

- `mysql`, ejecuta el cliente de línea de comandos de `MySQL`.
- `-u root`, indica que quieres conectarte como el usuario `root`.
- `-p`, indica que te pedirá la contraseña.

Como hemos configurado el puerto `3307` en nuestro host (`pc local`), podemos conectarnos a la base de datos del
contenedor utilizando herramientas como `DBeaver`.

![03.png](assets/section-12/03.png)

## Creando contenedor Docker de Postgres

Creamos el contenedor para `Postgres` ejecutando el siguiente comando.

````bash
$ docker container run -d -p 5433:5432 --name c-postgres -e POSTGRES_DB=db_user_service -e POSTGRES_PASSWORD=magadiflo --network microservices-project-net postgres:17-alpine
d69f3ce25cfd8e8e3d95273fb39716d99b28cb6b0c985da6b680f3e5fe32aa04
````

**Dónde**

- `docker container run`, crea y ejecuta un nuevo contenedor.
- `-d`, modo `detached`. El contenedor se ejecuta en segundo plano.
- `-p 5433:5432`.
    - `5433`, puerto expuesto en el host (`pc local`).
    - `5432`, puerto `interno del contenedor` (puerto por defecto de `PostgreSQL`).
- `--name c-postgres`, nombre asignado al contenedor (`c-postgres`).
- `-e POSTGRES_DB=db_user_service`, variable de entorno para crear automáticamente la base de datos `db_user_service` al
  iniciar `PostgreSQL`.
- `-e POSTGRES_PASSWORD=magadiflo`, variable de entorno para establecer la contraseña del usuario `postgres`.
- `--network microservices-project-net`, conecta el contenedor a una red `Docker` personalizada llamada
  `microservices-project-net`.
- `postgres:17-alpine`, imagen utilizada. Es la `versión 17` de `PostgreSQL` basada en alpine (una imagen ligera).

Si listamos los contenedores vemos que el que acabamos de crear está levantado.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                 COMMAND                  CREATED             STATUS                     PORTS                               NAMES
d69f3ce25cfd   postgres:17-alpine    "docker-entrypoint.s…"   4 minutes ago       Up 4 minutes               0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian   "docker-entrypoint.s…"   About an hour ago   Up About an hour           33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server      "/__cacert_entrypoin…"   2 days ago          Exited (255) 2 hours ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server         "/__cacert_entrypoin…"   2 days ago          Exited (255) 2 hours ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Para verificar su funcionamiento, podemos ingresar dentro del contenedor de `Postgres` ejecutando el siguiente comando.

````bash
$ docker container exec -it c-postgres /bin/bash
d69f3ce25cfd:/# psql -U postgres -d db_user_service
psql (17.4)
Type "help" for help.

db_user_service=# \l
                                                       List of databases
      Name       |  Owner   | Encoding | Locale Provider |  Collate   |   Ctype    | Locale | ICU Rules |   Access privileges
-----------------+----------+----------+-----------------+------------+------------+--------+-----------+-----------------------
 db_user_service | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           |
 postgres        | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           |
 template0       | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/postgres          +
                 |          |          |                 |            |            |        |           | postgres=CTc/postgres
 template1       | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/postgres          +
                 |          |          |                 |            |            |        |           | postgres=CTc/postgres
(4 rows)

db_user_service=#
````

Como hemos configurado el puerto `5433` en nuestro host (`pc local`), podemos conectarnos a la base de datos del
contenedor utilizando herramientas como `DBeaver`.

![04.png](assets/section-12/04.png)

## Creando contenedor Docker de MySQL para Zipkin y configurando esquema DDL

En el `capítulo 11` vimos cómo integrar `MySQL` como sistema de almacenamiento para `Zipkin`.
En este apartado continuaremos utilizando `MySQL` como almacenamiento de `Zipkin`, pero esta vez estará contenarizado.
Para ello, crearemos un contenedor de MySQL específicamente para Zipkin. Ejecutamos el siguiente comando:

````bash
$ docker container run -d -p 3308:3306 --name c-mysql-zipkin -e MYSQL_DATABASE=db_zipkin -e MYSQL_ROOT_PASSWORD=magadiflo -e MYSQL_USER=zipkin -e MYSQL_PASSWORD=zipkin --network microservices-project-net mysql:8.0.41-debian
07f5e85a4681c5431af63b7a11e102fd4d5e23dfb6f4ccb0f5f4e548e54e9af5
````

**Nota**

- `docker container run`, Comando para crear y ejecutar un contenedor.
- `-d`, ejecutar en modo `detached` (segundo plano).
- `-p 3308:3306`
    - `3308`, puerto del host (`pc local`).
    - `3306`, puerto interno del contenedor (puerto estándar de `MySQL`).
- `--name c-mysql-zipkin`, nombre asignado al contenedor (`c-mysql-zipkin`).
- `-e MYSQL_DATABASE=db_zipkin`, variable de entorno que indica que se creará automáticamente una base de datos llamada
  `db_zipkin`.
- `-e MYSQL_ROOT_PASSWORD=magadiflo`, variable de entorno que define la contraseña del usuario `root`.
- `-e MYSQL_USER=zipkin`, variable de entorno que define un usuario adicional llamado `zipkin`.
- `-e MYSQL_PASSWORD=zipkin`, contraseña asociada al usuario `zipkin`.
- `--network microservices-project-net`, se conecta el contenedor a la red `Docker` personalizada
  `microservices-project-net`.
- `mysql:8.0.41-debian`, imagen utilizada, específica de `MySQL versión 8.0.41` basada en `Debian`.

**Importante**

- `Usuario y contraseña personalizados`
    - La imagen oficial de MySQL permite crear un usuario adicional (en este caso zipkin) mediante las variables
      `MYSQL_USER` y `MYSQL_PASSWORD`.
    - Este usuario tiene acceso a la base de datos indicada en `MYSQL_DATABASE` (db_zipkin).
    - La configuración de `MYSQL_ROOT_PASSWORD` `es obligatoria` aunque uses un usuario personalizado, porque la imagen
      necesita definir siempre la contraseña del usuario root al iniciarse.

Listamos los contenedores y verificamos que el `c-mysql-zipkin` se ha creado correctamente.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS                     PORTS                               NAMES
07f5e85a4681   mysql:8.0.41-debian   "docker-entrypoint.s…"   26 seconds ago   Up 25 seconds              33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine    "docker-entrypoint.s…"   47 minutes ago   Up 47 minutes              0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian   "docker-entrypoint.s…"   2 hours ago      Up 2 hours                 33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server      "/__cacert_entrypoin…"   2 days ago       Exited (255) 2 hours ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server         "/__cacert_entrypoin…"   2 days ago       Exited (255) 2 hours ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Ingresamos dentro del contenedor para verificar nuestra base de datos.

````bash
$ docker container exec -it c-mysql-zipkin /bin/sh
# mysql -u zipkin -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 9
Server version: 8.0.41 MySQL Community Server - GPL

Copyright (c) 2000, 2025, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| db_zipkin          |
| information_schema |
| performance_schema |
+--------------------+
3 rows in set (0.01 sec)

mysql>
````

Utilizando `DBeaver` creamos una conexión a la base de datos contenerizada de `zipkin`, aquí estamos usando el puerto de
nuestro host (pc local) `3308`.

![05.png](assets/section-12/05.png)

En el `capítulo 11` ejecutamos un script para crear las tablas que usará `zipkin`, así que en este capítulo ejecutaremos
ese script para crear nuestras tablas.

![06.png](assets/section-12/06.png)

## Creando contenedor docker de Zipkin Server conectado a contenedor de MySQL

Anteriormente, habíamos descargado el `jar` de `zipkin` y en un archivo `start-zipkin.bat` de windows habíamos colocado
las variables de entorno para conectarnos a `mysql` y al final habíamos colocado el comando para ejecutar el `jar`.

En este apartado levantaremos el `Zipkin Server` a través de un contenedor de Docker y lo integraremos con el
contenedor que habíamos creado anteriormente `c-mysql-zipkin`. Para eso, simplemente ejecutamos el siguiente comando.

````bash
$ docker container run -d -p 9411:9411 --name c-zipkin -e STORAGE_TYPE=mysql -e MYSQL_HOST=c-mysql-zipkin -e MYSQL_DB=db_zipkin -e MYSQL_USER=zipkin -e MYSQL_PASS=zipkin --network microservices-project-net openzipkin/zipkin:3.5.1
Unable to find image 'openzipkin/zipkin:3.5.1' locally
3.5.1: Pulling from openzipkin/zipkin
d08e7a152654: Pull complete
f338522607a6: Pull complete
9024432539a6: Pull complete
217376793de7: Pull complete
c23605470ecb: Pull complete
0724a0c030ee: Pull complete
a45115d6642f: Pull complete
a2b7dbaa785d: Pull complete
4125d9c649e6: Pull complete
Digest: sha256:bb570eb45c2994eaf32da783cc098b3d51d1095b73ec92919863d73d0a9eaafb
Status: Downloaded newer image for openzipkin/zipkin:3.5.1
dd4f558582586ea8f79f9203e76c79220b2bae2708fc5b2d18bad2818f8892bf
````

**Dónde**

- `docker container run`, comando para crear y ejecutar un nuevo contenedor.
- `-d`, opción que indica que el contenedor se ejecutará en segundo plano (modo `detached`).
- `-p 9411:9411`, mapea el puerto `9411` del contenedor al puerto `9411` de la `pc local (host)`.
- `--name c-zipkin`, asigna el nombre `c-zipkin` al contenedor.
- `-e STORAGE_TYPE=mysql`, define que `Zipkin` usará `MySQL` como sistema de almacenamiento de trazas.
- `-e MYSQL_HOST=c-mysql-zipkin`, indica el nombre del contenedor de `MySQL` al que debe conectarse.
- `-e MYSQL_DB=db_zipkin`, nombre de la base de datos que `Zipkin` usará dentro de `MySQL`.
- `-e MYSQL_USER=zipkin`, usuario de la base de datos configurado en `MySQL`.
- `-e MYSQL_PASS=zipkin`, contraseña del usuario configurado en `MySQL`.
- `--network microservices-project-net`, conecta el contenedor a la red personalizada donde también está el contenedor
  de `MySQL`.
- `openzipkin/zipkin:3.5.1`: imagen oficial de `Zipkin` (no slim), `versión 3.5.1`.

**Nota importante sobre la imagen**

- Al no tener la imagen `openzipkin/zipkin:3.5.1` descargada localmente, `Docker` automáticamente la descarga desde el
  repositorio remoto cuando se ejecuta el comando `docker container run`.
- No es necesario descargar previamente la imagen usando `docker pull`, aunque también es una opción si se quiere tener
  la imagen disponible antes de crear el contenedor.

Si listamos los contenedores observamos que nuestro `c-zipkin` se está ejecutando sin problemas.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                     PORTS                               NAMES
dd4f55858258   openzipkin/zipkin:3.5.1   "start-zipkin"           42 seconds ago   Up 39 seconds (healthy)    9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   3 hours ago      Up 25 minutes              33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   4 hours ago      Up 25 minutes              0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   5 hours ago      Up 25 minutes              33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   2 days ago       Exited (255) 5 hours ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   2 days ago       Exited (255) 5 hours ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Si listamos las imágenes, podemos ver que Docker descargó por nosotros la imagen `openzipkin/zipkin:3.5.1` que usamos
para crear el contenedor anterior.

````bash
$ docker image ls
REPOSITORY          TAG             IMAGE ID       CREATED        SIZE
openzipkin/zipkin   3.5.1           bb570eb45c29   24 hours ago   379MB
discovery-server    latest          90c55dd49e12   2 days ago     391MB
config-server       latest          4deeb21c52dd   3 days ago     366MB
postgres            17-alpine       7062a2109c4b   2 months ago   398MB
mysql               8.0.41-debian   b2252987e0ec   3 months ago   812MB
````

Ingresamos al dashboard de `Zipkin Server` a través de la dirección `http://127.0.0.1:9411/`.

![07.png](assets/section-12/07.png)

Como ha sido la primera vez que ingresamos al dashboard de `Zipkin`, nos aparece en la consola que nuestro
`Zipkin Server` está conectado a `MySQL 8.0.41`.

````bash
$ docker container logs c-zipkin

                  oo
                 oooo
                oooooo
               oooooooo
              oooooooooo
             oooooooooooo
           ooooooo  ooooooo
          oooooo     ooooooo
         oooooo       ooooooo
        oooooo   o  o   oooooo
       oooooo   oo  oo   oooooo
     ooooooo  oooo  oooo  ooooooo
    oooooo   ooooo  ooooo  ooooooo
   oooooo   oooooo  oooooo  ooooooo
  oooooooo      oo  oo      oooooooo
  ooooooooooooo oo  oo ooooooooooooo
      oooooooooooo  oooooooooooo
          oooooooo  oooooooo
              oooo  oooo

     ________ ____  _  _____ _   _
    |__  /_ _|  _ \| |/ /_ _| \ | |
      / / | || |_) | ' / | ||  \| |
     / /_ | ||  __/| . \ | || |\  |
    |____|___|_|   |_|\_\___|_| \_|

:: version 3.5.1 :: commit b7e2258 ::

2025-04-28T20:20:00.058Z  WARN [/] 1 --- [           main] i.m.p.PrometheusMeterRegistry            : A MeterFilter is being configured after a Meter has been registered to this registry. All MeterFilters should be configured before any Meters are registered. If that is not possible or you have a use case where it should be allowed, let the Micrometer maintainers know at https://github.com/micrometer-metrics/micrometer/issues/4920. Enable DEBUG level logging on this logger to see a stack trace of the call configuring this MeterFilter.
2025-04-28T20:20:00.401Z  INFO [/] 1 --- [oss-http-*:9411] c.l.a.s.Server                           : Serving HTTP at /[0:0:0:0:0:0:0:0]:9411 - http://127.0.0.1:9411/
2025-04-28T20:20:02.278Z  INFO [/] 1 --- [cking-tasks-1-1] c.z.h.p.HikariPool                       : HikariPool-1 - Added connection org.mariadb.jdbc.Connection@4f06c23f
2025-04-28T20:20:02.879Z  INFO [/] 1 --- [cking-tasks-1-1] o.j.Constants                            :

jOOQ tip of the day: Want to upgrade your jOOQ experience? Check out these commercial only features of the jOOQ Professional and Enterprise Editions: https://www.jooq.org/commercial-only-features/

2025-04-28T20:20:02.910Z  INFO [/] 1 --- [cking-tasks-1-1] o.j.i.D.logVersionSupport                : Version                  : Database version is supported by dialect MYSQL: 8.0.41
````

## Configurando URL de Eureka, Config Server y Zipkin en microservicios

Vamos a realizar modificaciones en los `application.yml` de todos los microservicios para que apunten a trabajar con
contenedores.

Modificamos el `product-service`.

````yml
spring:
  application:
    name: product-service
  datasource:
    url: jdbc:mysql://c-mysql:3306/db_product_service?serverTimezone=America/Lima

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

**Dónde**

- `c-mysql` es el nombre del contenedor que usará este microservicio y donde hemos creado la `db_product_service`.
- Dentro de una red Docker (`microservices-project-net` en tu caso), los contenedores se comunican usando sus nombres
  como si fueran `hostnames`.
- El puerto interno del contenedor `c-mysql` es `3306`, así que está bien definida en nuestra configuración.
- `c-discovery-server`, es el nombre del contenedor de nuestro `Discovery Server`. Mismo principio: la comunicación en
  Docker se hace por el nombre del contenedor. Puerto `8761` también corresponde al `puerto interno` del contenedor.
- `c-zipkin` es nuestro contenedor de `Zipkin`. Puerto interno del contenedor: `9411`.

Modificamos el `item-service`.

````yml
spring:
  application:
    name: item-service
  config:
    import: optional:configserver:http://c-config-server:8888

management:
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/
````

Modificamos el `user-service`.

````yml
spring:
  application:
    name: user-service
  config:
    import: optional:configserver:http://c-config-server:8888
  datasource:
    url: jdbc:postgresql://c-postgres:5432/db_user_service

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  tracing:
    sampling:
      probability: 1.0
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

Modificamos el `user-service`.

````yml
spring:
  application:
    name: user-service
  config:
    import: optional:configserver:http://c-config-server:8888
  datasource:
    url: jdbc:postgresql://c-postgres:5432/db_user_service

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  tracing:
    sampling:
      probability: 1.0
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

Modificamos el `authorization-server`.

````yml
eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  tracing:
    sampling:
      probability: 1.0
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

Modificamos el `gateway-server`.

````yml
spring:
  application:
    name: gateway-server

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

Recordar que en las configuraciones del `gateway-server` tenemos rutas a donde nuestro `gateway` redirecciona. En estas
rutas nosotros hemos definido como `uri` el nombre del microservicio con el prefijo `lb://` similar a cómo se muestra.

````bash
uri: lb://product-service
uri: lb://item-service
uri: lb://user-service
uri: lb://authorization-server
````

Aquí es muy importante tener en cuenta que estos nombres son nombres de los microservicios definidos en
`spring.application.name` y no es el nombre de los contenedores que crearemos más adelante como `c-product-service` o
`c-item-service`, etc.

Cuando ponemos `lb://`, el `Gateway` va a preguntarle a `Eureka ("c-discovery-server")` en qué contenedor está corriendo
el microservicio por su `spring.application.name`.

Recordar también que en las configuraciones del `gateway-server` hay configuraciones que son propias del
`authorization-server`, como los siguientes.

````bash
issuer-uri: http://127.0.0.1:9000
http://127.0.0.1:8090/authorized
````

Estas configuraciones las dejaremos tal cual.

## Creando Dockerfile para servicio productos, build y run

Vamos a crear el archivo `Dockerfile` en la raíz del microservicio `product-service` y colocar las instrucciones que se
muestran a continuación.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests

FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract

FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

**!Observación importante¡**

> Si nuestro microservicio `Spring Boot` está configurado con `server.port=0` para usar un puerto aleatorio en cada
> instancia, `NO debemos definir un EXPOSE fijo` en el `Dockerfile`.
>
> Dejar el EXPOSE fijo, como por ejemplo `EXPOSE 8888` hace que `Docker` espere un servicio en ese puerto, causando
> problemas de conexión. Simplemente eliminemos el `EXPOSE` y permitamos que `Eureka` maneje la ubicación `(host:port)`
> de cada instancia correctamente.
>
> En nuestro caso, el microservicio `product-service` tiene configurado `server.port=0`, lo que significa que Spring
> Boot generará un puerto aleatorio en cada instancia.
>
> Por este motivo, en el `Dockerfile` no debemos usar `EXPOSE <puerto>`, porque el puerto interno real de la aplicación
> será aleatorio, no fijo. Si colocáramos, por ejemplo, `EXPOSE 8888`, `Docker` mapearía el puerto aleatorio del host
> hacia el puerto `8888` interno, pero Spring Boot podría estar escuchando en otro puerto diferente, causando problemas
> de conexión. Esto solo ocurre cuando usamos la bandera `-P (mayúscula)`, pero si usamos la `-p (minúscula)`, aquí
> el `EXPOSE` solo funciona como documentación.
>
> Además:
>
> Si usamos el parámetro `-p (minúscula)` en `docker container run` (es decir, hacemos un mapeo manual
> `-p HOST:CONTENEDOR`), el `EXPOSE` solo servirá como documentación y no tendrá un efecto práctico.
>
> Pero si usamos el parámetro `-P (mayúscula)`, `Docker` nos permitirá seleccionar un puerto aleatorio del
> `host (pc local)` y se redirigirá al puerto o puertos que estén definidos en el `EXPOSE`.
>
> Por lo tanto, si tu microservicio Spring Boot genera puertos aleatorios (`server.port=0`), es mejor no definir
> `EXPOSE` en el `Dockerfile` para evitar inconsistencias en el mapeo de puertos.

Luego, mediante la terminal nos ubicamos en la raíz del microservicio donde está el `Dockerfile` y ejecutamos el
siguiente comando para construir la imagen que tendrá el mismo nombre que nuestro microservicio `product-service`.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\business-domain\product-service (feature/section-12)
$ docker image build -t product-service .
````

Posteriormente, procedemos a construir un contenedor a partir de la imagen `product-service` construida anteriormente.

````bash
$ docker container run -d -P --name c-product-service --network microservices-project-net product-service
761e8e5d7932cd30cbc58982a61d59ca0133945a005162d93f859fa088bee25e
````

**Dónde**

- El parámetro `-P` (`mayúscula`) `publica automáticamente` todos los puertos expuestos en el contenedor en
  `puertos aleatorios` de nuestra máquina anfitriona (host).
- `Docker` tomará el puerto interno aleatorio que use nuestro microservicio y lo mapeará automáticamente a un puerto
  disponible del host.

Listamos los contenedores que tenemos hasta ahora.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                 PORTS                               NAMES
761e8e5d7932   product-service           "/__cacert_entrypoin…"   17 minutes ago   Up 17 minutes                                              c-product-service
dd4f55858258   openzipkin/zipkin:3.5.1   "start-zipkin"           3 hours ago      Up 3 hours (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   6 hours ago      Up 4 hours             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   7 hours ago      Up 4 hours             0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   8 hours ago      Up 4 hours             33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   3 days ago       Up 2 hours             0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   3 days ago       Up 2 hours             0.0.0.0:8888->8888/tcp              c-config-server
````

Cuando ejecutamos el contenedor `c-product-service` con la opción `-P (mayúscula)`, esperábamos que `Docker` mapee
automáticamente un puerto aleatorio del host hacia un puerto interno del contenedor.

Sin embargo, al listar los contenedores (`docker container ls -a`), observamos que la columna `PORTS` aparece vacía, es
decir, no hay ningún puerto mapeado.

La razón es la siguiente:

- La opción `-P (mayúscula)` de `Docker` depende del `EXPOSE` definido en el `Dockerfile` de la imagen.
- El `EXPOSE` indica a `Docker` qué puertos internos del contenedor están disponibles para ser expuestos/mapeados
  automáticamente.
- En este caso, como en el `Dockerfile` de `product-service` `NO` se definimos ningún `EXPOSE`,
  `Docker` no sabe qué puerto interno debería mapear, aunque usemos `-P`.
- Como resultado, `Docker` no mapea ningún puerto del contenedor hacia el host, y por eso no vemos puertos en la salida
  del comando.

Nuestro microservicio `product-service` tiene configurado el `server.port=0`, esto significa que Spring Boot elegirá
dinámicamente un puerto aleatorio cada vez que el servicio se inicie. Por lo tanto:

- No hay un puerto fijo que se pueda exponer de antemano.
- Definir un `EXPOSE <port>` en el `Dockerfile` sería incorrecto, porque no coincidiría con el puerto aleatorio real que
  `Spring Boot` usaría al arrancar.
- No requerimos que los microservicios expongan puertos al host, ya que toda la comunicación es interna entre
  contenedores.

Ahora, si vemos los logs del contenedor `c-product-service` observamos que todo está trabajando correctamente, el puerto
en el que el microservicio `product-service` se está ejecutando al interior del contenedor es el puerto `39865`.

````bash
$ docker container logs c-product-service

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

INFO 1 --- [product-service] [           main] [                                                 ] d.m.p.app.ProductServiceApplication      : Starting ProductServiceApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app)
INFO 1 --- [product-service] [           main] [                                                 ] d.m.p.app.ProductServiceApplication      : No active profile set, falling back to 1 default profile: "default"
INFO 1 --- [product-service] [           main] [                                                 ] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
INFO 1 --- [product-service] [           main] [                                                 ] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 159 ms. Found 1 JPA repository interface.
INFO 1 --- [product-service] [           main] [                                                 ] o.s.cloud.context.scope.GenericScope     : BeanFactory id=7c08f00b-ffd2-38e6-b40d-c4e7a244ced0
INFO 1 --- [product-service] [           main] [                                                 ] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 0 (http)
INFO 1 --- [product-service] [           main] [                                                 ] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
INFO 1 --- [product-service] [           main] [                                                 ] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
INFO 1 --- [product-service] [           main] [                                                 ] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
INFO 1 --- [product-service] [           main] [                                                 ] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2835 ms
INFO 1 --- [product-service] [           main] [                                                 ] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
INFO 1 --- [product-service] [           main] [                                                 ] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.6.11.Final
INFO 1 --- [product-service] [           main] [                                                 ] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
INFO 1 --- [product-service] [           main] [                                                 ] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
INFO 1 --- [product-service] [           main] [                                                 ] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
INFO 1 --- [product-service] [           main] [                                                 ] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection com.mysql.cj.jdbc.ConnectionImpl@a72925
INFO 1 --- [product-service] [           main] [                                                 ] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
INFO 1 --- [product-service] [           main] [                                                 ] org.hibernate.orm.connections.pooling    : HHH10001005: Database info:
        Database JDBC URL [Connecting through datasource 'HikariDataSource (HikariPool-1)']
        Database driver: undefined/unknown
        Database version: 8.0.41
        Autocommit mode: undefined/unknown
        Isolation level: undefined/unknown
        Minimum pool size: undefined/unknown
        Maximum pool size: undefined/unknown
INFO 1 --- [product-service] [           main] [                                                 ] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
INFO 1 --- [product-service] [           main] [                                                 ] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
WARN 1 --- [product-service] [           main] [                                                 ] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
INFO 1 --- [product-service] [           main] [                                                 ] DiscoveryClientOptionalArgsConfiguration : Eureka HTTP Client uses RestTemplate.
WARN 1 --- [product-service] [           main] [                                                 ] iguration$LoadBalancerCaffeineWarnLogger : Spring Cloud LoadBalancer is currently working with the default cache. While this cache implementation is useful for development and tests, it's recommended to use Caffeine cache in production.You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
INFO 1 --- [product-service] [           main] [                                                 ] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint beneath base path '/actuator'
INFO 1 --- [product-service] [           main] [                                                 ] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 39865 (http) with context path '/'
INFO 1 --- [product-service] [           main] [                                                 ] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 39865
INFO 1 --- [product-service] [           main] [                                                 ] o.s.c.n.eureka.InstanceInfoFactory       : Setting initial instance status as: STARTING
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Initializing Eureka in region us-east-1
INFO 1 --- [product-service] [           main] [                                                 ] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Disable delta property : false
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Single vip registry refresh property : null
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Force full registry fetch : false
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application is null : false
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Registered Applications size is zero : true
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application version is -1: true
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Getting all instance registry info from the eureka server
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : The response status is 200
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Starting heartbeat executor: renew interval is: 30
INFO 1 --- [product-service] [           main] [                                                 ] c.n.discovery.InstanceInfoReplicator     : InstanceInfoReplicator onDemand update allowed rate per min is 4
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Discovery Client initialized at timestamp 1745881830595 with initial instances count: 0
INFO 1 --- [product-service] [           main] [                                                 ] o.s.c.n.e.s.EurekaServiceRegistry        : Registering application PRODUCT-SERVICE with eureka with status UP
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Saw local status change event StatusChangeEvent [timestamp=1745881830604, current=UP, previous=STARTING]
INFO 1 --- [product-service] [foReplicator-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_PRODUCT-SERVICE/761e8e5d7932:product-service:656e27633c990a9e985dab520364dac4: registering service...
INFO 1 --- [product-service] [           main] [                                                 ] d.m.p.app.ProductServiceApplication      : Started ProductServiceApplication in 8.604 seconds (process running for 9.507)
INFO 1 --- [product-service] [foReplicator-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_PRODUCT-SERVICE/761e8e5d7932:product-service:656e27633c990a9e985dab520364dac4 - registration status: 204
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Disable delta property : false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Single vip registry refresh property : null
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Force full registry fetch : false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application is null : false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Registered Applications size is zero : true
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application version is -1: false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Getting all instance registry info from the eureka server
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : The response status is 200
````

Si ahora revisamos `Eureka Server` podemos ver que nuestro `product-service` se ha registrado correctamente.

![08.png](assets/section-12/08.png)

Si revisamos el servidor de zipkin, podemos ver que nuestro `product-service` está enviando trazas, eso indica que los
contenedores se están comunicando correctamente.

![09.png](assets/section-12/09.png)
