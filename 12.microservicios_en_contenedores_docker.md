# Sección 12: Desplegando Microservicios en Contenedores Docker

---

## [Creando archivo Dockerfile eficiente para el Config Server](https://github.com/magadiflo/spring-microservices-in-action-2021/blob/main/04.welcome-to-docker.md#creando-imagen-de-docker-eficiente)

En la raíz del `config-server` creamos un archivo sin extensión llamado `Dockerfile`. Este archivo nos permitirá crear
la imagen de nuestro servidor de configuraciones.

Si bien es posible convertir un `jar` de `Spring Boot` en una imagen de `Docker` con solo unas pocas líneas en el
`Dockerfile`, usaremos la `función de capas` para crear una imagen de `Docker optimizada`. El comando `extract` se puede
utilizar para dividir fácilmente la aplicación en capas que se agregarán al `dockerfile`.

Agregamos el archivo `Dockerfile` en la raíz de nuestro microservicio para crear la `imagen docker optimizada` que
utiliza un `jarmode`.

El siguiente `Dockerfile` es un archivo `multi-stage` de tres etapas(`dependencies`, `builder`, `runner`).

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

EXPOSE 8888
CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

**Dónde**

- En la primera etapa estamos usando la imagen base del `eclipse-temurin:21-jdk-alpine` de java, esto es importante
  porque en esta etapa vamos a descargar las dependencias y compilar el `jar`, por eso necesitamos que sea un `JDK`.


- En las dos últimas etapas usamos como imagen base `eclipse-temurin:21-jre-alpine`, es decir, únicamente el entorno
  para ejecutar una aplicación java, aquí ya no necesitamos el `JDK`, sino solo el `JRE`, además que es más liviano.


- `RUN sed -i -e 's/\r$//' ./mvnw`, es un comando de `Linux` que utiliza `sed`, el editor de flujo, para eliminar los
  caracteres
  de retorno de carro `(\r)` al final de cada línea en el archivo `mvnw`.


- `RUN ./mvnw dependency:go-offline`, iniciamos la descarga de las dependencias de maven.


- `RUN java -Djarmode=layertools -jar app.jar extract`:
    - `Spring Boot 2.3` introdujo un modo especial para manejar los archivos `jar`, llamado `Layertools`. Esto permite
      que el `jar` se estructure en capas lógicas como `dependencies`, `spring-boot-loader`, `snapshot-dependencies`,
      y `application`.
    - `El objetivo de estas capas es optimizar la reconstrucción de imágenes Docker`. Al tener una estructura de capas,
      si una parte del código cambia, solo se necesita reconstruir las capas afectadas en lugar de todo el `jar`. Esto
      puede resultar en una construcción de `imágenes Docker` más `rápida` y `eficiente`.
    - Este comando utiliza el modo de herramientas de capa (`layertools`) que permite ejecutar operaciones específicas
      relacionadas con el manejo de capas.
    - El comando `extract` extrae capas del `jar` para la creación de la imagen. Este comando descompone el archivo
      `jar` de la siguiente manera:
        - `Dependencies`: Las dependencias de tiempo de ejecución.
        - `Spring-Boot-Loader`: La parte responsable de iniciar una aplicación Spring Boot.
        - `Snapshot-Dependencies`: Dependencias etiquetadas como "snapshot".
        - `Application`: El código específico de la aplicación, como tus clases y recursos.


- `COPY --from=builder /app/dependencies ./` (y los otros `COPY` de las capas del `jar`), copiamos las distintas
  capas que han sido extraídas por el comando descrito anteriormente. Las copiamos en una nueva etapa del
  `Dockerfile`.


- `EXPOSE 8888`, el comando `EXPOSE` en un `Dockerfile` sirve para `documentar` que un contenedor escucha en un puerto
  específico. No abre ni publica el puerto hacia el exterior por sí mismo, simplemente declara que tu aplicación dentro
  del contenedor usará ese puerto. Es básicamente `una forma de documentar` qué puerto utiliza la aplicación dentro del
  contenedor. Le dice a otros desarrolladores (y a herramientas como Docker Compose) que la app escucha en ese puerto.


- `CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]`, hasta este punto no estamos trabajando con un
  `jar`, sino más bien con el desempaquetado del `jar`, eso significa que no podemos usar el clásico
  `CMD ["java","-jar","app.jar"]` para ejecutar la aplicación cuando inicie el contenedor. El `JarLauncher` es parte del
  sistema de carga de `Spring Boot`. Este lanzador permite cargar la aplicación desde capas separadas, esencialmente
  emulando el comportamiento de un `jar` ejecutable pero permitiendo el uso de capas para optimizar la reconstrucción
  del contenedor. En resumen, en esta última etapa del `Dockerfile` estamos copiando el desempaquetado del `jar` de la
  etapa `builder` al directorio `/app` de esta última etapa `runner`. Eso significa, que el contenedor final, tendrá en
  el directorio `/app` todos los directorios y archivos del desempaquetado del `jar` y para poder iniciar la aplicación
  utilizaremos el `JarLauncher` ubicado en el directorio `org.springframework.boot.loader.launch`. Más adelante, cuando
  creemos un contendor ingresaremos dentro de él para ver el contenido del directorio `/app`. Nos daremos cuenta que
  este directorio contendrá los directorios y archivos del desempaquetado del `jar`.

## Construyendo imagen Docker del Config Server

Mediante la línea de comandos nos posicionamos en la raíz del `config-server` donde tenemos nuestro `Dockerfile`. Luego,
escribimos el siguiente comando que nos permitirá generar la `imagen docker` de este microservicio.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\infrastructure\config-server (feature/section-12)
$ docker image build -t config-server . 
````

**Dónde**

- `docker image build`, comando de Docker para construir una imagen a partir de un `Dockerfile`.


- `-t`, es una bandera que nos permite definir un `tag name` *(nombre y versión)* para la imagen. En nuestro caso, el
  nombre que le daremos a la imagen será `config-server`. Como no especificamos un `tag` explícito, `Docker` usará por
  defecto el tag `latest`. Si quisiéramos agregar un tag personalizado, lo haríamos así: `config-server:mi-version-tag`.


- `.`, el punto al final de la instrucción indica que Docker debe buscar el archivo `Dockerfile` en el directorio
  actual. En este caso, el directorio actual es la raíz del proyecto `/config-server`, donde estamos ubicados en la
  terminal. Justo allí se encuentra nuestro `Dockerfile`, así que le estamos diciendo a `Docker` que use ese contexto
  para construir la imagen.

Una vez explicado cada parte del comando procedemos a ejecutarlo.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\infrastructure\config-server (feature/section-12)
$ docker image build -t config-server .                                                                                                                
[+] Building 51.1s (24/24) FINISHED                                                                                                                    
 => [internal] load build definition from Dockerfile                                                                                                   
 => => transferring dockerfile: 782B                                                                                                                   
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jre-alpine                                                                       
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jdk-alpine                                                                       
 => [auth] library/eclipse-temurin:pull token for registry-1.docker.io                                                                                 
 => [internal] load .dockerignore                                                                                                                      
 => => transferring context: 2B                                                                                                                        
 => [dependencies 1/9] FROM docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0    
 => => resolve docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0                 
 => [builder 1/4] FROM docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd         
 => => resolve docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd                 
 => => sha256:cda86626eeb372589c3378d030f4522ba1b0c78ec58b1db87960fa4e5fcd3e34 2.28kB / 2.28kB                                                         
 => => sha256:e6744199aa66ab985e37e72924f1568a6751afa2c508c42a1b3b945f3a8850a7 126B / 126B                                                             
 => => sha256:74f6a226ed936757680facf9b217f62a2af16b663a69df8e4b3ece925e27ed2a 53.06MB / 53.06MB                                                       
 => => sha256:f6cd406c8d97cafcb893e824126c17fa19907b2bbc8d759931089e1be1e75750 16.18MB / 16.18MB                                                       
 => => extracting sha256:f6cd406c8d97cafcb893e824126c17fa19907b2bbc8d759931089e1be1e75750                                                              
 => => extracting sha256:74f6a226ed936757680facf9b217f62a2af16b663a69df8e4b3ece925e27ed2a                                                              
 => => extracting sha256:e6744199aa66ab985e37e72924f1568a6751afa2c508c42a1b3b945f3a8850a7                                                              
 => => extracting sha256:cda86626eeb372589c3378d030f4522ba1b0c78ec58b1db87960fa4e5fcd3e34                                                              
 => [internal] load build context                                                                                                                      
 => => transferring context: 13.95kB                                                                                                                   
 => CACHED [dependencies 2/9] WORKDIR /app                                                                                                             
 => [dependencies 3/9] COPY ./mvnw ./                                                                                                                  
 => [dependencies 4/9] COPY ./.mvn ./.mvn                                                                                                              
 => [dependencies 5/9] COPY ./pom.xml ./                                                                                                               
 => [dependencies 6/9] RUN sed -i -e 's/\r$//' ./mvnw                                                                                                  
 => [dependencies 7/9] RUN ./mvnw dependency:go-offline                                                                                                
 => [builder 2/4] WORKDIR /app                                                                                                                         
 => [dependencies 8/9] COPY ./src ./src                                                                                                                
 => [dependencies 9/9] RUN ./mvnw clean package -DskipTests                                                                                            
 => [builder 3/4] COPY --from=dependencies /app/target/*.jar ./app.jar                                                                                 
 => [builder 4/4] RUN java -Djarmode=layertools -jar app.jar extract                                                                                   
 => [runner 3/6] COPY --from=builder /app/dependencies ./                                                                                              
 => [runner 4/6] COPY --from=builder /app/spring-boot-loader ./                                                                                        
 => [runner 5/6] COPY --from=builder /app/snapshot-dependencies ./                                                                                     
 => [runner 6/6] COPY --from=builder /app/application ./                                                                                               
 => exporting to image                                                                                                                                 
 => => exporting layers                                                                                                                                
 => => exporting manifest sha256:ac034572e56e413bb510510d6e5013ef06c3fdcb2451a8d4202c6e45155b78ff                                                      
 => => exporting config sha256:49e50361b329d89f16d543879e5529c9db482271c8aa708184ffa23780ebdb60                                                        
 => => exporting attestation manifest sha256:c14bc17e6104177e6937e649ed3485c948cfd3dfc3e5d614df8eb7a1d6bcc961                                          
 => => exporting manifest list sha256:4deeb21c52dd73d36cb71af8a8d844d91beaf67cd21facc95fdc17c5c4f32439                                                 
 => => naming to docker.io/library/config-server:latest                                                                                                
 => => unpacking to docker.io/library/config-server:latest                                                                                             
                                                                                                                                                       
View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/ievhbmkf37mzrg72bqzg54dzm                                             
````

Finalizada la construcción, listamos las imágenes para ver el que acabamos de crear.

````bash
$ docker image ls
REPOSITORY      TAG       IMAGE ID       CREATED          SIZE
config-server   latest    4deeb21c52dd   21 minutes ago   366MB
````

Observamos que la imagen tiene el tag `latest`. Esto ocurre porque al construir la imagen con la opción
`-t config-server` no especificamos un tag explícito (por ejemplo, `config-server:1.0.0`).


> `Docker`, por convención, asigna automáticamente el `tag latest` cuando no se indica uno.
> Es una forma de marcar la versión más reciente por defecto, pero no significa que sea necesariamente la más nueva o
> recomendada — eso depende de la gestión de versiones que nosotros hagamos.

## Creando red en Docker

Creamos una red personalizada que luego asociaremos a los contenedores que forman parte de nuestro ecosistema de
microservicios.

````bash
$ docker network create microservices-project-net
4320f82dad9c8d95cf4e05cc5f6664a51d338df8b5b2c7b63d22e24275c212a6
````

**Dónde**

- `docker network create`, permite crear una nueva red personalizada en `Docker`.
- `microservices-project-net`, es el nombre que le damos a nuestra red.

Al crear una red personalizada:

- `Docker` usará por defecto el `driver bridge`, que permite la comunicación entre contenedores dentro de la misma red.
- Los contenedores conectados a esta red podrán resolverse entre sí por nombre, lo que simplifica mucho la comunicación
  entre microservicios (por ejemplo, `config-server` podrá ser accedido desde `eureka-server` solo usando su nombre de
  contenedor).
- Esta práctica `evita depender` de direcciones `IP dinámicas` y mejora la escalabilidad del proyecto.

> Es una buena práctica usar redes personalizadas en proyectos con múltiples contenedores, como una arquitectura de
> microservicios, ya que te da mayor control, aislamiento y facilidad para escalar.

Si listamos las redes, podemos verificar que está la que acabamos de crear.

````bash
$ docker network ls
NETWORK ID     NAME                        DRIVER    SCOPE
16b8d6f5da55   bridge                      bridge    local
8c77b18a87f1   host                        host      local
4320f82dad9c   microservices-project-net   bridge    local
fe251eccddcc   none                        null      local
````

Como se muestra en el resultado, `Docker` nos lista todas las redes disponibles actualmente:

- `bridge`, red predeterminada que `Docker` crea para permitir la comunicación entre contenedores si no se especifica
  una red personalizada.
- `host`, red especial donde el contenedor comparte directamente el stack de red del host (solo disponible en Linux).
- `none`, aisla completamente la red del contenedor (sin conectividad).
- `microservices-project-net`, es la red personalizada que acabamos de crear, usando el driver bridge.

Al crear redes personalizadas como `microservices-project-net`, ganamos ventajas clave como:

- Resolución de nombres entre contenedores.
- Mejor organización de servicios.
- Mayor seguridad y aislamiento del tráfico de red.

## Levantando contenedor a partir de la imagen Docker del Config Server

Una vez construida la `imagen Docker` del `Config Server`, podemos crear y ejecutar un contenedor a partir de ella
utilizando el siguiente comando.

````bash
$ docker container run -d -p 8888:8888 --name c-config-server --network microservices-project-net config-server
a59c00698b3fb1d3638c32a182e0110b50d3cb9deb1fc2252fc0d89a39031a3f
````

**Dónde**

- `docker container run`, comando para crear y ejecutar un contenedor a partir de una imagen.
- `-d`, ejecuta el contenedor en modo `detached` (segundo plano), permitiéndonos seguir usando la terminal.
- `-p 8888:8888`, especifica el mapeo de puertos en el formato HOST_PORT:CONTAINER_PORT. En este caso:
    - `8888 (izquierda)` es el puerto expuesto en nuestra máquina local.
    - `8888 (derecha)` es el puerto interno del contenedor en el que se ejecuta la aplicación Spring Boot.
- `--name c-config-server`, asigna un nombre identificador al contenedor.
- `--network microservices-project-net`, conecta el contenedor a la red personalizada que creamos anteriormente, lo que
  permite que se comunique fácilmente con otros contenedores de la misma red.
- `config-server`, es el nombre de la imagen `Docker` desde la cual se va a crear el contenedor.
    - Como no se especificó una versión (`tag`), Docker usará por defecto la etiqueta `latest`.
    - Si se desea usar una versión específica, se puede indicar así `config-server:1.0.0`.

Si ahora listamos los contenedores veremos que el contenedor `c-config-server` se está ejecutando correctamente, como lo
indica el estado `Up`. El mapeo de puertos `0.0.0.0:8888->8888/tcp` significa que cualquier solicitud hecha al puerto
`8888` de la máquina local será redirigida al puerto 8888 del contenedor. Esto nos permitirá acceder al `Config Server`
desde el navegador o cualquier cliente HTTP usando http://localhost:8888.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS          PORTS                    NAMES
a59c00698b3f   config-server   "/__cacert_entrypoin…"   23 seconds ago   Up 21 seconds   0.0.0.0:8888->8888/tcp   c-config-server
````

## Verificando que el contenedor está en la red docker

Después de levantar el contenedor, podemos `inspeccionar la red` que creamos para asegurarnos de que el contenedor está
efectivamente conectado a ella.

````bash
$ docker network inspect microservices-project-net
[
    {
        "Name": "microservices-project-net",
        "Id": "4320f82dad9c8d95cf4e05cc5f6664a51d338df8b5b2c7b63d22e24275c212a6",
        "Created": "2025-04-25T18:05:20.033232455Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv4": true,
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "172.18.0.0/16",
                    "Gateway": "172.18.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "a59c00698b3fb1d3638c32a182e0110b50d3cb9deb1fc2252fc0d89a39031a3f": {
                "Name": "c-config-server",
                "EndpointID": "fa81b04a669bb2440a5c71d275afcd5d5a71cc84fdf22c01ffc2c695e4bb43e8",
                "MacAddress": "c2:3b:21:ff:cb:18",
                "IPv4Address": "172.18.0.2/16",
                "IPv6Address": ""
            }
        },
        "Options": {},
        "Labels": {}
    }
]
````

En la salida del comando, dentro del atributo `Containers`, podemos ver que nuestro contenedor `c-config-server` está
registrado. Aparecen datos como:

- `Name: c-config-server` (el nombre que le dimos al contenedor).
- `IPv4Address: 172.18.0.2/16` (la IP asignada dentro de esta red).
- `MacAddress`, dirección física virtual asignada.

Este paso es útil para:

- Validar la conectividad entre contenedores cuando trabajamos en un entorno de microservicios.
- Usar nombres de contenedores como `hostnames` al momento de comunicar servicios entre sí dentro de la misma red
  Docker.

También podríamos `inspeccionar el contenedor` y verificar que la red a la que estamos conectados sea el que
definimos en la creación.

````bash
$ docker container inspect c-config-server
[
    {
        "HostConfig": {
            "NetworkMode": "microservices-project-net",
            ...
        },
        "NetworkSettings": {
            ...
            "Networks": {
                "microservices-project-net": {...}
            }
        },
        ...
    }
]
````

Como vemos el contenedor `c-config-server` está conectado a la red `microservices-project-net`, como se puede ver en
el resultado anterior.

## Logs del Config Server al iniciar el contenedor

Al iniciar el `contenedor Docker` del `Config Server`, es útil observar los `logs` para verificar que la aplicación se
ha levantado correctamente y que está leyendo los archivos de configuración como se espera.

Entonces, ejecutamos el siguiente comando para ver los logs del contenedor:

````bash
$ docker container logs c-config-server

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

2025-04-25T20:35:38.503Z  INFO 1 --- [config-server] [           main] d.m.c.s.app.ConfigServerApplication      : Starting ConfigServerApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app)
2025-04-25T20:35:38.506Z  INFO 1 --- [config-server] [           main] d.m.c.s.app.ConfigServerApplication      : No active profile set, falling back to 1 default profile: "default"
2025-04-25T20:35:39.888Z  INFO 1 --- [config-server] [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=209bd33f-ee6e-361d-bc59-b6fdc05076d4
2025-04-25T20:35:40.318Z  INFO 1 --- [config-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8888 (http)
2025-04-25T20:35:40.338Z  INFO 1 --- [config-server] [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-04-25T20:35:40.339Z  INFO 1 --- [config-server] [           main] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
2025-04-25T20:35:40.385Z  INFO 1 --- [config-server] [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-04-25T20:35:40.387Z  INFO 1 --- [config-server] [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1804 ms
2025-04-25T20:35:41.806Z  INFO 1 --- [config-server] [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint beneath base path '/actuator'
2025-04-25T20:35:41.891Z  INFO 1 --- [config-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8888 (http) with context path '/'
2025-04-25T20:35:41.913Z  INFO 1 --- [config-server] [           main] d.m.c.s.app.ConfigServerApplication      : Started ConfigServerApplication in 4.054 seconds (process running for 5.082)
2025-04-25T20:39:52.480Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-04-25T20:39:52.480Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-04-25T20:39:52.483Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 3 ms
2025-04-25T20:39:58.252Z  INFO 1 --- [config-server] [nio-8888-exec-1] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T20:40:03.094Z  INFO 1 --- [config-server] [nio-8888-exec-3] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T20:40:40.633Z  INFO 1 --- [config-server] [nio-8888-exec-5] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service-dev.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T20:40:40.633Z  INFO 1 --- [config-server] [nio-8888-exec-5] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T21:34:38.249Z  INFO 1 --- [config-server] [nio-8888-exec-7] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service-prod.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
2025-04-25T21:34:38.249Z  INFO 1 --- [config-server] [nio-8888-exec-7] o.s.c.c.s.e.NativeEnvironmentRepository  : Adding property source: Config resource 'file [/tmp/config-repo-10370607234881797303/item-service.yml]' via location 'file:/tmp/config-repo-10370607234881797303/'
````

Los `logs` del contenedor del `Config Server` confirman que la aplicación se ha iniciado correctamente usando `Java 21`,
desplegada en un contenedor con servidor embebido `Tomcat` escuchando en el puerto `8888`. Además, se evidencia que el
servidor ha cargado exitosamente los archivos de configuración (`item-service.yml`, `item-service-dev.yml` y
`item-service-prod.yml`) a partir de peticiones http realizadas para consultar estos archivos.
Esto indica que el `Config Server` está funcionando correctamente.

## Verificando el funcionamiento del Config Server

Podemos probar que el `Config Server` está corriendo correctamente y que tiene acceso al repositorio remoto de
configuración realizando peticiones http. En el siguiente ejemplo consultaremos por el archivo o perfil `dev`.

````bash
$ curl -v http://localhost:8888/item-service/dev | jq
>
< HTTP/1.1 200
< Content-Type: application/json
< Transfer-Encoding: chunked
< Date: Fri, 25 Apr 2025 20:40:40 GMT
<
{
  "name": "item-service",
  "profiles": [
    "dev"
  ],
  "label": null,
  "version": "6d1fd0fc178a112daa21f5705cd28023c55470f3",
  "state": "",
  "propertySources": [
    {
      "name": "https://github.com/magadiflo/config-server-repo.git/item-service-dev.yml",
      "source": {
        "custom.text": "item-service-dev.yml en config server con perfil dev (desde GitHub)",
        "custom.author.name": "Martín (desde GitHub)",
        "custom.author.email": "magadiflo@gmail.com (desde GitHub)"
      }
    },
    {
      "name": "https://github.com/magadiflo/config-server-repo.git/item-service.yml",
      "source": {
        "server.port": 8085,
        "custom.text": "item-service.yml en config server con perfil por defecto"
      }
    }
  ]
}
````

La petición anterior solicita el archivo de configuración para el servicio llamado `item-service` con el perfil `dev`.
Si todo está funcionando, obtendremos una respuesta `JSON` con las propiedades definidas en los archivos
`item-service.yml` y `item-service-dev.yml` del repositorio remoto, tal como se muestra en el resultado anterior.

## Renombrando una Imagen en Docker

En `Docker`, podemos `renombrar` o `etiquetar` imágenes existentes utilizando el comando `docker image tag`. Este
proceso es útil cuando queremos cambiar el nombre de una imagen o asignarle una nueva versión. A continuación, mostramos
cómo hacerlo:

1. `Listamos las imágenes actuales`. Primero, listamos las imágenes disponibles en la plataforma Docker con el siguiente
   comando.
    ````bash
    $ docker image ls
    REPOSITORY      TAG       IMAGE ID       CREATED       SIZE
    config-server   latest    4deeb21c52dd   4 hours ago   366MB
    ````
   Aquí, podemos ver que tenemos una imagen llamada `config-server` con la etiqueta `latest`.


3. `Renombrando la imagen`. Podemos renombrar la imagen utilizando el comando `docker image tag`. Existen dos maneras
   principales de hacerlo:
    - *Asignar un nombre y etiqueta diferente*
    ````bash
    $ docker image tag config-server:latest servidor-configuraciones:v1
    ````
   En este ejemplo, renombramos la imagen `config-server` con la etiqueta `latest` a `servidor-configuraciones` con la
   etiqueta `v1`.<br><br>

    - *Cambiar la versión de la imagen*
    ````bash
    $ docker image tag config-server:latest config-server:1.0.0
    ````
   Aquí, simplemente cambiamos la versión de `latest` a `1.0.0`.


3. `Verificando las Imágenes Renombradas`. Después de realizar los cambios, podemos listar nuevamente las imágenes para
   verificar que la operación se haya realizado correctamente.
    ````bash
    $ docker image ls
    REPOSITORY                 TAG       IMAGE ID       CREATED       SIZE
    config-server              1.0.0     4deeb21c52dd   4 hours ago   366MB
    config-server              latest    4deeb21c52dd   4 hours ago   366MB
    servidor-configuraciones   v1        4deeb21c52dd   4 hours ago   366MB
    ````

Como se observa, las tres imágenes (`config-server:1.0.0`, `config-server:latest`, y `servidor-configuraciones:v1`)
comparten el mismo `IMAGE ID`, ya que todas son basadas en la misma imagen base y no han sufrido modificaciones en su
contenido, solo han cambiado su nombre o versión.

## Eliminando imagen

Primero listamos las imágenes actuales.

````bash
$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED       SIZE
config-server              1.0.0     4deeb21c52dd   4 hours ago   366MB
config-server              latest    4deeb21c52dd   4 hours ago   366MB
servidor-configuraciones   v1        4deeb21c52dd   4 hours ago   366MB
````

Procedemos a eliminar la imagen `servidor-configuraciones` con tag `v1` utilizando el siguiente comando.

````bash
$ docker image rm servidor-configuraciones:v1
Untagged: servidor-configuraciones:v1
````

Volvemos a listar las imágenes y verificamos que la imagen fue eliminada.

````bash
$ docker image ls
REPOSITORY      TAG       IMAGE ID       CREATED       SIZE
config-server   1.0.0     4deeb21c52dd   4 hours ago   366MB
config-server   latest    4deeb21c52dd   4 hours ago   366MB
````

## Eliminando contenedor

Primero listamos los contenedores actuales.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE           COMMAND                  CREATED       STATUS       PORTS                    NAMES
a59c00698b3f   config-server   "/__cacert_entrypoin…"   2 hours ago   Up 2 hours   0.0.0.0:8888->8888/tcp   c-config-server
````

Como el contenedor se encuentra en ejecución (`Up`) primero debemos detenerlo.

````bash
$ docker container stop c-config-server
c-config-server
````

Ahora sí procedemos a eliminar el contenedor utilizando el siguiente comando.

````bash
$ docker container rm c-config-server
c-config-server
````

Si listamos nuevamente los contenedores veremos que la eliminación ha sido exitosa.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
````

## Explorando el contenido del contenedor del Config Server

Primero, listamos los contenedores que se encuentran en ejecución (o detenidos) en nuestra plataforma Docker. Esto nos
permite identificar el nombre del contenedor que deseamos inspeccionar.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS         PORTS                    NAMES
683472a028ac   config-server   "/__cacert_entrypoin…"   11 minutes ago   Up 5 minutes   0.0.0.0:8888->8888/tcp   c-config-server
````

En este caso, el contenedor que nos interesa tiene como nombre `c-config-server`.

A continuación, accedemos de forma interactiva al contenedor utilizando una `shell` mínima (`/bin/sh`).
Una vez dentro del contenedor, nos ubicamos en la ruta `/app`, donde se encuentra desplegada la aplicación. Listamos el
contenido del directorio para verificar los archivos y carpetas disponibles.

````bash
$ docker container exec -it c-config-server /bin/sh
/app # ls -l
total 12
drwxr-xr-x 1 root root 4096 Apr 25 17:35 BOOT-INF
drwxr-xr-x 4 root root 4096 Apr 25 17:35 META-INF
drwxr-xr-x 3 root root 4096 Apr 25 17:35 org
/app #
````

Estos directorios (`BOOT-INF`, `META-INF`, `org`) son característicos de una aplicación `Spring Boot` empacada como
ejecutable `.jar`, lo cual confirma que la aplicación se está ejecutando correctamente dentro del contenedor.

Para aclarar un poco sobre los directorios `BOOT-INF`, `META-INF` y `org`, estos directorios están presentes porque la
imagen `Docker` se ha construido usando el enfoque de capas de `Spring Boot` mediante `layertools`. Este enfoque permite
extraer y copiar de manera explícita las capas internas del `jar`, lo cual favorece la eficiencia en la construcción y
el cacheo de capas.

## Dockerfile para Eureka Server: Construcción de imagen y despliegue del contenedor

En la raíz del `discovery-server` creamos un archivo sin extensión llamado `Dockerfile`. Este archivo contendrá los
mismos comandos que usamos anteriormente en el `Dockerfile` del config server.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

EXPOSE 8761
CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

Mediante la línea de comandos nos posicionamos en la raíz del `discovery-server` donde tenemos nuestro `Dockerfile`.
Luego, ejecutamos el siguiente comando que nos permitirá generar la imagen docker de este microservicio.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\infrastructure\discovery-server (feature/section-12)
$ docker image build -t discovery-server .                                                                                                                
[+] Building 43.2s (24/24) FINISHED                                                                                                                       
 => [internal] load build definition from Dockerfile                                                                                                      
 => => transferring dockerfile: 782B                                                                                                                      
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jre-alpine                                                                          
 => [internal] load metadata for docker.io/library/eclipse-temurin:21-jdk-alpine                                                                          
 => [auth] library/eclipse-temurin:pull token for registry-1.docker.io                                                                                    
 => [internal] load .dockerignore                                                                                                                         
 => => transferring context: 2B                                                                                                                           
 => [builder 1/4] FROM docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd            
 => => resolve docker.io/library/eclipse-temurin:21-jre-alpine@sha256:8728e354e012e18310faa7f364d00185277dec741f4f6d593af6c61fc0eb15fd                    
 => [dependencies 1/9] FROM docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0       
 => => resolve docker.io/library/eclipse-temurin:21-jdk-alpine@sha256:2f2f553ce09d25e2d2f0f521ab94cd73f70c9b21327a29149c23a2b63b8e29a0                    
 => [internal] load build context                                                                                                                         
 => => transferring context: 15.89kB                                                                                                                      
 => CACHED [dependencies 2/9] WORKDIR /app                                                                                                                
 => CACHED [dependencies 3/9] COPY ./mvnw ./                                                                                                              
 => CACHED [dependencies 4/9] COPY ./.mvn ./.mvn                                                                                                          
 => [dependencies 5/9] COPY ./pom.xml ./                                                                                                                  
 => [dependencies 6/9] RUN sed -i -e 's/\r$//' ./mvnw                                                                                                     
 => [dependencies 7/9] RUN ./mvnw dependency:go-offline                                                                                                   
 => [dependencies 8/9] COPY ./src ./src                                                                                                                   
 => [dependencies 9/9] RUN ./mvnw clean package -DskipTests                                                                                               
 => CACHED [builder 2/4] WORKDIR /app                                                                                                                     
 => [builder 3/4] COPY --from=dependencies /app/target/*.jar ./app.jar                                                                                    
 => [builder 4/4] RUN java -Djarmode=layertools -jar app.jar extract                                                                                      
 => [runner 3/6] COPY --from=builder /app/dependencies ./                                                                                                 
 => [runner 4/6] COPY --from=builder /app/spring-boot-loader ./                                                                                           
 => [runner 5/6] COPY --from=builder /app/snapshot-dependencies ./                                                                                        
 => [runner 6/6] COPY --from=builder /app/application ./                                                                                                  
 => exporting to image                                                                                                                                    
 => => exporting layers                                                                                                                                   
 => => exporting manifest sha256:116aac5e68b4c476a72c57476d0a5d2a09314efd6640d2f564818f396b251ab1                                                         
 => => exporting config sha256:8874e0918b17917cb62ac0e1424516e04f1a5be46adbc3a8754f82cad7c10a4d                                                           
 => => exporting attestation manifest sha256:2e565cb5e43b32e8ab95ba5da4dd641ac8f4eba735568ef1bdd00453c152be3e                                             
 => => exporting manifest list sha256:90c55dd49e1283370a196a216b9d3111a0f90cc3919fad01c042d81f297842e0                                                    
 => => naming to docker.io/library/discovery-server:latest                                                                                                
 => => unpacking to docker.io/library/discovery-server:latest                                                                                             
                                                                                                                                                          
View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/3l7y2msljhy5kfbheb2zokjcq                                                
````

Finalizada la construcción, listamos las imágenes para ver el que acabamos de crear.

````bash
$ docker image ls
REPOSITORY         TAG       IMAGE ID       CREATED              SIZE
discovery-server   latest    90c55dd49e12   About a minute ago   391MB
config-server      latest    4deeb21c52dd   5 hours ago          366MB
````

## Levantando contenedor a partir de la imagen Docker del Discovery Server

Una vez construida la imagen Docker del `Discovery Server`, podemos crear y ejecutar un contenedor a partir de ella
utilizando el siguiente comando.

````bash
$ docker container run -d -p 8761:8761 --name c-discovery-server --network microservices-project-net discovery-server
efdda60f4dbb938dfc553c0fa192a5ef21fb5fa94ca1e010ee95a4725eec6eb6
````

**Importante**
> Como anteriormente hemos creado una red personalizada llamada `microservices-project-net` y la hemos asignado al
> contenedor `c-config-server`, es importante que a los nuevos contenedores también le asignemos esa misma red para que
> puedan comunicarse entre sí.

Si ahora listamos los contenedores podremos ver el que acabamos de crear y levantar.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS          PORTS                    NAMES
efdda60f4dbb   discovery-server   "/__cacert_entrypoin…"   4 minutes ago    Up 4 minutes    0.0.0.0:8761->8761/tcp   c-discovery-server
683472a028ac   config-server      "/__cacert_entrypoin…"   49 minutes ago   Up 44 minutes   0.0.0.0:8888->8888/tcp   c-config-server
````

Si hacemos un logs de contenedor `c-discovery-server` veremos que se está ejecutando correctamente.

````bash
docker container logs c-discovery-server

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

2025-04-25T22:58:39.927Z  INFO 1 --- [discovery-server] [           main] d.m.d.s.app.DiscoveryServerApplication   : Starting DiscoveryServerApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app)
2025-04-25T22:58:39.930Z  INFO 1 --- [discovery-server] [           main] d.m.d.s.app.DiscoveryServerApplication   : No active profile set, falling back to 1 default profile: "default"
2025-04-25T22:58:41.192Z  INFO 1 --- [discovery-server] [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=0111aded-c619-3f67-ab04-f0283131fe86
2025-04-25T22:58:41.590Z  INFO 1 --- [discovery-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8761 (http)
2025-04-25T22:58:41.607Z  INFO 1 --- [discovery-server] [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-04-25T22:58:41.608Z  INFO 1 --- [discovery-server] [           main] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
2025-04-25T22:58:41.655Z  INFO 1 --- [discovery-server] [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-04-25T22:58:41.656Z  INFO 1 --- [discovery-server] [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1661 ms
2025-04-25T22:58:42.740Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON encoding codec LegacyJacksonJson
2025-04-25T22:58:42.742Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using JSON decoding codec LegacyJacksonJson
2025-04-25T22:58:42.938Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML encoding codec XStreamXml
2025-04-25T22:58:42.938Z  INFO 1 --- [discovery-server] [           main] c.n.d.provider.DiscoveryJerseyProvider   : Using XML decoding codec XStreamXml
2025-04-25T22:58:43.112Z  INFO 1 --- [discovery-server] [           main] o.s.v.b.OptionalValidatorFactoryBean     : Failed to set up a Bean Validation provider: jakarta.validation.NoProviderFoundException: Unable to create a Configuration, because no Jakarta Bean Validation provider could be found. Add a provider like Hibernate Validator (RI) to your classpath.
2025-04-25T22:58:43.920Z  WARN 1 --- [discovery-server] [           main] iguration$LoadBalancerCaffeineWarnLogger : Spring Cloud LoadBalancer is currently working with the default cache. While this cache implementation is useful for development and tests, it's recommended to use Caffeine cache in production.You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
2025-04-25T22:58:43.948Z  INFO 1 --- [discovery-server] [           main] o.s.c.n.eureka.InstanceInfoFactory       : Setting initial instance status as: STARTING
2025-04-25T22:58:43.975Z  INFO 1 --- [discovery-server] [           main] com.netflix.discovery.DiscoveryClient    : Initializing Eureka in region us-east-1
2025-04-25T22:58:43.975Z  INFO 1 --- [discovery-server] [           main] com.netflix.discovery.DiscoveryClient    : Client configured to neither register nor query for data.
2025-04-25T22:58:43.977Z  INFO 1 --- [discovery-server] [           main] com.netflix.discovery.DiscoveryClient    : Discovery Client initialized at timestamp 1745621923977 with initial instances count: 0
2025-04-25T22:58:44.031Z  INFO 1 --- [discovery-server] [           main] c.n.eureka.DefaultEurekaServerContext    : Initializing ...
2025-04-25T22:58:44.033Z  WARN 1 --- [discovery-server] [           main] c.n.eureka.cluster.PeerEurekaNodes       : The replica size seems to be empty. Check the route 53 DNS Registry
2025-04-25T22:58:44.046Z  INFO 1 --- [discovery-server] [           main] c.n.e.registry.AbstractInstanceRegistry  : Finished initializing remote region registries. All known remote regions: []
2025-04-25T22:58:44.046Z  INFO 1 --- [discovery-server] [           main] c.n.eureka.DefaultEurekaServerContext    : Initialized
2025-04-25T22:58:44.067Z  INFO 1 --- [discovery-server] [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint beneath base path '/actuator'
2025-04-25T22:58:44.142Z  INFO 1 --- [discovery-server] [           main] o.s.c.n.e.s.EurekaServiceRegistry        : Registering application DISCOVERY-SERVER with eureka with status UP
2025-04-25T22:58:44.157Z  INFO 1 --- [discovery-server] [       Thread-9] o.s.c.n.e.server.EurekaServerBootstrap   : isAws returned false
2025-04-25T22:58:44.158Z  INFO 1 --- [discovery-server] [       Thread-9] o.s.c.n.e.server.EurekaServerBootstrap   : Initialized server context
2025-04-25T22:58:44.159Z  INFO 1 --- [discovery-server] [       Thread-9] c.n.e.r.PeerAwareInstanceRegistryImpl    : Got 1 instances from neighboring DS node
2025-04-25T22:58:44.159Z  INFO 1 --- [discovery-server] [       Thread-9] c.n.e.r.PeerAwareInstanceRegistryImpl    : Renew threshold is: 1
2025-04-25T22:58:44.159Z  INFO 1 --- [discovery-server] [       Thread-9] c.n.e.r.PeerAwareInstanceRegistryImpl    : Changing status to UP
2025-04-25T22:58:44.163Z  INFO 1 --- [discovery-server] [       Thread-9] e.s.EurekaServerInitializerConfiguration : Started Eureka Server
2025-04-25T22:58:44.187Z  INFO 1 --- [discovery-server] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8761 (http) with context path '/'
2025-04-25T22:58:44.188Z  INFO 1 --- [discovery-server] [           main] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 8761
2025-04-25T22:58:44.210Z  INFO 1 --- [discovery-server] [           main] d.m.d.s.app.DiscoveryServerApplication   : Started DiscoveryServerApplication in 4.808 seconds (process running for 5.492)
2025-04-25T22:59:44.160Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 0ms
2025-04-25T23:00:44.182Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1404ms
2025-04-25T23:01:44.182Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1520ms
2025-04-25T23:02:44.182Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1508ms
2025-04-25T23:03:44.186Z  INFO 1 --- [discovery-server] [a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry  : Running the evict task with compensationTime 1383ms
````

Si ahora accedemos desde nuestro navegador hacia la dirección `http://localhost:8761/` veremos que nuestro servidor de
`eureka` se ha creado correctamente y está en ejecución.

![01.png](assets/section-12/01.png)

## Descargando imagen Docker para MySQL y PostgreSQL

Para descargar las imágenes `Docker` nos vamos a la página de [hub.docker](https://hub.docker.com/) donde se encuentran
las imágenes oficiales de los distintos productos de software.

En la imagen siguiente se muestra cómo es que buscamos la imagen de `MySQL` y el tag `8.0.41`. Una vez que encontremos
la versión que necesitamos copiamos el comando de descarga para tener la imagen en nuestro local.

![02.png](assets/section-12/02.png)

Utilizando el comando `docker pull` podemos descargar la imagen desde `docker hub`, tal como se muestra a continuación.

````bash
$ docker pull mysql:8.0.41-debian
8.0.41-debian: Pulling from library/mysql
80e697a2a38a: Pull complete
e2d15768547d: Pull complete
8a628cdd7ccc: Pull complete
3d80c213d73c: Pull complete
cdc6bb1849f7: Pull complete
fcc101123521: Pull complete
eccb44e37d22: Pull complete
dd34483b65e5: Pull complete
5bac1e6ff6c3: Pull complete
156e4bd92e5c: Pull complete
51f55966c0b7: Pull complete
f50c587f3059: Pull complete
Digest: sha256:b2252987e0ecdb820e96928948ac3bca1adcd2b4a2a2c7b0d7ea78f77a9dc6ac
Status: Downloaded newer image for mysql:8.0.41-debian
docker.io/library/mysql:8.0.41-debian
````

**Dónde**

- `docker pull`, comando utilizado para descargar una imagen desde un repositorio remoto (por defecto, `Docker Hub`).
- `mysql`, nombre de la `imagen` que se desea descargar.
- `8.0.41-debian`, `tag` que especifica la versión de la imagen, en este caso `MySQL 8.0.41` basada en `Debian`.

Haremos lo mismo para descargar la imagen de `PostgreSQL`.

````bash
$ docker pull postgres:17-alpine
17-alpine: Pulling from library/postgres
8f4971a5dfe7: Pull complete
215ba3ecdc26: Pull complete
d8d8fb695a5a: Pull complete
83efd74bc97e: Pull complete
4f18957d9158: Pull complete
c24c1ba610df: Pull complete
15ca4c67ed92: Pull complete
9e4acb9ca7d3: Pull complete
404c53e09e31: Pull complete
Digest: sha256:7062a2109c4b51f3c792c7ea01e83ed12ef9a980886e3b3d380a7d2e5f6ce3f5
Status: Downloaded newer image for postgres:17-alpine
docker.io/library/postgres:17-alpine
````

Finalizada las descargas, listamos las imágenes para ver que la imagen de `MySQL` y la de `PostgreSQL` fueron
descargadas correctamente.

````bash
$ docker image ls
REPOSITORY         TAG             IMAGE ID       CREATED        SIZE
discovery-server   latest          90c55dd49e12   2 days ago     391MB
config-server      latest          4deeb21c52dd   2 days ago     366MB
postgres           17-alpine       7062a2109c4b   2 months ago   398MB
mysql              8.0.41-debian   b2252987e0ec   3 months ago   812MB
````

**Nota**
> **No es estrictamente necesario descargar primero la imagen mediante `docker pull`.**
> Si se conoce el `nombre de la imagen` y el `tag`, se puede ejecutar directamente el comando `docker container run` o
> `docker run` para crear un contenedor. Si la imagen no se encuentra disponible en el entorno local, `Docker`
> automáticamente la descargará antes de crear el contenedor.

## Creando contenedor Docker de MySQL

Primero creamos el contenedor para `MySQL` ejecutando el siguiente comando.

````bash
$ docker container run -d -p 3307:3306 --name c-mysql -e MYSQL_DATABASE=db_product_service -e MYSQL_ROOT_PASSWORD=magadiflo --network microservices-project-net mysql:8.0.41-debian
a60e121908c8f16bf1544e1d5fbc4a3391a461d34abe663ef421a8f0a4766d37
````

**Donde**

- `docker container run`, comando utilizado para crear y ejecutar un nuevo contenedor a partir de una imagen.
- `-d`, ejecuta el contenedor en segundo plano (modo `detached`).
- `-p 3307:3306`, mapea el puerto `3306` del contenedor (`MySQL`) al puerto `3307` del host `(pc local)`.
- `--name c-mysql`, asigna el nombre `c-mysql` al contenedor para su identificación.
- `-e MYSQL_DATABASE=db_product_service`, define una variable de entorno para que al iniciar, el contenedor cree
  automáticamente una base de datos llamada `db_product_service`.
- `-e MYSQL_ROOT_PASSWORD=magadiflo`, define la contraseña del usuario `root` de `MySQL` dentro del contenedor.
- `--network microservices-project-net`, conecta el contenedor a una red personalizada de `Docker` llamada
  `microservices-project-net`, útil para la comunicación entre microservicios.

**Nota**

- `Creación automática de la imagen si no está en local`, si la imagen `mysql:8.0.41-debian` no está descargada en el
  entorno local, `Docker` la descargará automáticamente antes de crear el contenedor.


- `Importante sobre puertos`, se definió el puerto `3307` como puerto externo en el host `(pc local)` debido a que ya
  existe una instalación de `MySQL` localmente. Para evitar conflictos de puertos, se asignó un puerto diferente y
  disponible al contenedor. Con respecto al puerto del lado derecho de la instrucción `-p` (`3306`), corresponde al
  puerto interno del contenedor, que es el puerto por defecto donde el servicio de `MySQL` escucha
  `dentro del contenedor`.

Si listamos los contenedores vemos que el que acabamos de crear está levantado.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS                        PORTS                               NAMES
a60e121908c8   mysql:8.0.41-debian   "docker-entrypoint.s…"   26 minutes ago   Up 26 minutes                 33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server      "/__cacert_entrypoin…"   2 days ago       Exited (255) 57 minutes ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server         "/__cacert_entrypoin…"   2 days ago       Exited (255) 57 minutes ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Para verificar su funcionamiento, podemos ingresar dentro del contenedor de `MySQL` ejecutando el siguiente comando.

````bash
$ docker container exec -it c-mysql /bin/sh
# mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.41 MySQL Community Server - GPL

Copyright (c) 2000, 2025, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| db_product_service |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)

mysql>
````

**Dónde**

- `docker container exec`, comando que permite ejecutar una instrucción dentro de un contenedor en ejecución.
- `-i`, modo interactivo. Mantiene la sesión abierta para recibir entradas del usuario.
- `-t`, asocia un pseudo-TTY, proporcionando un terminal interactivo dentro del contenedor.
- `c-mysql`, nombre del contenedor donde se desea ejecutar el comando.
- `/bin/sh`, especifica el programa a ejecutar dentro del contenedor. En este caso, se abre una shell (sh) básica para
  interactuar con el sistema de archivos del contenedor.

Ahora, para acceder a la consola de `MySQL` desde ahí, debemos ejecutar este comando dentro del contenedor.

````bash
$ mysql -u root -p
````

**Dónde**

- `mysql`, ejecuta el cliente de línea de comandos de `MySQL`.
- `-u root`, indica que quieres conectarte como el usuario `root`.
- `-p`, indica que te pedirá la contraseña.

Como hemos configurado el puerto `3307` en nuestro host (`pc local`), podemos conectarnos a la base de datos del
contenedor utilizando herramientas como `DBeaver`.

![03.png](assets/section-12/03.png)

## Creando contenedor Docker de Postgres

Creamos el contenedor para `Postgres` ejecutando el siguiente comando.

````bash
$ docker container run -d -p 5433:5432 --name c-postgres -e POSTGRES_DB=db_user_service -e POSTGRES_PASSWORD=magadiflo --network microservices-project-net postgres:17-alpine
d69f3ce25cfd8e8e3d95273fb39716d99b28cb6b0c985da6b680f3e5fe32aa04
````

**Dónde**

- `docker container run`, crea y ejecuta un nuevo contenedor.
- `-d`, modo `detached`. El contenedor se ejecuta en segundo plano.
- `-p 5433:5432`.
    - `5433`, puerto expuesto en el host (`pc local`).
    - `5432`, puerto `interno del contenedor` (puerto por defecto de `PostgreSQL`).
- `--name c-postgres`, nombre asignado al contenedor (`c-postgres`).
- `-e POSTGRES_DB=db_user_service`, variable de entorno para crear automáticamente la base de datos `db_user_service` al
  iniciar `PostgreSQL`.
- `-e POSTGRES_PASSWORD=magadiflo`, variable de entorno para establecer la contraseña del usuario `postgres`.
- `--network microservices-project-net`, conecta el contenedor a una red `Docker` personalizada llamada
  `microservices-project-net`.
- `postgres:17-alpine`, imagen utilizada. Es la `versión 17` de `PostgreSQL` basada en alpine (una imagen ligera).

Si listamos los contenedores vemos que el que acabamos de crear está levantado.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                 COMMAND                  CREATED             STATUS                     PORTS                               NAMES
d69f3ce25cfd   postgres:17-alpine    "docker-entrypoint.s…"   4 minutes ago       Up 4 minutes               0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian   "docker-entrypoint.s…"   About an hour ago   Up About an hour           33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server      "/__cacert_entrypoin…"   2 days ago          Exited (255) 2 hours ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server         "/__cacert_entrypoin…"   2 days ago          Exited (255) 2 hours ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Para verificar su funcionamiento, podemos ingresar dentro del contenedor de `Postgres` ejecutando el siguiente comando.

````bash
$ docker container exec -it c-postgres /bin/bash
d69f3ce25cfd:/# psql -U postgres -d db_user_service
psql (17.4)
Type "help" for help.

db_user_service=# \l
                                                       List of databases
      Name       |  Owner   | Encoding | Locale Provider |  Collate   |   Ctype    | Locale | ICU Rules |   Access privileges
-----------------+----------+----------+-----------------+------------+------------+--------+-----------+-----------------------
 db_user_service | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           |
 postgres        | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           |
 template0       | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/postgres          +
                 |          |          |                 |            |            |        |           | postgres=CTc/postgres
 template1       | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/postgres          +
                 |          |          |                 |            |            |        |           | postgres=CTc/postgres
(4 rows)

db_user_service=#
````

Como hemos configurado el puerto `5433` en nuestro host (`pc local`), podemos conectarnos a la base de datos del
contenedor utilizando herramientas como `DBeaver`.

![04.png](assets/section-12/04.png)

## Persistencia de datos en el contenedor MySQL para Zipkin

Para garantizar que los datos de la base de datos no se pierdan al `reiniciar` o `eliminar` el contenedor, es necesario
utilizar un `volumen persistente` de Docker. Este `volumen` almacena los datos fuera del contenedor, permitiendo que
sobrevivan a reinicios o recreaciones del mismo.

Creamos un volumen con el siguiente comando.

````bash
$ docker volume create mysql-zipkin-data
mysql-zipkin-data
````

- Los datos se almacenarán en el volumen `mysql-zipkin-data`, que puede ser asociado a cualquier contenedor `MySQL`.
- Aunque el contenedor se detenga o elimine, mientras no se elimine el `volumen`, los datos permanecerán intactos.
- Podemos crear otro contenedor `MySQL` apuntando a este `volumen` y seguirá usando los mismos datos.

Si ahora listamos los volúmenes, podemos ver el que acabamos de crear.

````bash
$ docker volume ls
DRIVER    VOLUME NAME
local     mysql-zipkin-data
````

## Creando contenedor docker de MySQL para Zipkin y configurando esquema DDL

En el `capítulo 11` vimos cómo integrar `MySQL` como sistema de almacenamiento para `Zipkin`.
En este apartado continuaremos utilizando `MySQL` como almacenamiento de `Zipkin`, pero esta vez estará contenarizado.

Para ello, crearemos un contenedor de `MySQL` específicamente para `Zipkin`. Ejecutamos el siguiente comando:

````bash
$ docker container run -d -p 3308:3306 --name c-mysql-zipkin -e MYSQL_DATABASE=db_zipkin -e MYSQL_ROOT_PASSWORD=magadiflo -e MYSQL_USER=zipkin -e MYSQL_PASSWORD=zipkin -v mysql-zipkin-data:/var/lib/mysql --network microservices-project-net mysql:8.0.41-debian
cb64115ce63b5931c70826ed8cd67a17b95c1dad5e3288f5a468c9a4221a0ca5
````

**Nota**

- `docker container run`, Comando para crear y ejecutar un contenedor.
- `-d`, ejecutar en modo `detached` (segundo plano).
- `-p 3308:3306`
    - `3308`, puerto del host (`pc local`).
    - `3306`, puerto interno del contenedor (puerto estándar de `MySQL`).
- `--name c-mysql-zipkin`, nombre asignado al contenedor (`c-mysql-zipkin`).
- `-e MYSQL_DATABASE=db_zipkin`, variable de entorno que indica que se creará automáticamente una base de datos llamada
  `db_zipkin`.
- `-e MYSQL_ROOT_PASSWORD=magadiflo`, variable de entorno que define la contraseña del usuario `root`.
- `-e MYSQL_USER=zipkin`, variable de entorno que define un usuario adicional llamado `zipkin`.
- `-e MYSQL_PASSWORD=zipkin`, contraseña asociada al usuario `zipkin`.
- `-v mysql-zipkin-data:/var/lib/mysql`, permite montar un volumen de docker dentro del contenedor para garantizar la
  persistencia de los datos.
    - `mysql-zipkin-data`, es el nombre del volumen de docker que previamente creamos con `docker volume create`.
    - `/var/lib/mysql`, es la ruta dentro del contenedor donde `MySQL` guarda todos sus datos (bases de datos, tablas,
      índices, etc.).
- `--network microservices-project-net`, se conecta el contenedor a la red `Docker` personalizada
  `microservices-project-net`.
- `mysql:8.0.41-debian`, imagen utilizada, específica de `MySQL versión 8.0.41` basada en `Debian`.

**Importante**

- `Usuario y contraseña personalizados`
    - La imagen oficial de MySQL permite crear un usuario adicional (en este caso zipkin) mediante las variables
      `MYSQL_USER` y `MYSQL_PASSWORD`.
    - Este usuario tiene acceso a la base de datos indicada en `MYSQL_DATABASE` (db_zipkin).
    - La configuración de `MYSQL_ROOT_PASSWORD` `es obligatoria` aunque uses un usuario personalizado, porque la imagen
      necesita definir siempre la contraseña del usuario root al iniciarse.

- Si no se especifica un volumen, los datos almacenados en la base de datos se perderán al eliminar el contenedor, ya
  que `MySQL` guarda sus datos en el sistema de archivos interno del contenedor, el cual se destruye al eliminarlo.

Listamos los contenedores y verificamos que el `c-mysql-zipkin` se ha creado correctamente.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS                     PORTS                               NAMES
cb64115ce63b   mysql:8.0.41-debian    "docker-entrypoint.s…"   17 seconds ago   Up 17 seconds             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine    "docker-entrypoint.s…"   47 minutes ago   Up 47 minutes              0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian   "docker-entrypoint.s…"   2 hours ago      Up 2 hours                 33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server      "/__cacert_entrypoin…"   2 days ago       Exited (255) 2 hours ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server         "/__cacert_entrypoin…"   2 days ago       Exited (255) 2 hours ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Ingresamos dentro del contenedor para verificar nuestra base de datos.

````bash
$ docker container exec -it c-mysql-zipkin /bin/sh
# mysql -u zipkin -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.41 MySQL Community Server - GPL

Copyright (c) 2000, 2025, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> SHOW DATABASES;
+--------------------+
| Database           |
+--------------------+
| db_zipkin          |
| information_schema |
| performance_schema |
+--------------------+
3 rows in set (0.00 sec)

mysql> USE db_zipkin;
Database changed
mysql> SHOW TABLES;
Empty set (0.00 sec)

mysql>
````

Utilizando `DBeaver` creamos una conexión a la base de datos contenerizada de `zipkin`, aquí estamos usando el puerto de
nuestro host (pc local) `3308`.

![05.png](assets/section-12/05.png)

En el `capítulo 11` ejecutamos un script en nuestro `mysql` de nuestra `pc local` para crear las tablas que usará
`zipkin`. En este capítulo también debemos ejecutar ese mismo script para crear las tablas pero en nuestra base de
datos contenerizada. Podemos apoyarnos de `DBeaver` para poder acceder a la base de datos que acabamos de contenerizar
`c-mysql-zipkin`.

![06.png](assets/section-12/06.png)

## Creando contenedor docker de Zipkin Server conectado a contenedor de MySQL

Anteriormente, habíamos descargado el `jar` de `zipkin` y en un archivo `start-zipkin.bat` de windows habíamos colocado
las variables de entorno para conectarnos a `mysql` y al final habíamos colocado el comando para ejecutar el `jar`.

En este apartado levantaremos el `Zipkin Server` a través de un contenedor de Docker y lo integraremos con el
contenedor que habíamos creado anteriormente `c-mysql-zipkin`. Para eso, simplemente ejecutamos el siguiente comando.

````bash
$ docker container run -d -p 9411:9411 --name c-zipkin -e STORAGE_TYPE=mysql -e MYSQL_HOST=c-mysql-zipkin -e MYSQL_DB=db_zipkin -e MYSQL_USER=zipkin -e MYSQL_PASS=zipkin --network microservices-project-net openzipkin/zipkin:3.5.1
Unable to find image 'openzipkin/zipkin:3.5.1' locally
3.5.1: Pulling from openzipkin/zipkin
d08e7a152654: Pull complete
f338522607a6: Pull complete
9024432539a6: Pull complete
217376793de7: Pull complete
c23605470ecb: Pull complete
0724a0c030ee: Pull complete
a45115d6642f: Pull complete
a2b7dbaa785d: Pull complete
4125d9c649e6: Pull complete
Digest: sha256:bb570eb45c2994eaf32da783cc098b3d51d1095b73ec92919863d73d0a9eaafb
Status: Downloaded newer image for openzipkin/zipkin:3.5.1
dd4f558582586ea8f79f9203e76c79220b2bae2708fc5b2d18bad2818f8892bf
````

**Dónde**

- `docker container run`, comando para crear y ejecutar un nuevo contenedor.
- `-d`, opción que indica que el contenedor se ejecutará en segundo plano (modo `detached`).
- `-p 9411:9411`, mapea el puerto `9411` del contenedor al puerto `9411` de la `pc local (host)`.
- `--name c-zipkin`, asigna el nombre `c-zipkin` al contenedor.
- `-e STORAGE_TYPE=mysql`, define que `Zipkin` usará `MySQL` como sistema de almacenamiento de trazas.
- `-e MYSQL_HOST=c-mysql-zipkin`, indica el nombre del contenedor de `MySQL` al que debe conectarse.
- `-e MYSQL_DB=db_zipkin`, nombre de la base de datos que `Zipkin` usará dentro de `MySQL`.
- `-e MYSQL_USER=zipkin`, usuario de la base de datos configurado en `MySQL`.
- `-e MYSQL_PASS=zipkin`, contraseña del usuario configurado en `MySQL`.
- `--network microservices-project-net`, conecta el contenedor a la red personalizada donde también está el contenedor
  de `MySQL`.
- `openzipkin/zipkin:3.5.1`: imagen oficial de `Zipkin` (no slim), `versión 3.5.1`.

**Nota importante sobre la imagen**

- Al no tener la imagen `openzipkin/zipkin:3.5.1` descargada localmente, `Docker` automáticamente la descarga desde el
  repositorio remoto cuando se ejecuta el comando `docker container run`.
- No es necesario descargar previamente la imagen usando `docker pull`, aunque también es una opción si se quiere tener
  la imagen disponible antes de crear el contenedor.

Si listamos los contenedores observamos que nuestro `c-zipkin` se está ejecutando sin problemas.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                     PORTS                               NAMES
dd4f55858258   openzipkin/zipkin:3.5.1   "start-zipkin"           42 seconds ago   Up 39 seconds (healthy)    9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   3 hours ago      Up 25 minutes              33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   4 hours ago      Up 25 minutes              0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   5 hours ago      Up 25 minutes              33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   2 days ago       Exited (255) 5 hours ago   0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   2 days ago       Exited (255) 5 hours ago   0.0.0.0:8888->8888/tcp              c-config-server
````

Si listamos las imágenes, podemos ver que Docker descargó por nosotros la imagen `openzipkin/zipkin:3.5.1` que usamos
para crear el contenedor anterior.

````bash
$ docker image ls
REPOSITORY          TAG             IMAGE ID       CREATED        SIZE
openzipkin/zipkin   3.5.1           bb570eb45c29   24 hours ago   379MB
discovery-server    latest          90c55dd49e12   2 days ago     391MB
config-server       latest          4deeb21c52dd   3 days ago     366MB
postgres            17-alpine       7062a2109c4b   2 months ago   398MB
mysql               8.0.41-debian   b2252987e0ec   3 months ago   812MB
````

Ingresamos al dashboard de `Zipkin Server` a través de la dirección `http://127.0.0.1:9411/`.

![07.png](assets/section-12/07.png)

Como ha sido la primera vez que ingresamos al dashboard de `Zipkin`, nos aparece en la consola que nuestro
`Zipkin Server` está conectado a `MySQL 8.0.41`.

````bash
$ docker container logs c-zipkin

                  oo
                 oooo
                oooooo
               oooooooo
              oooooooooo
             oooooooooooo
           ooooooo  ooooooo
          oooooo     ooooooo
         oooooo       ooooooo
        oooooo   o  o   oooooo
       oooooo   oo  oo   oooooo
     ooooooo  oooo  oooo  ooooooo
    oooooo   ooooo  ooooo  ooooooo
   oooooo   oooooo  oooooo  ooooooo
  oooooooo      oo  oo      oooooooo
  ooooooooooooo oo  oo ooooooooooooo
      oooooooooooo  oooooooooooo
          oooooooo  oooooooo
              oooo  oooo

     ________ ____  _  _____ _   _
    |__  /_ _|  _ \| |/ /_ _| \ | |
      / / | || |_) | ' / | ||  \| |
     / /_ | ||  __/| . \ | || |\  |
    |____|___|_|   |_|\_\___|_| \_|

:: version 3.5.1 :: commit b7e2258 ::

2025-04-28T20:20:00.058Z  WARN [/] 1 --- [           main] i.m.p.PrometheusMeterRegistry            : A MeterFilter is being configured after a Meter has been registered to this registry. All MeterFilters should be configured before any Meters are registered. If that is not possible or you have a use case where it should be allowed, let the Micrometer maintainers know at https://github.com/micrometer-metrics/micrometer/issues/4920. Enable DEBUG level logging on this logger to see a stack trace of the call configuring this MeterFilter.
2025-04-28T20:20:00.401Z  INFO [/] 1 --- [oss-http-*:9411] c.l.a.s.Server                           : Serving HTTP at /[0:0:0:0:0:0:0:0]:9411 - http://127.0.0.1:9411/
2025-04-28T20:20:02.278Z  INFO [/] 1 --- [cking-tasks-1-1] c.z.h.p.HikariPool                       : HikariPool-1 - Added connection org.mariadb.jdbc.Connection@4f06c23f
2025-04-28T20:20:02.879Z  INFO [/] 1 --- [cking-tasks-1-1] o.j.Constants                            :

jOOQ tip of the day: Want to upgrade your jOOQ experience? Check out these commercial only features of the jOOQ Professional and Enterprise Editions: https://www.jooq.org/commercial-only-features/

2025-04-28T20:20:02.910Z  INFO [/] 1 --- [cking-tasks-1-1] o.j.i.D.logVersionSupport                : Version                  : Database version is supported by dialect MYSQL: 8.0.41
````

## Configurando URL de Eureka, Config Server y Zipkin en microservicios

Vamos a realizar modificaciones a las configuraciones de los microservicios para que apunten a trabajar con
contenedores.

Modificamos el `product-service`.

````yml
spring:
  application:
    name: product-service
  datasource:
    url: jdbc:mysql://c-mysql:3306/db_product_service?serverTimezone=America/Lima

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

**Dónde**

- `c-mysql` es el nombre del contenedor que usará este microservicio y donde hemos creado la `db_product_service`.
- Dentro de una red Docker (`microservices-project-net` en tu caso), los contenedores se comunican usando sus nombres
  como si fueran `hostnames`.
- El puerto interno del contenedor `c-mysql` es `3306`, así que está bien definida en nuestra configuración.
- `c-discovery-server`, es el nombre del contenedor de nuestro `Discovery Server`. Mismo principio: la comunicación en
  Docker se hace por el nombre del contenedor. Puerto `8761` también corresponde al `puerto interno` del contenedor.
- `c-zipkin` es nuestro contenedor de `Zipkin`. Puerto interno del contenedor: `9411`.

Modificamos el `item-service`.

````yml
spring:
  application:
    name: item-service
  config:
    import: optional:configserver:http://c-config-server:8888

management:
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/
````

Modificamos el `user-service`.

````yml
spring:
  application:
    name: user-service
  config:
    import: optional:configserver:http://c-config-server:8888
  datasource:
    url: jdbc:postgresql://c-postgres:5432/db_user_service

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  tracing:
    sampling:
      probability: 1.0
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

Modificamos el `authorization-server`.

````yml
eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  tracing:
    sampling:
      probability: 1.0
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans

custom:
  auth-server:
    issuer: http://c-authorization-server:9000
````

Si observamos los cambios anteriores, podemos ver que también hemos agregado una configuración personalizada
`custom.auth-server.issuer` cuyo valor asume que el contenedor del `authorization-server` tiene el nombre
`c-authorization-server` en la red Docker `microservices-project-net`.

La configuración `custom.auth-server.issuer` lo inyectaremos en el método `authorizationServerSettings()` de la clase
`SecurityConfig` de nuestro `authorization-server`.

````java

@RequiredArgsConstructor
@Configuration
public class SecurityConfig {
    /* code */
    @Bean
    public AuthorizationServerSettings authorizationServerSettings(@Value("${custom.auth-server.issuer}") String authServerIssuer) {
        return AuthorizationServerSettings.builder().issuer(authServerIssuer).build();
    }
    /* code */
}
````

El `.issuer(...)`, define el `Issuer URI`, es decir, la `identidad del servidor de autorización`. Este valor se
incluye en los tokens emitidos (como el `iss en el JWT`) y **debe coincidir con el que esperan los recursos
protegidos.** Esto es fundamental para validar correctamente los `tokens JWT emitidos` por este servidor.

#### ¿Cuál es el punto importante a considerar?

Cuando todo esté ejecutándose en contenedores `Docker`, debemos asegurarnos de que la URL del `issuer` se adapte al
entorno de contenedores. Aquí tenemos dos consideraciones clave:

1. `Contenedores en Docker`: Si los contenedores están en la misma red (`microservices-project-net`), la URL de `issuer`
   debería referirse a un nombre de contenedor o una IP dentro de la red Docker, no a `localhost` o `127.0.0.1`.

2. `Interacción con el Gateway`: Cuando el `gateway-server` hace las peticiones de autorización a este
   `authorization-server`, `localhost` o `127.0.0.1` no funcionarán, porque dentro de un contenedor, `localhost` se
   refiere al contenedor mismo. Necesitaremos usar algo como `c-authorization-server` (el nombre de contenedor) en lugar
   de `127.0.0.1`.

Cuando los contenedores están en la misma red Docker, Docker crea una red interna entre ellos. Dentro de esta red, los
contenedores pueden resolver sus nombres de host. Esto significa que:

- `c-authorization-server` será el nombre de host dentro de la red Docker, y Docker se encargará de resolverlo a la IP
  interna del contenedor correspondiente.

- No necesitamos preocuparnos directamente por la IP. Usando `c-authorization-server` como nombre de contenedor, Docker
  manejará automáticamente la resolución de este nombre a la IP correcta en su red interna.

Es decir, cuando el `gateway-server` u otros microservicios intentan acceder a `http://c-authorization-server:9000`,
Docker se encarga de traducir `c-authorization-server` al IP del contenedor correspondiente.

#### Impacto en el JWT

- `JWT`: El valor del atributo `iss` en el JWT, que es lo que el `authorization-server` pondrá en los tokens generados,
  será `http://c-authorization-server:9000`, como se configuró en el bean `AuthorizationServerSettings`.

- `Distribución y Resolución`: El valor de `iss` es importante cuando otros servicios o clientes verifican la validez de
  los tokens. En nuestro caso, cuando el `gateway-server` o cualquier otro servicio recibe un `JWT`, puede comprobar que
  el `iss` concuerda con el valor de su servidor de autorización.

Modificamos el `gateway-server`.

````yml
spring:
  application:
    name: gateway-server
  security:
    oauth2:
      resourceserver:
        jwt:
          issuer-uri: http://c-authorization-server:9000
      client:
        provider:
          my-auth-server:
            issuer-uri: http://c-authorization-server:9000

eureka:
  client:
    service-url:
      defaultZone: http://c-discovery-server:8761/eureka/

management:
  zipkin:
    tracing:
      endpoint: http://c-zipkin:9411/api/v2/spans
````

En la configuración anterior, inicialmente cuando no trabajamos con contenedores teníamos configurado el `issuer-uri`
con valor `http://127.0.0.1:9000` porque el `gateway-server` y el `authorization-server` estaban en la misma máquina.
Sin embargo, en un entorno de contenedores `Docker`, el `gateway-server` no podrá resolver `127.0.0.1` cuando se
ejecute en contenedores separados. En lugar de `127.0.0.1`, deberíamos usar el nombre del contenedor del
`authorization-server` (`c-authorization-server`).

- `issuer-uri`: Cuando estás trabajando con contenedores Docker, los contenedores pueden resolver los nombres de otros
  contenedores dentro de la misma red. Usar `c-authorization-server` como el `issuer-uri` asegura que el
  `gateway-server` pueda acceder al `authorization-server` en el contenedor correspondiente.


- `Interacción Cliente y Servidor`: Al hacer estas modificaciones, el `gateway-server` podrá actuar como
  `cliente de OAuth2` (autenticación) y también como `servidor de recursos` (verificación de JWT).

Recordar que en las configuraciones del `gateway-server` tenemos rutas a donde nuestro `gateway` redirecciona. En estas
rutas nosotros hemos definido como `uri` el nombre del microservicio con el prefijo `lb://` similar a cómo se muestra.

````bash
uri: lb://product-service
uri: lb://item-service
uri: lb://user-service
uri: lb://authorization-server
````

Aquí es muy importante tener en cuenta que estos nombres son nombres de los microservicios definidos en
`spring.application.name` y no es el nombre de los contenedores que crearemos más adelante como `c-product-service` o
`c-item-service`, etc.

Cuando ponemos `lb://`, el `Gateway` va a preguntarle a `Eureka ("c-discovery-server")` en qué contenedor está corriendo
el microservicio por su `spring.application.name`.

## Creando Dockerfile para servicio productos, build y run

Vamos a crear el archivo `Dockerfile` en la raíz del microservicio `product-service` y colocar las instrucciones que se
muestran a continuación.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

**!Observación importante¡**

> Si nuestro microservicio `Spring Boot` está configurado con `server.port=0` para usar un puerto aleatorio en cada
> instancia, `NO debemos definir un EXPOSE fijo` en el `Dockerfile`.
>
> Dejar el EXPOSE fijo, como por ejemplo `EXPOSE 8888` hace que `Docker` espere un servicio en ese puerto, causando
> problemas de conexión. Simplemente eliminemos el `EXPOSE` y permitamos que `Eureka` maneje la ubicación `(host:port)`
> de cada instancia correctamente.
>
> En nuestro caso, el microservicio `product-service` tiene configurado `server.port=0`, lo que significa que Spring
> Boot generará un puerto aleatorio en cada instancia.
>
> Por este motivo, en el `Dockerfile` no debemos usar `EXPOSE <puerto>`, porque el puerto interno real de la aplicación
> será aleatorio, no fijo. Si colocáramos, por ejemplo, `EXPOSE 8888`, `Docker` mapearía el puerto aleatorio del host
> hacia el puerto `8888` interno, pero Spring Boot podría estar escuchando en otro puerto diferente, causando problemas
> de conexión. Esto solo ocurre cuando usamos la bandera `-P (mayúscula)`, pero si usamos la `-p (minúscula)`, aquí
> el `EXPOSE` solo funciona como documentación.
>
> Además:
>
> Si usamos el parámetro `-p (minúscula)` en `docker container run` (es decir, hacemos un mapeo manual
> `-p HOST:CONTENEDOR`), el `EXPOSE` solo servirá como documentación y no tendrá un efecto práctico.
>
> Pero si usamos el parámetro `-P (mayúscula)`, `Docker` nos permitirá seleccionar un puerto aleatorio del
> `host (pc local)` y se redirigirá al puerto o puertos que estén definidos en el `EXPOSE`.
>
> Por lo tanto, si tu microservicio Spring Boot genera puertos aleatorios (`server.port=0`), es mejor no definir
> `EXPOSE` en el `Dockerfile` para evitar inconsistencias en el mapeo de puertos.

Luego, mediante la terminal nos ubicamos en la raíz del microservicio donde está el `Dockerfile` y ejecutamos el
siguiente comando para construir la imagen que tendrá el mismo nombre que nuestro microservicio `product-service`.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\business-domain\product-service (feature/section-12)
$ docker image build -t product-service .
````

Posteriormente, procedemos a construir un contenedor a partir de la imagen `product-service` construida anteriormente.

````bash
$ docker container run -d -P --name c-product-service --network microservices-project-net product-service
761e8e5d7932cd30cbc58982a61d59ca0133945a005162d93f859fa088bee25e
````

**Dónde**

- El parámetro `-P` (`mayúscula`) `publica automáticamente` todos los puertos expuestos en el contenedor en
  `puertos aleatorios` de nuestra máquina anfitriona (host).
- `Docker` tomará el puerto interno aleatorio que use nuestro microservicio y lo mapeará automáticamente a un puerto
  disponible del host.

Listamos los contenedores que tenemos hasta ahora.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                 PORTS                               NAMES
761e8e5d7932   product-service           "/__cacert_entrypoin…"   17 minutes ago   Up 17 minutes                                              c-product-service
dd4f55858258   openzipkin/zipkin:3.5.1   "start-zipkin"           3 hours ago      Up 3 hours (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   6 hours ago      Up 4 hours             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   7 hours ago      Up 4 hours             0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   8 hours ago      Up 4 hours             33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   3 days ago       Up 2 hours             0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   3 days ago       Up 2 hours             0.0.0.0:8888->8888/tcp              c-config-server
````

Cuando ejecutamos el contenedor `c-product-service` con la opción `-P (mayúscula)`, esperábamos que `Docker` mapee
automáticamente un puerto aleatorio del host hacia un puerto interno del contenedor.

Sin embargo, al listar los contenedores (`docker container ls -a`), observamos que la columna `PORTS` aparece vacía, es
decir, no hay ningún puerto mapeado.

La razón es la siguiente:

- La opción `-P (mayúscula)` de `Docker` depende del `EXPOSE` definido en el `Dockerfile` de la imagen.
- El `EXPOSE` indica a `Docker` qué puertos internos del contenedor están disponibles para ser expuestos/mapeados
  automáticamente.
- En este caso, como en el `Dockerfile` de `product-service` `NO` se definimos ningún `EXPOSE`,
  `Docker` no sabe qué puerto interno debería mapear, aunque usemos `-P`.
- Como resultado, `Docker` no mapea ningún puerto del contenedor hacia el host, y por eso no vemos puertos en la salida
  del comando.

Nuestro microservicio `product-service` tiene configurado el `server.port=0`, esto significa que Spring Boot elegirá
dinámicamente un puerto aleatorio cada vez que el servicio se inicie. Por lo tanto:

- No hay un puerto fijo que se pueda exponer de antemano.
- Definir un `EXPOSE <port>` en el `Dockerfile` sería incorrecto, porque no coincidiría con el puerto aleatorio real que
  `Spring Boot` usaría al arrancar.
- No requerimos que los microservicios expongan puertos al host, ya que toda la comunicación es interna entre
  contenedores.

Ahora, si vemos los logs del contenedor `c-product-service` observamos que todo está trabajando correctamente, el puerto
en el que el microservicio `product-service` se está ejecutando al interior del contenedor es el puerto `39865`.

````bash
$ docker container logs c-product-service

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

INFO 1 --- [product-service] [           main] [                                                 ] d.m.p.app.ProductServiceApplication      : Starting ProductServiceApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app)
INFO 1 --- [product-service] [           main] [                                                 ] d.m.p.app.ProductServiceApplication      : No active profile set, falling back to 1 default profile: "default"
INFO 1 --- [product-service] [           main] [                                                 ] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
INFO 1 --- [product-service] [           main] [                                                 ] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 159 ms. Found 1 JPA repository interface.
INFO 1 --- [product-service] [           main] [                                                 ] o.s.cloud.context.scope.GenericScope     : BeanFactory id=7c08f00b-ffd2-38e6-b40d-c4e7a244ced0
INFO 1 --- [product-service] [           main] [                                                 ] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 0 (http)
INFO 1 --- [product-service] [           main] [                                                 ] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
INFO 1 --- [product-service] [           main] [                                                 ] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
INFO 1 --- [product-service] [           main] [                                                 ] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
INFO 1 --- [product-service] [           main] [                                                 ] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2835 ms
INFO 1 --- [product-service] [           main] [                                                 ] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
INFO 1 --- [product-service] [           main] [                                                 ] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.6.11.Final
INFO 1 --- [product-service] [           main] [                                                 ] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
INFO 1 --- [product-service] [           main] [                                                 ] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
INFO 1 --- [product-service] [           main] [                                                 ] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
INFO 1 --- [product-service] [           main] [                                                 ] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection com.mysql.cj.jdbc.ConnectionImpl@a72925
INFO 1 --- [product-service] [           main] [                                                 ] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
INFO 1 --- [product-service] [           main] [                                                 ] org.hibernate.orm.connections.pooling    : HHH10001005: Database info:
        Database JDBC URL [Connecting through datasource 'HikariDataSource (HikariPool-1)']
        Database driver: undefined/unknown
        Database version: 8.0.41
        Autocommit mode: undefined/unknown
        Isolation level: undefined/unknown
        Minimum pool size: undefined/unknown
        Maximum pool size: undefined/unknown
INFO 1 --- [product-service] [           main] [                                                 ] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
INFO 1 --- [product-service] [           main] [                                                 ] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
WARN 1 --- [product-service] [           main] [                                                 ] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
INFO 1 --- [product-service] [           main] [                                                 ] DiscoveryClientOptionalArgsConfiguration : Eureka HTTP Client uses RestTemplate.
WARN 1 --- [product-service] [           main] [                                                 ] iguration$LoadBalancerCaffeineWarnLogger : Spring Cloud LoadBalancer is currently working with the default cache. While this cache implementation is useful for development and tests, it's recommended to use Caffeine cache in production.You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
INFO 1 --- [product-service] [           main] [                                                 ] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint beneath base path '/actuator'
INFO 1 --- [product-service] [           main] [                                                 ] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 39865 (http) with context path '/'
INFO 1 --- [product-service] [           main] [                                                 ] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 39865
INFO 1 --- [product-service] [           main] [                                                 ] o.s.c.n.eureka.InstanceInfoFactory       : Setting initial instance status as: STARTING
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Initializing Eureka in region us-east-1
INFO 1 --- [product-service] [           main] [                                                 ] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Disable delta property : false
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Single vip registry refresh property : null
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Force full registry fetch : false
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application is null : false
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Registered Applications size is zero : true
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application version is -1: true
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Getting all instance registry info from the eureka server
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : The response status is 200
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Starting heartbeat executor: renew interval is: 30
INFO 1 --- [product-service] [           main] [                                                 ] c.n.discovery.InstanceInfoReplicator     : InstanceInfoReplicator onDemand update allowed rate per min is 4
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Discovery Client initialized at timestamp 1745881830595 with initial instances count: 0
INFO 1 --- [product-service] [           main] [                                                 ] o.s.c.n.e.s.EurekaServiceRegistry        : Registering application PRODUCT-SERVICE with eureka with status UP
INFO 1 --- [product-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Saw local status change event StatusChangeEvent [timestamp=1745881830604, current=UP, previous=STARTING]
INFO 1 --- [product-service] [foReplicator-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_PRODUCT-SERVICE/761e8e5d7932:product-service:656e27633c990a9e985dab520364dac4: registering service...
INFO 1 --- [product-service] [           main] [                                                 ] d.m.p.app.ProductServiceApplication      : Started ProductServiceApplication in 8.604 seconds (process running for 9.507)
INFO 1 --- [product-service] [foReplicator-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_PRODUCT-SERVICE/761e8e5d7932:product-service:656e27633c990a9e985dab520364dac4 - registration status: 204
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Disable delta property : false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Single vip registry refresh property : null
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Force full registry fetch : false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application is null : false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Registered Applications size is zero : true
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application version is -1: false
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : Getting all instance registry info from the eureka server
INFO 1 --- [product-service] [reshExecutor-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : The response status is 200
````

Si ahora revisamos `Eureka Server` podemos ver que nuestro `product-service` se ha registrado correctamente.

![08.png](assets/section-12/08.png)

Si revisamos el servidor de zipkin, podemos ver que nuestro `product-service` está enviando trazas, eso indica que los
contenedores se están comunicando correctamente.

![09.png](assets/section-12/09.png)

## Creando Dockerfile para servicio Items, build y run

Iniciamos creando el archivo `Dockerfile` en la raíz del `item-service`. Notar que aquí sí estamos usando el
`EXPOSE 8085`, dado que sabemos de antemano en qué puerto correrá nuestro `item-service` al interior del contenedor.
Recordar que dicho puerto lo hemos configurado en el perfil por defecto `item-service.yml` gestionado por el
`config-server`.

Sin embargo, esta instrucción solo nos servirá como una documentación dado que cuando construyamos el contenedor lo
haremos usando la bandera `-p (minúscula)` asignándole manualmente puertos `HOST_PORT:CONTAINER_PORT`.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

EXPOSE 8085
CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

Construimos la imagen del `item-service`.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\business-domain\item-service (feature/section-12)
$ docker image build -t item-service .
````

Procedemos a crear un contenedor de la imagen creada anteriormente.

````bash
$ docker container run -d -p 8085:8085 --name c-item-service --network microservices-project-net item-service
71fd6b928ea2d82acf38f6c84b706a022d55f3bb6156eb959c60697ffe0a8ec8
````

Si hasta este punto verificamos nuestros contenedores, veremos que todos se están ejecutando correctamente.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                         PORTS                               NAMES
71fd6b928ea2   item-service              "/__cacert_entrypoin…"   39 seconds ago   Up 39 seconds                  0.0.0.0:8085->8085/tcp              c-item-service
761e8e5d7932   product-service           "/__cacert_entrypoin…"   17 hours ago     Up About an hour                                                   c-product-service
dd4f55858258   openzipkin/zipkin:3.5.1   "start-zipkin"           20 hours ago     Up About an hour (unhealthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   23 hours ago     Up About an hour               33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   24 hours ago     Up About an hour               0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   25 hours ago     Up About an hour               33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   3 days ago       Up About an hour               0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   3 days ago       Up About an hour               0.0.0.0:8888->8888/tcp              c-config-server
````

Recordar que si hemos tenido detenido los contenedores, es muy importante levantarlos en orden para que no ocurra ningún
problema. El orden en que debemos levantar los contendores es el siguiente.

- c-mysql-zipkin
- c-zipkin
- c-mysql, c-postgres
- c-config-server
- c-discovery-server
- c-product-service
- c-item-service (el último que acabamos de levantar)

Ahora, si revisamos `eureka server` veremos que nuestros dos servicios que están corriendo en los contenedores
`c-product-service` y `c-item-service` se han registrado correctamente.

![10.png](assets/section-12/10.png)

Lo mismo ocurre si es que revisamos `zipkin`. Vemos que nuestros microservicios están enviando las trazas que van
generando. En este caso se muestra las trazas generadas al haber realizado una petición al `items-service`.

![11.png](assets/section-12/11.png)

Precisamente la petición que generó las trazas de la imagen anterior corresponden al siguiente.

````bash
$ curl -v http://localhost:8085/api/v1/items | jq
>
< HTTP/1.1 200
< Content-Type: application/json
< Transfer-Encoding: chunked
< Date: Tue, 29 Apr 2025 16:48:53 GMT
<
[
  {
    "product": {
      "id": 1,
      "name": "Panasonic",
      "price": 800.00,
      "createAt": "2025-04-29T21:40:50",
      "port": 45573
    },
    "quantity": 1,
    "total": 800.00
  }
  {...}
]
````

## Creando Dockerfile para servicio Users, build y run

Iniciamos creando el archivo `Dockerfile` en la raíz del `user-service`. Aquí no estamos usando el
`EXPOSE <port>`, dado que hemos configurado el `server.port=0` para generar un puerto aleatorio al servicio.
Eso significa que crearemos el contenedor usando la bandera `-P (mayúscula)`.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

Construimos la imagen del `user-service`.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\security\user-service (feature/section-12)
$ docker image build -t user-service .
````

Creamos el contenedor de la imagen anterior teniendo en cuenta que el puerto que generará el microservicio será un
puerto aleatorio, por lo tanto, utilizaremos la bandera `-P`.

````bash
$ docker container run -d -P --name c-user-service --network microservices-project-net user-service
8468b4747cb51e407661c41181a0391a0409291907ee305b47d965f3857e4a9f
````

Si listamos los contenedores, veremos que todos se están ejecutando sin problemas.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED              STATUS                    PORTS                               NAMES
8468b4747cb5   user-service              "/__cacert_entrypoin…"   About a minute ago   Up About a minute                                             c-user-service
71fd6b928ea2   item-service              "/__cacert_entrypoin…"   43 minutes ago       Up 23 minutes             0.0.0.0:8085->8085/tcp              c-item-service
761e8e5d7932   product-service           "/__cacert_entrypoin…"   18 hours ago         Up 24 minutes                                                 c-product-service
dd4f55858258   openzipkin/zipkin:3.5.1   "start-zipkin"           21 hours ago         Up 28 minutes (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   24 hours ago         Up 29 minutes             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   24 hours ago         Up 26 minutes             0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   25 hours ago         Up 26 minutes             33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   3 days ago           Up 25 minutes             0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   3 days ago           Up 25 minutes             0.0.0.0:8888->8888/tcp              c-config-server
````

Si hacemos un log del contenedor `c-user-service` veremos lo siguiente.

````bash
$ docker container logs c-user-service

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.4)

INFO 1 --- [user-service] [           main] [                                                 ] d.m.user.app.UserServiceApplication      : Starting UserServiceApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app)
INFO 1 --- [user-service] [           main] [                                                 ] d.m.user.app.UserServiceApplication      : The following 1 profile is active: "default"
INFO 1 --- [user-service] [           main] [                                                 ] o.s.c.c.c.ConfigServerConfigDataLoader   : Fetching config from server at : http://c-config-server:8888
INFO 1 --- [user-service] [           main] [                                                 ] o.s.c.c.c.ConfigServerConfigDataLoader   : Located environment: name=user-service, profiles=[default], label=null, version=6d1fd0fc178a112daa21f5705cd28023c55470f3, state=
INFO 1 --- [user-service] [           main] [                                                 ] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
INFO 1 --- [user-service] [           main] [                                                 ] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 175 ms. Found 2 JPA repository interfaces.
INFO 1 --- [user-service] [           main] [                                                 ] o.s.cloud.context.scope.GenericScope     : BeanFactory id=55bec185-8609-30a8-9d0d-3450565a3669
INFO 1 --- [user-service] [           main] [                                                 ] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 0 (http)
INFO 1 --- [user-service] [           main] [                                                 ] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
INFO 1 --- [user-service] [           main] [                                                 ] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.39]
INFO 1 --- [user-service] [           main] [                                                 ] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
INFO 1 --- [user-service] [           main] [                                                 ] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2971 ms
INFO 1 --- [user-service] [           main] [                                                 ] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
INFO 1 --- [user-service] [           main] [                                                 ] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.6.11.Final
INFO 1 --- [user-service] [           main] [                                                 ] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
INFO 1 --- [user-service] [           main] [                                                 ] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
INFO 1 --- [user-service] [           main] [                                                 ] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
INFO 1 --- [user-service] [           main] [                                                 ] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@1d884a192025-04-29T17:03:45.462Z  INFO 1 --- [user-service] [           main] [                                                 ] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
INFO 1 --- [user-service] [           main] [                                                 ] org.hibernate.orm.connections.pooling    : HHH10001005: Database info:
        Database JDBC URL [Connecting through datasource 'HikariDataSource (HikariPool-1)']
        Database driver: undefined/unknown
        Database version: 17.4
        Autocommit mode: undefined/unknown
        Isolation level: undefined/unknown
        Minimum pool size: undefined/unknown
        Maximum pool size: undefined/unknown
INFO 1 --- [user-service] [           main] [                                                 ] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    create table roles (
        id bigint generated by default as identity,
        name varchar(255),
        primary key (id)
    )
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    create table users (
        id bigint generated by default as identity,
        email varchar(255),
        enabled boolean not null,
        password varchar(255),
        username varchar(255),
        primary key (id)
    )
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    create table users_roles (
        user_id bigint not null,
        role_id bigint not null,
        primary key (user_id, role_id)
    )
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists roles
       drop constraint if exists UKofx66keruapi6vyqpv6f2or37
WARN 1 --- [user-service] [           main] [                                                 ] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
WARN 1 --- [user-service] [           main] [                                                 ] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "ukofx66keruapi6vyqpv6f2or37" of relation "roles" does not exist, skipping
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists roles
       add constraint UKofx66keruapi6vyqpv6f2or37 unique (name)
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists users
       drop constraint if exists UK6dotkott2kjsp8vw4d0m25fb7
WARN 1 --- [user-service] [           main] [                                                 ] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
WARN 1 --- [user-service] [           main] [                                                 ] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "uk6dotkott2kjsp8vw4d0m25fb7" of relation "users" does not exist, skipping
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists users
       add constraint UK6dotkott2kjsp8vw4d0m25fb7 unique (email)
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists users
       drop constraint if exists UKr43af9ap4edm43mmtq01oddj6
WARN 1 --- [user-service] [           main] [                                                 ] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
WARN 1 --- [user-service] [           main] [                                                 ] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "ukr43af9ap4edm43mmtq01oddj6" of relation "users" does not exist, skipping
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists users
       add constraint UKr43af9ap4edm43mmtq01oddj6 unique (username)
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists users_roles
       add constraint FKj6m8fwv7oqv74fcehir1a9ffy
       foreign key (role_id)
       references roles
DEBUG 1 --- [user-service] [           main] [                                                 ] org.hibernate.SQL                        :
    alter table if exists users_roles
       add constraint FK2o0jvgh89lemvvo17cbqvdxaa
       foreign key (user_id)
       references users
INFO 1 --- [user-service] [           main] [                                                 ] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
WARN 1 --- [user-service] [           main] [                                                 ] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
INFO 1 --- [user-service] [           main] [                                                 ] DiscoveryClientOptionalArgsConfiguration : Eureka HTTP Client uses RestTemplate.
WARN 1 --- [user-service] [           main] [                                                 ] iguration$LoadBalancerCaffeineWarnLogger : Spring Cloud LoadBalancer is currently working with the default cache. While this cache implementation is useful for development and tests, it's recommended to use Caffeine cache in production.You can switch to using Caffeine cache, by adding it and org.springframework.cache.caffeine.CaffeineCacheManager to the classpath.
INFO 1 --- [user-service] [           main] [                                                 ] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint beneath base path '/actuator'
INFO 1 --- [user-service] [           main] [                                                 ] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 46119 (http) with context path '/'
INFO 1 --- [user-service] [           main] [                                                 ] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 46119
INFO 1 --- [user-service] [           main] [                                                 ] o.s.c.n.eureka.InstanceInfoFactory       : Setting initial instance status as: STARTING
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Initializing Eureka in region us-east-1
INFO 1 --- [user-service] [           main] [                                                 ] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Disable delta property : false
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Single vip registry refresh property : null
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Force full registry fetch : false
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application is null : false
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Registered Applications size is zero : true
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Application version is -1: true
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Getting all instance registry info from the eureka server
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : The response status is 200
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Starting heartbeat executor: renew interval is: 30
INFO 1 --- [user-service] [           main] [                                                 ] c.n.discovery.InstanceInfoReplicator     : InstanceInfoReplicator onDemand update allowed rate per min is 4
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Discovery Client initialized at timestamp 1745946229499 with initial instances count: 2
INFO 1 --- [user-service] [           main] [                                                 ] o.s.c.n.e.s.EurekaServiceRegistry        : Registering application USER-SERVICE with eureka with status UP
INFO 1 --- [user-service] [           main] [                                                 ] com.netflix.discovery.DiscoveryClient    : Saw local status change event StatusChangeEvent [timestamp=1745946229507, current=UP, previous=STARTING]
INFO 1 --- [user-service] [foReplicator-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_USER-SERVICE/8468b4747cb5:user-service:937bf82de4ad1b793975b64300a54ea4: registering service...
INFO 1 --- [user-service] [           main] [                                                 ] d.m.user.app.UserServiceApplication      : Started UserServiceApplication in 10.83 seconds (process running for 11.588)
INFO 1 --- [user-service] [foReplicator-%d] [                                                 ] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_USER-SERVICE/8468b4747cb5:user-service:937bf82de4ad1b793975b64300a54ea4 - registration status: 204
````

> Si ahora revisamos los dashboards de `Eureka Server` y de `Zipkin` veremos que nuestro `user-service` ha sido
> registrado correctamente y está enviando trazas sin problema.

## Creando Dockerfile para servicio OAuth2, build y run

Iniciamos creando el archivo `Dockerfile` en la raíz del `authorization-server`.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

EXPOSE 9000
CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

Creamos la imagen del `authorization-server`.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\security\authorization-server (feature/section-12)
$ docker image build -t authorization-server .
````

Procedemos a crear el contendor.

````bash
$ docker container run -d -p 9000:9000 --name c-authorization-server --network microservices-project-net authorization-server
a4aeccd2f2ee622f3537b29269481c935233d572ee4f8dd0e6d1f3a6a636d49b
````

Si listamos todos los contenedores veremos que todos se están ejecutando sin problema.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                 PORTS                               NAMES
a4aeccd2f2ee   authorization-server      "/__cacert_entrypoin…"   24 seconds ago   Up 23 seconds          0.0.0.0:9000->9000/tcp              c-authorization-server
713d8799afe4   openzipkin/zipkin:3.5.1   "start-zipkin"           2 hours ago      Up 2 hours (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
8468b4747cb5   user-service              "/__cacert_entrypoin…"   25 hours ago     Up 2 hours                                                 c-user-service
71fd6b928ea2   item-service              "/__cacert_entrypoin…"   26 hours ago     Up 2 hours             0.0.0.0:8085->8085/tcp              c-item-service
761e8e5d7932   product-service           "/__cacert_entrypoin…"   43 hours ago     Up 2 hours                                                 c-product-service
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 days ago       Up 2 hours             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   2 days ago       Up 2 hours             0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 days ago       Up 2 hours             33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   4 days ago       Up 2 hours             0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   4 days ago       Up 2 hours             0.0.0.0:8888->8888/tcp              c-config-server
````

> Si ahora revisamos los dashboards de `Eureka Server` y de `Zipkin` veremos que nuestro `authorization-server` ha sido
> registrado correctamente y está enviando trazas sin problema.

## Creando Dockerfile para Gateway, build y run

Iniciamos creando el archivo `Dockerfile` en la raíz del `gateway-server`.

````dockerfile
FROM eclipse-temurin:21-jdk-alpine AS dependencies
WORKDIR /app
COPY ./mvnw ./
COPY ./.mvn ./.mvn
COPY ./pom.xml ./

RUN sed -i -e 's/\r$//' ./mvnw
RUN ./mvnw dependency:go-offline

COPY ./src ./src
RUN ./mvnw clean package -DskipTests
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS builder
WORKDIR /app
COPY --from=dependencies /app/target/*.jar ./app.jar
RUN java -Djarmode=layertools -jar app.jar extract
#-----------------------------------------------------------------
FROM eclipse-temurin:21-jre-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dependencies ./
COPY --from=builder /app/spring-boot-loader ./
COPY --from=builder /app/snapshot-dependencies ./
COPY --from=builder /app/application ./

EXPOSE 8090
CMD ["java", "org.springframework.boot.loader.launch.JarLauncher"]
````

Creamos la imagen de nuestro `gateway-server` a partir del `Dockerfile` anterior.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project\infrastructure\gateway-server (feature/section-12)
$ docker image build -t gateway-server .
````

Creamos el contenedor `c-gateway-server` a partir de la imagen anterior.

````bash
$ docker container run -d -p 8090:8090 --name c-gateway-server --network microservices-project-net gateway-server
49d6c3ac3161552a259e2d3acc45ab2122c8cf510de7b4032a4796e6b523412b
````

Si listamos todos los contenedores, veremos que todos se están ejecutando sin problemas.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                 PORTS                               NAMES
49d6c3ac3161   gateway-server            "/__cacert_entrypoin…"   59 seconds ago   Up 58 seconds          0.0.0.0:8090->8090/tcp              c-gateway-server
a4aeccd2f2ee   authorization-server      "/__cacert_entrypoin…"   4 minutes ago    Up 4 minutes           0.0.0.0:9000->9000/tcp              c-authorization-server
713d8799afe4   openzipkin/zipkin:3.5.1   "start-zipkin"           2 hours ago      Up 2 hours (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
8468b4747cb5   user-service              "/__cacert_entrypoin…"   25 hours ago     Up 2 hours                                                 c-user-service
71fd6b928ea2   item-service              "/__cacert_entrypoin…"   26 hours ago     Up 2 hours             0.0.0.0:8085->8085/tcp              c-item-service
761e8e5d7932   product-service           "/__cacert_entrypoin…"   43 hours ago     Up 2 hours                                                 c-product-service
07f5e85a4681   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 days ago       Up 3 hours             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   2 days ago       Up 2 hours             0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 days ago       Up 2 hours             33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   4 days ago       Up 2 hours             0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   4 days ago       Up 2 hours             0.0.0.0:8888->8888/tcp              c-config-server
````

Procedemos a generar un código de autorización apoyándonos de la página `https://oauthdebugger.com/debug`. Aquí estamos
usando como un `Authorize URI` el `http://127.0.0.1:9000/oauth2/authorize` que apunta al contenedor
`c-authorization-server`. Es decir, la petición lo estamos haciendo desde nuestra `máquina local` hacia el contenedor
del `servidor de autorización`.

![12.png](assets/section-12/12.png)

Luego de dar clic en el botón `SEND REQUEST`, seremos redireccionados al login del servidor de autorización.

![13.png](assets/section-12/13.png)

Al dar clic en el botón `Sing in`, el servidor de autorización verificará nuestras credenciales y como son válidas
generará el código de autorización que nos lo retornará.

![14.png](assets/section-12/14.png)

Con este código de autorización podemos realizar una petición al servidor de autorización para solicitar el
`access_token`. Nuevamente, observamos que la petición lo hacemos apuntando al contenedor `c-authorization-server` cuyo
puerto es el `9000`.

````bash
$ curl -v -X POST -u gateway-app:123456 -d "grant_type=authorization_code&redirect_uri=https://oauthdebugger.com/debug&code=0Y1mdsCTgvqb0E_XyzQ-RsixRsu4yTGc-x6b9Ex45adOmnqZRTOGg0VaVCgYSf_GvQPG0lGSNxpJqvk1M2Pt2ODVfLv3NLRmt44cSM_LsAhpSYmSXHNrpciTHVCiNoqr" http://localhost:9000/oauth2/token | jq
>
< HTTP/1.1 200
< X-Content-Type-Options: nosniff
< X-XSS-Protection: 0
< Cache-Control: no-cache, no-store, max-age=0, must-revalidate
< Pragma: no-cache
< Expires: 0
< X-Frame-Options: DENY
< Content-Type: application/json;charset=UTF-8
< Transfer-Encoding: chunked
< Date: Wed, 30 Apr 2025 18:03:56 GMT
<
{
  "access_token": "eyJraWQiOiIxYjMzZmYyZS1hODJjLTQ5NzQtYmQ3My0wNjA1N2Y3N2Q3MzkiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1ZCI6ImdhdGV3YXktYXBwIiwibmJmIjoxNzQ2MDM2MjM2LCJzY29wZSI6WyJvcGVuaWQiLCJwcm9maWxlIl0sInJvbGVzIjpbIlJPTEVfVVNFUiIsIlJPTEVfQURNSU4iXSwiaXNzIjoiaHR0cDovL2MtYXV0aG9yaXphdGlvbi1zZXJ2ZXI6OTAwMCIsImV4cCI6MTc0NjAzOTgzNiwidG9rZW5fdHlwZSI6IkFjY2VzcyBUb2tlbiIsImlhdCI6MTc0NjAzNjIzNiwianRpIjoiMGY2M2JhNWEtZjYzNi00ZmQxLWEyY2ItODZmYzViNTQ3YmVhIn0.ScGQeHV4ROo-_8Dq_chY2lwd9zrEVE899bysTsoeyn7ZIyxP4oI53TT4y7wGDLJqtgl2Up1fkKY6OG8LUzOBX8ubn3TKU4tmlQbGdBsgS_I2kLUxjntbDL0GAWJZGd1ajYqnLaBk6FfsWZ_D1wILEsuNTVgqK8MvmyLfmn6sImbKQXloOqCv1ePqWE45DzwwcQcZH5u5qpxlUXCYu0gD00TSGj36RERp9amPl5YFhFvE2_w-TT9xSc_M5MyozSLajg4jMjEjmREZmFiWLONDQzV_qmJIiIhWTVouLp_2S3nRYhtjR1A8H8JKeQPAG9eA8Zn6OkUp6PA2HjDEyujplw",
  "refresh_token": "q1fNlaRT3Rv4DLsQ69WsmXbfm7xOVpxqJ-A7xXywu-037itRs7pf_BnosIM2R_jPhpCyS9Ngvrru-e5g1yHSG5ZOPND42aJqTjxxdV87xQH10s5_uaAzYKdyVM1yVAuA",
  "scope": "openid profile",
  "id_token": "eyJraWQiOiIxYjMzZmYyZS1hODJjLTQ5NzQtYmQ3My0wNjA1N2Y3N2Q3MzkiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1ZCI6ImdhdGV3YXktYXBwIiwiYXpwIjoiZ2F0ZXdheS1hcHAiLCJhdXRoX3RpbWUiOjE3NDYwMzYxOTgsInJvbGVzIjpbIlJPTEVfVVNFUiIsIlJPTEVfQURNSU4iXSwiaXNzIjoiaHR0cDovL2MtYXV0aG9yaXphdGlvbi1zZXJ2ZXI6OTAwMCIsImV4cCI6MTc0NjAzODAzNiwidG9rZW5fdHlwZSI6IklkIFRva2VuIiwiaWF0IjoxNzQ2MDM2MjM2LCJub25jZSI6IndpMWV3a2lraDQiLCJqdGkiOiJlOWM3MzkzMy0zMTg1LTRkYmItODljZi0zYzQ2MTE4NjM4NzIiLCJzaWQiOiJ4M3BSd0pxX1BSaDJ0cEw3cEtLX0dkYVJlWEdYUHFoTUNnNDNaY2dJSmRrIn0.KSuKPD9M8ZKsCDMTAuX21WAwDjSlf7MqLbUiJmEUvI1zYVGRAecLQy_iBK3VpM7lFwijD8PuFctBwINVZp1Ug2Q6EW4XA_v1KWRDlwXE-6-3oT75udkZoBmXvtV5gHzd2IUYKvJeL4qNov7OTwJJxxMqubQtSUgJrvtWQD8uwK3yZjZiP30-dRPKWYV59qklA-xNpd-B9EfD2x7_YTU9cOsH-jBTPCZiYKx0ddaVBhHIYN3tnBeOP7K56H_jZjlacyKUj-2UAwcPlQ8BqrSKsCKNJZkQU_2KoOwBKrs-EDRUqKYANYEJKM8yy1uLDhQj-YS_vVFitWLVnje8eaXDQA",
  "token_type": "Bearer",
  "expires_in": 3599
}
````

Una vez que hemos obtenido el access token lo podemos decodificar para ver su contenido. Observamos que el atributo
`iss` tiene como valor la url registrada en el `issuer` (`http://c-authorization-server:9000`) del bean
`AuthorizationServerSettings` en el servidor de autorización.

![15.png](assets/section-12/15.png)

Utilizamos el `access_token` para registrar un usuario en el `user-service` enviando la petición a través del
`c-gateway-server`.

````bash
$ curl -v -X POST -H "Authorization: Bearer eyJraWQiOiIxYjMzZmYyZS1hODJjLTQ5NzQtYmQ3My0wNjA1N2Y3N2Q3MzkiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1ZCI6ImdhdGV3YXktYXBwIiwibmJmIjoxNzQ2MDM2MjM2LCJzY29wZSI6WyJvcGVuaWQiLCJwcm9maWxlIl0sInJvbGVzIjpbIlJPTEVfVVNFUiIsIlJPTEVfQURNSU4iXSwiaXNzIjoiaHR0cDovL2MtYXV0aG9yaXphdGlvbi1zZXJ2ZXI6OTAwMCIsImV4cCI6MTc0NjAzOTgzNiwidG9rZW5fdHlwZSI6IkFjY2VzcyBUb2tlbiIsImlhdCI6MTc0NjAzNjIzNiwianRpIjoiMGY2M2JhNWEtZjYzNi00ZmQxLWEyY2ItODZmYzViNTQ3YmVhIn0.ScGQeHV4ROo-_8Dq_chY2lwd9zrEVE899bysTsoeyn7ZIyxP4oI53TT4y7wGDLJqtgl2Up1fkKY6OG8LUzOBX8ubn3TKU4tmlQbGdBsgS_I2kLUxjntbDL0GAWJZGd1ajYqnLaBk6FfsWZ_D1wILEsuNTVgqK8MvmyLfmn6sImbKQXloOqCv1ePqWE45DzwwcQcZH5u5qpxlUXCYu0gD00TSGj36RERp9amPl5YFhFvE2_w-TT9xSc_M5MyozSLajg4jMjEjmREZmFiWLONDQzV_qmJIiIhWTVouLp_2S3nRYhtjR1A8H8JKeQPAG9eA8Zn6OkUp6PA2HjDEyujplw" -H "Content-type: application/json" -d "{\"username\": \"systems\", \"password\": \"123456\", \"email\": \"systems@gmail.com\"}" http://localhost:8090/api/v1/users | jq
>
< HTTP/1.1 201 Created
< transfer-encoding: chunked
< Location: /api/v1/users/4
< Content-Type: application/json
< Date: Wed, 30 Apr 2025 18:09:24 GMT
< token-response: 123456
< Cache-Control: no-cache, no-store, max-age=0, must-revalidate
< Pragma: no-cache
< Expires: 0
< X-Content-Type-Options: nosniff
< X-Frame-Options: DENY
< X-XSS-Protection: 0
< Referrer-Policy: no-referrer
< set-cookie: color=red
<
{
  "id": 4,
  "username": "systems",
  "password": "$2a$10$snNRRwnmDirDgXpbjlsnXe8P5y/yt26RTWa6LFwBmbMzccbzH24de",
  "enabled": true,
  "email": "systems@gmail.com",
  "roles": [
    {
      "id": 2,
      "name": "ROLE_USER"
    }
  ]
}
````

## Escalando más instancias en servicio productos

Hasta este punto tenemos los siguientes contenedores levantados y ejecutándose correctamente.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                 PORTS                               NAMES
467706c27278   authorization-server      "/__cacert_entrypoin…"   33 minutes ago   Up 33 minutes          0.0.0.0:9000->9000/tcp              c-authorization-server
a07098c7aff5   openzipkin/zipkin:3.5.1   "start-zipkin"           2 hours ago      Up 2 hours (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
cb64115ce63b   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 hours ago      Up 2 hours             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
49d6c3ac3161   gateway-server            "/__cacert_entrypoin…"   5 hours ago      Up About an hour       0.0.0.0:8090->8090/tcp              c-gateway-server
8468b4747cb5   user-service              "/__cacert_entrypoin…"   30 hours ago     Up About an hour                                           c-user-service
71fd6b928ea2   item-service              "/__cacert_entrypoin…"   30 hours ago     Up About an hour       0.0.0.0:8085->8085/tcp              c-item-service
761e8e5d7932   product-service           "/__cacert_entrypoin…"   47 hours ago     Up About an hour                                           c-product-service
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   2 days ago       Up About an hour       0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 days ago       Up About an hour       33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   5 days ago       Up About an hour       0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   5 days ago       Up About an hour       0.0.0.0:8888->8888/tcp              c-config-server
````

**Nota**
> Es posible que algunos contenedores tengan un identificador diferente al que se ha utilizado previamente. Esto se debe
> a que, hasta este punto, he estado realizando diversas pruebas, lo que ha implicado eliminar y volver a crear
> contenedores en varias ocasiones.

En este apartado vamos a crear nuevos contenedores a partir de la imagen `product-service` para observar el
comportamiento del balanceo de carga. Usaremos la opción `-P (mayúscula)`, dado que aquí estamos trabajando con puertos
dinámicos. Además, no usaremos la bandera `--name`, pues al trabajar con puertos aleatorios no interesa mucho el
nombre; dejaremos que `Docker` le asigne un nombre por nosotros.

Ejecutamos dos veces el siguiente comando para crear dos contenedores más.

````bash
$ docker container run -d -P --network microservices-project-net product-service
````

Si listamos todos los contenedores, veremos que ahora tenemos 3 instancias de la imagen `product-service`.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                 PORTS                               NAMES
7add2701cf1c   product-service           "/__cacert_entrypoin…"   3 minutes ago    Up 3 minutes                                               vigilant_meninsky
65a42250d9ae   product-service           "/__cacert_entrypoin…"   4 minutes ago    Up 3 minutes                                               wizardly_blackwell
467706c27278   authorization-server      "/__cacert_entrypoin…"   49 minutes ago   Up 49 minutes          0.0.0.0:9000->9000/tcp              c-authorization-server
a07098c7aff5   openzipkin/zipkin:3.5.1   "start-zipkin"           2 hours ago      Up 2 hours (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
cb64115ce63b   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 hours ago      Up 2 hours             33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
49d6c3ac3161   gateway-server            "/__cacert_entrypoin…"   5 hours ago      Up 2 hours             0.0.0.0:8090->8090/tcp              c-gateway-server
8468b4747cb5   user-service              "/__cacert_entrypoin…"   30 hours ago     Up 2 hours                                                 c-user-service
71fd6b928ea2   item-service              "/__cacert_entrypoin…"   31 hours ago     Up 2 hours             0.0.0.0:8085->8085/tcp              c-item-service
761e8e5d7932   product-service           "/__cacert_entrypoin…"   2 days ago       Up 2 hours                                                 c-product-service
d69f3ce25cfd   postgres:17-alpine        "docker-entrypoint.s…"   2 days ago       Up 2 hours             0.0.0.0:5433->5432/tcp              c-postgres
a60e121908c8   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 days ago       Up 2 hours             33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
efdda60f4dbb   discovery-server          "/__cacert_entrypoin…"   5 days ago       Up 2 hours             0.0.0.0:8761->8761/tcp              c-discovery-server
683472a028ac   config-server             "/__cacert_entrypoin…"   5 days ago       Up 2 hours             0.0.0.0:8888->8888/tcp              c-config-server
````

Notar que `Docker` les asignó nombres aleatorios a los contenedores que hemos creado anteriormente dado que no hemos
usado la bandera `--name`.

## Probando las instancias escaladas del servicio producto

En este momento tenemos tres instancias del servicio product-service ejecutándose, así que realizaremos tres peticiones
consecutivas para verificar que todas están respondiendo, gracias al balanceo de carga que aplica el `gateway-server`.

Recordar que en el `application.yml` del `gateway-server` estamos usando rutas para distintos servicios como por ejemplo
para el `product-service`, definimos el `uri` de esta manera `uri: lb://product-service`.

El prefijo `lb://` (de `load balancer`) indica que `Spring Cloud Gateway` utilizará un mecanismo de descubrimiento de
servicios, como `Eureka`, para enrutar las peticiones a una instancia disponible del servicio correspondiente.

Entonces:

- Al tener varias instancias registradas en `Eureka` bajo el mismo `spring.application.name` (en este caso
  `product-service`), el `gateway` las reconoce.
- Gracias al prefijo `lb://`, el `gateway` consulta `Eureka` y aplica `balanceo de carga` (usualmente con estrategia
  `round-robin`) entre las instancias disponibles.

Esta instancia usa el puerto `46297`.

````bash
$ curl -v http://localhost:8090/api/v1/products | jq
>
< HTTP/1.1 200 OK
< transfer-encoding: chunked
< Content-Type: application/json
< Date: Wed, 30 Apr 2025 22:55:33 GMT
< token-response: 123456
< Cache-Control: no-cache, no-store, max-age=0, must-revalidate
< Pragma: no-cache
< Expires: 0
< X-Content-Type-Options: nosniff
< X-Frame-Options: DENY
< X-XSS-Protection: 0
< Referrer-Policy: no-referrer
< set-cookie: color=red
< set-cookie: userCookie=magadiflo
<
[
  {
    "id": 1,
    "name": "Panasonic",
    "price": 800.00,
    "createAt": "2025-05-01T03:47:43",
    "port": 46297
  },
  {...}
]
````

Esta instancia usa el puerto `39563`.

````bash
$ curl -v http://localhost:8090/api/v1/products | jq
>
< HTTP/1.1 200 OK
< transfer-encoding: chunked
< Content-Type: application/json
< Date: Wed, 30 Apr 2025 22:55:39 GMT
< token-response: 123456
< Cache-Control: no-cache, no-store, max-age=0, must-revalidate
< Pragma: no-cache
< Expires: 0
< X-Content-Type-Options: nosniff
< X-Frame-Options: DENY
< X-XSS-Protection: 0
< Referrer-Policy: no-referrer
< set-cookie: color=red
< set-cookie: userCookie=magadiflo
<
[
  {
    "id": 1,
    "name": "Panasonic",
    "price": 800.00,
    "createAt": "2025-05-01T03:47:43",
    "port": 39563
  },
  {...}
]
````

Esta instancia usa el puerto `41133`.

````bash
$ curl -v http://localhost:8090/api/v1/products | jq
>
< HTTP/1.1 200 OK
< transfer-encoding: chunked
< Content-Type: application/json
< Date: Wed, 30 Apr 2025 22:55:42 GMT
< token-response: 123456
< Cache-Control: no-cache, no-store, max-age=0, must-revalidate
< Pragma: no-cache
< Expires: 0
< X-Content-Type-Options: nosniff
< X-Frame-Options: DENY
< X-XSS-Protection: 0
< Referrer-Policy: no-referrer
< set-cookie: color=red
< set-cookie: userCookie=magadiflo
<
[
  {
    "id": 1,
    "name": "Panasonic",
    "price": 800.00,
    "createAt": "2025-05-01T03:47:43",
    "port": 41133
  },
  {...}
]
````

## Limpieza completa de Docker: eliminando imágenes, contenedores, redes y volúmenes

Dado que en este apartado trabajaremos con `Docker Compose` vamos a eliminar todo lo que hemos realizado hasta el
momento en `Docker`, es decir, dejaremos limpio `Docker` para empezar a trabajar desde cero.

Para eso ejecutaremos el siguiente comando.

````bash
$ docker system prune -a
WARNING! This will remove:
  - all stopped containers
  - all networks not used by at least one container
  - all images without at least one container associated to them
  - all build cache
````

Como se observa en el mensaje de `WARNING`, este comando elimina:

- Todos los contenedores detenidos.
- Todas las redes no utilizadas por al menos un contenedor.
- Todas las imágenes sin al menos un contenedor asociado.
- Toda la caché de compilación.

El siguiente comando eliminará los `volúmenes locales anónimos` que no sean utilizados por al menos un contenedor.

````bash
$ docker volume prune
WARNING! This will remove anonymous local volumes not used by at least one container.
Are you sure you want to continue? [y/N]
````

También eliminaremos el `volumen` que creamos.

````bash
$ docker volume rm mysql-zipkin-data
````

## Despliegue de contenedores con Docker Compose

### Docker Compose

`Docker Compose` es una herramienta para definir y ejecutar aplicaciones multicontenedor. Es la clave para lograr
una experiencia de desarrollo e implementación optimizada y eficiente.

`Compose` simplifica el control de toda la pila de aplicaciones, facilitando la gestión de `servicios`, `redes` y
`volúmenes` en un único archivo de configuración `YAML`. Con un solo comando, se crean e inician todos los servicios
desde el archivo de configuración.

`Compose` funciona en todos los entornos: `production`, `staging`, `development`, `testing` y flujos de trabajo de
integración continua (`CI`).

### Definiendo servicios en un archivo compose

`Compose` simplifica el control de toda la pila de aplicaciones, facilitando la gestión de `servicios`, `redes` y
`volúmenes` en un único archivo de configuración `YAML` fácil de entender.

Crearemos un archivo llamado `compose.yml` en el directorio de nuestro proyecto `microservices-project` y agregaremos
las siguientes instrucciones.

````yml
services:
  s-config-server:
    build:
      context: ./infrastructure/config-server
    image: config-server
    container_name: c-config-server
    restart: unless-stopped
    ports:
      - '8888:8888'
    networks:
      - microservices-project-net
  s-discovery-server:
    build:
      context: ./infrastructure/discovery-server
    image: discovery-server
    container_name: c-discovery-server
    restart: unless-stopped
    ports:
      - '8761:8761'
    networks:
      - microservices-project-net

networks:
  microservices-project-net:
    name: microservices-project-net
````

A continuación explicaremos las instrucciones usadas tomando como referencia el `config-server`.

- `services`, sección principal donde se definen todos los contenedores (servicios) que `Docker Compose` debe crear y
  administrar.
- `s-config-server`, nombre lógico del servicio dentro de `Docker Compose` (no es el nombre del contenedor).
- `build.context`, le indica a `Docker Compose` que construya la imagen desde un `Dockerfile` ubicado en el directorio
  `./infrastructure/config-server`. El `context` define el directorio base desde donde `Docker` buscará el `Dockerfile`
  y los archivos necesarios.
- `image`, en esta configuración, el `image` asigna un nombre a la imagen que se construirá (`config-server`). Si ya
  existe una imagen con ese nombre, se usará, a menos que se indique `--build`.
- `container_name`, nombre explícito que tendrá el contenedor cuando se cree. Permite una identificación más clara que
  el nombre generado automáticamente por `Docker`.
- `restart`, define la política de reinicio. El `unless-stopped` reinicia el contenedor automáticamente si falla o si el
  sistema se reinicia. No lo reinicia si fue detenido manualmente por el usuario con `docker stop`.
- `ports`, expone puertos. El formato es `host:contenedor`.
- `networks`, conecta este servicio a la red personalizada `microservices-project-net`, permitiendo que se comunique
  con otros servicios en la misma red.
- `networks.microservices-project-net.name`, define una red personalizada llamada `microservices-project-net`. Permite
  que varios servicios se comuniquen por nombre de servicio, como si fueran `DNS` (por ejemplo,
  http://s-config-server:8888 desde otro contenedor).

### Ejecutando servicios desde un archivo compose

Con un solo comando, creamos e iniciamos todos los servicios desde nuestro archivo de configuración. Así que,
posicionados mediante la línea de comandos en el directorio raíz de nuestro proyecto donde está el archivo
`compose.yml`, ejecutamos el siguiente comando:

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project (feature/section-12)                                            
$ docker compose up -d                                                                                                                                                
[+] Running 2/2                                                                                                                                                       
 ! s-discovery-server Warning pull access denied for discovery-server, repository does not exist or may require 'docker login'                                        
 ! s-config-server Warning    pull access denied for config-server, repository does not exist or may require 'docker login'                                           
Compose can now delegate builds to bake for better performance.                                                                                                       
 To do so, set COMPOSE_BAKE=true.                                                                                                                                     
[+] Building 97.4s (43/43) FINISHED                                                                                                                                   
 => [s-discovery-server internal] load build definition from Dockerfile                                                                                               
 => => transferring dockerfile: 782B                                                                                                                                  
 => [s-config-server internal] load build definition from Dockerfile                                                                                                  
 => => transferring dockerfile: 782B                                                                                                                                  
...                                                                     
...                                                                     
...                                                                                                                                                      
 => [s-config-server runner 6/6] COPY --from=builder /app/application ./                                                                                              
 => [s-config-server] exporting to image                                                                                                                              
 => => exporting layers                                                                                                                                               
 => => exporting manifest sha256:12a4f469ec4c53fa99563ce94d25ab27399bda4ec5ee4dc323139fe8ad9c400f                                                                     
 => => exporting config sha256:a2eb81e084135138505091dafcee9eb6c03ba726d1b81de694669cb4b36d86dd                                                                       
 => => exporting attestation manifest sha256:2d46737db4c1b2cc9d11ef81ae388910cd008d747dd37ab8df99b24ed768c43c                                                         
 => => exporting manifest list sha256:e0da0620b90495ac31760d190033e17b0f60448788e5242bca1a4aa57709fcef                                                                
 => => naming to docker.io/library/config-server:latest                                                                                                               
 => => unpacking to docker.io/library/config-server:latest                                                                                                            
 => [s-discovery-server] resolving provenance for metadata file                                                                                                       
 => [s-config-server] resolving provenance for metadata file                                                                                                          
[+] Running 5/5                                                                                                                                                       
 ✔ s-config-server                    Built                                                                                                                           
 ✔ s-discovery-server                 Built                                                                                                                           
 ✔ Network microservices-project-net  Created                                                                                                                         
 ✔ Container c-discovery-server       Started                                                                                                                         
 ✔ Container c-config-server          Started                                                                                                                                                                  
````

**Interpretación del resultado**

- `Docker Compose` intentó descargar (`pull`) imágenes llamadas `discovery-server` y `config-server` desde `Docker Hub`
  u otro registry.
- Como no encontró esas imágenes en el `registry` (porque son personalizadas y no están publicadas), lanzó un `warning`.
- Esto es normal si estamos usando `build` en nuestro `compose.yml`, ya que luego `Docker` procederá a construirlas
  localmente.
- Luego `Docker Compose` empieza a construir las imágenes localmente porque no pudo descargarlas.
- Utiliza los `Dockerfile` de cada servicio (`s-discovery-server` y `s-config-server`).
- La construcción se completó satisfactoriamente (`FINISHED`), lo cual indica que las imágenes fueron creadas
  localmente.

**Explicación del comando `docker compose up -d`**

- `docker compose`, invoca `Docker Compose` (modo plugin). Administra múltiples contenedores como un solo conjunto.
- `up`, crea e inicia los contenedores, redes y volúmenes definidos en el archivo `compose.yml`.
- `-d` o `--detach`, ejecuta los servicios en segundo plano (detached), liberando la terminal.

### 📌 Nota:

1. **Primera vez (sin imágenes ni contenedores):**
    - Ejecutamos `docker compose up -d`
    - `Docker Compose` detecta que no hay imagen, entonces construye la imagen desde el `Dockerfile`.


2. **Ya tienes la imagen y no hubo cambios:**
    - Ejecutamos `docker compose up -d`
    - `Docker Compose` usa la imagen existente, no reconstruye nada.


3. **Hiciste cambios en el código fuente o Dockerfile:**
    - Ejecutamos `docker compose up -d --build`
    - `Docker Compose` reconstruye la imagen antes de levantar los contenedores.

### Verificando los servicios creados desde el archivo compose

Si listamos las imágenes, podemos observar que `docker compose` nos creó las imágenes de nuestros microservicios
`config-server` y `discovery-server`. Esto fue gracias a que en el `compose.yml` definimos la ubicación del `Dockerfile`
a través del `context`.

````bash
$ docker image ls
REPOSITORY         TAG       IMAGE ID       CREATED         SIZE
config-server      latest    e0da0620b904   4 minutes ago   366MB
discovery-server   latest    453e62cdacb5   4 minutes ago   391MB
````

Si listamos los contenedores, podemos ver que también se crearon correctamente y está en ejecución con status `Up`.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS          PORTS                    NAMES
f7c3d96ede73   discovery-server   "/__cacert_entrypoin…"   12 minutes ago   Up 12 minutes   0.0.0.0:8761->8761/tcp   c-discovery-server
83599c43381b   config-server      "/__cacert_entrypoin…"   12 minutes ago   Up 12 minutes   0.0.0.0:8888->8888/tcp   c-config-server
````

Si listamos las redes, podemos ver que nuestra red `microservices-project-net` fue creada correctamente.

````bash
$ docker network ls
NETWORK ID     NAME                        DRIVER    SCOPE
4a4120e9b5ab   bridge                      bridge    local
8c77b18a87f1   host                        host      local
1361fbd37e51   microservices-project-net   bridge    local
fe251eccddcc   none                        null      local
````

Finalmente, si listamos los volúmenes podemos ver que no hay ningún volumen creado porque hasta el momento no estamos
trabajando con volúmenes.

````bash
$ docker volume ls
DRIVER    VOLUME NAME
````

Si ahora ingresamos al dashboard de `Eureka Server` que se está ejecutando dentro del contenedor `c-discovery-server`
utilizando la siguiente url `http://localhost:8761/`, podemos ver que se está ejecutando sin problemas.

![16.png](assets/section-12/16.png)

Lo mismo sucede si es que vemos el funcionamiento de nuestro `config-server` que está siendo ejecutado dentro del
contenedor `c-config-server`.

````bash
$ curl -v http://localhost:8888/item-service/dev | jq
>
< HTTP/1.1 200
< Content-Type: application/json
< Transfer-Encoding: chunked
< Date: Thu, 01 May 2025 17:26:55 GMT
<
{
  "name": "item-service",
  "profiles": [
    "dev"
  ],
  "label": null,
  "version": "6d1fd0fc178a112daa21f5705cd28023c55470f3",
  "state": "",
  "propertySources": [
    {
      "name": "https://github.com/magadiflo/config-server-repo.git/item-service-dev.yml",
      "source": {
        "custom.text": "item-service-dev.yml en config server con perfil dev (desde GitHub)",
        "custom.author.name": "Martín (desde GitHub)",
        "custom.author.email": "magadiflo@gmail.com (desde GitHub)"
      }
    },
    {
      "name": "https://github.com/magadiflo/config-server-repo.git/item-service.yml",
      "source": {
        "server.port": 8085,
        "custom.text": "item-service.yml en config server con perfil por defecto"
      }
    }
  ]
}
````

## Despliegue de servicios mysql, postgres, zipkin con Docker Compose

En este apartado crearemos los siguientes contenedores apoyándonos de `docker compose`:

- c-mysql
- c-postgres
- c-mysql-zipkin
- c-zipkin

Debemos tener en cuenta que el `c-zipkin` dependerá de que `c-mysql-zipkin` se haya creado primero; esté ejecutándose
correctamente y tenga creada las tablas `zipkin_spans`, `zipkin_annotations`, `zipkin_dependencies` y se le hayan
agregado algunos índices.

Entonces, debemos crear previamente un archivo donde colocaremos el script sql que necesita la base de datos para
zipkin, de tal forma que cuando creamos el contenedor de esa base de datos, se cree en automático las tablas e índices.

En la raíz principal de este proyecto creamos el archivo `scheme-mysql-zipkin.sql` donde colocaremos todo el script que
usamos en el `capítulo 11`. Este script nos permitirá crear las tablas `zipkin_spans`, `zipkin_annotations`,
`zipkin_dependencies` y agregar índices.

````bash
microservices-project/scripts/zipkin/scheme-mysql-zipkin.sql
````

A continuación, en el archivo `compose.yml` agregaremos el siguiente contenido.

````yml
services:
  s-config-server:
    build:
      context: ./infrastructure/config-server
    image: config-server
    container_name: c-config-server
    restart: unless-stopped
    ports:
      - '8888:8888'
    networks:
      - microservices-project-net

  s-discovery-server:
    build:
      context: ./infrastructure/discovery-server
    image: discovery-server
    container_name: c-discovery-server
    restart: unless-stopped
    ports:
      - '8761:8761'
    networks:
      - microservices-project-net

  s-mysql:
    image: mysql:8.0.41-debian
    container_name: c-mysql
    restart: unless-stopped
    ports:
      - '3307:3306'
    environment:
      MYSQL_DATABASE: db_product_service
      MYSQL_ROOT_PASSWORD: magadiflo
      MYSQL_USER: root
      MYSQL_PASSWORD: magadiflo
    volumes:
      - mysql-data:/var/lib/mysql
    networks:
      - microservices-project-net
    healthcheck:
      test: [ "CMD-SHELL", "mysqladmin ping -h 127.0.0.1 -u$${MYSQL_USER} -p$${MYSQL_PASSWORD}" ]
      interval: 10s
      timeout: 5s
      retries: 5

  s-postgres:
    image: postgres:17-alpine
    container_name: c-postgres
    restart: unless-stopped
    ports:
      - '5433:5432'
    environment:
      POSTGRES_DB: db_user_service
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: magadiflo
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - microservices-project-net
    healthcheck:
      test: [ 'CMD-SHELL', 'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}' ]
      interval: 10s
      timeout: 5s
      retries: 5

  s-mysql-zipkin:
    image: mysql:8.0.41-debian
    container_name: c-mysql-zipkin
    restart: unless-stopped
    ports:
      - '3308:3306'
    environment:
      MYSQL_DATABASE: db_zipkin
      MYSQL_ROOT_PASSWORD: magadiflo
      MYSQL_USER: zipkin
      MYSQL_PASSWORD: zipkin
    volumes:
      - mysql-zipkin-data:/var/lib/mysql
      - ./scripts/zipkin:/docker-entrypoint-initdb.d
    networks:
      - microservices-project-net
    healthcheck:
      test: [ "CMD-SHELL", "mysqladmin ping -h 127.0.0.1 -u$${MYSQL_USER} -p$${MYSQL_PASSWORD}" ]
      interval: 10s
      timeout: 5s
      retries: 5

  s-zipkin:
    image: openzipkin/zipkin:3.5.1
    container_name: c-zipkin
    restart: unless-stopped
    ports:
      - '9411:9411'
    environment:
      STORAGE_TYPE: mysql
      MYSQL_HOST: s-mysql-zipkin
      MYSQL_DB: db_zipkin
      MYSQL_USER: zipkin
      MYSQL_PASS: zipkin
    networks:
      - microservices-project-net
    depends_on:
      s-mysql-zipkin:
        condition: service_healthy

volumes:
  mysql-data:
    name: mysql-data
  postgres-data:
    name: postgres-data
  mysql-zipkin-data:
    name: mysql-zipkin-data

networks:
  microservices-project-net:
    name: microservices-project-net
````

Del contenido anterior, solo hablaré de puntos importantes que considero necesario tener en cuenta.

- En el servicio `s-mysql-zipkin` estamos usando un `bind mount` de la siguiente manera:
    - `./scripts/zipkin:/docker-entrypoint-initdb.d`. El directorio `/scripts/zipkin` es de nuestra máquina local, en
      este directorio he colocado el script `scheme-mysql-zipkin.sql`. El directorio `/docker-entrypoint-initdb.d`, es
      un directorio dentro del contenedor de `mysql`.
    - Cuando creamos un `bind mount` entre un archivo o un directorio de la máquina local y un contenedor, el contenido
      de la máquina local reemplazará el contenido que exista en el archivo o directorio del contenedor.
    - Montaje en `docker-entrypoint-initdb.d`, este es el mecanismo que utiliza la imagen oficial de `MySQL` para
      ejecutar scripts `.sql` o `.sh` `solo la primera vez que se crea el volumen`. Es decir, los archivos `.sql` en esa
      carpeta se ejecutan automáticamente `la primera vez` que `MySQL` inicializa el volumen (cuando `/var/lib/mysql`
      está vacío).
    - Docker ejecuta automáticamente los scripts en `docker-entrypoint-initdb.d` solo si el volumen de datos de MySQL
      está vacío. Esto ocurre solamente la primera vez que el contenedor se inicia con un volumen vacío (es decir, sin
      el directorio `/var/lib/mysql` previamente poblado).
    - Después de eso, al reiniciar el contenedor o volver a levantar el `docker compose`, Docker detecta que el volumen
      ya
      contiene una base de datos inicializada y omite la ejecución de los scripts SQL.

  > **Inicialización de una nueva instancia**
  >
  > `Al iniciar un contenedor por primera vez`, se creará una nueva base de datos con el nombre especificado y se
  > inicializará con las variables de configuración proporcionadas. Además, ejecutará archivos con extensiones `.sh`,
  > `.sql` y `.sql.gz`, ubicados en `/docker-entrypoint-initdb.d`. Los archivos se ejecutarán en orden alfabético. Puede
  > completar fácilmente sus servicios MySQL montando un volcado de SQL en ese directorio y proporcionando imágenes
  > personalizadas con los datos aportados. Los archivos SQL se importarán por defecto a la base de datos especificada
  > por la variable `MYSQL_DATABASE`.

- En varios servicios estoy usando el `healthcheck`. Un `healthcheck` en Docker es una instrucción que define cómo
  Docker puede verificar si un contenedor está funcionando correctamente desde adentro (no solo si está encendido, sino
  si realmente está operativo).


- Un `healthcheck` permite que Docker ejecute un comando dentro del contenedor para comprobar su estado de salud a
  intervalos regulares. Si el comando falla varias veces seguidas, el contenedor se marca como unhealthy. Esto es útil
  para orquestadores o scripts que dependan de contenedores realmente listos para funcionar.


- En el servicio `s-mysql-zipkin` estoy usando un `healthcheck`. Aquí el `test` ejecuta el comando `mysqladmin ping`
  para verificar que el servicio `MySQL` esté disponible.
    - Se conecta a `127.0.0.1` (localhost dentro del contenedor).
    - Usa las variables `${MYSQL_USER}` y `${MYSQL_PASSWORD}` definidas en environment.
    - Si `MySQL` está respondiendo, el comando devuelve `mysqld is alive`.

Creamos los contenedores ejecutando el comando de docker compose.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project (feature/section-12)
$ docker compose up -d                                                                                                    
[+] Running 34/34                                                                                                         
 ✔ s-postgres Pulled                                                                                                      
   ✔ 83efd74bc97e Pull complete                                                                                           
   ✔ 215ba3ecdc26 Pull complete                                                                                           
   ✔ d8d8fb695a5a Pull complete                                                                                           
   ✔ c24c1ba610df Pull complete                                                                                           
   ✔ 8f4971a5dfe7 Pull complete                                                                                           
   ✔ 404c53e09e31 Pull complete                                                                                           
   ✔ 4f18957d9158 Pull complete                                                                                           
   ✔ 15ca4c67ed92 Pull complete                                                                                           
   ✔ 9e4acb9ca7d3 Pull complete                                                                                           
 ✔ s-mysql Pulled                                                                                                         
   ✔ 3d80c213d73c Pull complete                                                                                           
   ✔ dd34483b65e5 Pull complete                                                                                           
   ✔ e2d15768547d Pull complete                                                                                           
   ✔ 51f55966c0b7 Pull complete                                                                                           
   ✔ fcc101123521 Pull complete                                                                                           
   ✔ 156e4bd92e5c Pull complete                                                                                           
   ✔ eccb44e37d22 Pull complete                                                                                           
   ✔ 8a628cdd7ccc Pull complete                                                                                           
   ✔ cdc6bb1849f7 Pull complete                                                                                           
   ✔ f50c587f3059 Pull complete                                                                                           
   ✔ 80e697a2a38a Pull complete                                                                                           
   ✔ 5bac1e6ff6c3 Pull complete                                                                                           
 ✔ s-zipkin Pulled                                                                                                        
   ✔ 9024432539a6 Pull complete                                                                                           
   ✔ 4125d9c649e6 Pull complete                                                                                           
   ✔ a2b7dbaa785d Pull complete                                                                                           
   ✔ 0724a0c030ee Pull complete                                                                                           
   ✔ d08e7a152654 Pull complete                                                                                           
   ✔ a45115d6642f Pull complete                                                                                           
   ✔ f338522607a6 Pull complete                                                                                           
   ✔ 217376793de7 Pull complete                                                                                           
   ✔ c23605470ecb Pull complete                                                                                           
 ✔ s-mysql-zipkin Pulled                                                                                                  
[+] Running 9/9                                                                                                           
 ✔ Volume "mysql-zipkin-data"    Created                                                                                  
 ✔ Volume "mysql-data"           Created                                                                                  
 ✔ Volume "postgres-data"        Created                                                                                  
 ✔ Container c-mysql             Started                                                                                  
 ✔ Container c-mysql-zipkin      Healthy                                                                                  
 ✔ Container c-postgres          Started                                                                                  
 ✔ Container c-discovery-server  Running                                                                                  
 ✔ Container c-config-server     Running                                                                                  
 ✔ Container c-zipkin            Started                                                                                  
````

Verificamos las imágenes que nos ha creado docker compose.

````bash
$ docker image ls
REPOSITORY          TAG             IMAGE ID       CREATED        SIZE
config-server       latest          e0da0620b904   5 hours ago    366MB
discovery-server    latest          453e62cdacb5   5 hours ago    391MB
openzipkin/zipkin   3.5.1           bb570eb45c29   4 days ago     379MB
postgres            17-alpine       7062a2109c4b   2 months ago   398MB
mysql               8.0.41-debian   b2252987e0ec   3 months ago   812MB
````

Listamos los contenedores creados y vemos que todos se han ejecutado correctamente.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED              STATUS                        PORTS                               NAMES
fdb908163ffc   openzipkin/zipkin:3.5.1   "start-zipkin"           About a minute ago   Up About a minute (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
a4f9b3c617ca   postgres:17-alpine        "docker-entrypoint.s…"   2 minutes ago        Up About a minute             0.0.0.0:5433->5432/tcp              c-postgres
2826ade5012d   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 minutes ago        Up About a minute (healthy)   33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
1cbbe9b2c2f0   mysql:8.0.41-debian       "docker-entrypoint.s…"   2 minutes ago        Up About a minute             33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
f7c3d96ede73   discovery-server          "/__cacert_entrypoin…"   5 hours ago          Up 2 hours                    0.0.0.0:8761->8761/tcp              c-discovery-server
83599c43381b   config-server             "/__cacert_entrypoin…"   5 hours ago          Up 2 hours                    0.0.0.0:8888->8888/tcp              c-config-server
````

> Si revisamos la web de Eureka Server, Zipkin, hacemos pruebas al config-server, revisamos las bases de datos, veremos
> que todos están trabajando correctamente.

## Despliegue de servicios productos e items con docker compose

En este apartado vamos a agregar dos servicios adicionales (productos e items). Además, vamos a realizar modificaciones
a los servicios que son dependencia de estos nuevos servicios para que usen el `healthcheck`.

Entonces, a continuación se muestra la configuración realizada en este apartado.

````yml
services:
  s-config-server:
    build:
      context: ./infrastructure/config-server
    image: config-server
    container_name: c-config-server
    restart: unless-stopped
    ports:
      - '8888:8888'
    networks:
      - microservices-project-net
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:8888/actuator/health" ]
      interval: 10s
      timeout: 5s
      retries: 5

  s-discovery-server:
    build:
      context: ./infrastructure/discovery-server
    image: discovery-server
    container_name: c-discovery-server
    restart: unless-stopped
    ports:
      - '8761:8761'
    networks:
      - microservices-project-net
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:8761/actuator/health" ]
      interval: 10s
      timeout: 5s
      retries: 5

  s-zipkin:
    image: openzipkin/zipkin:3.5.1
    container_name: c-zipkin
    restart: unless-stopped
    ports:
      - '9411:9411'
    environment:
      STORAGE_TYPE: mysql
      MYSQL_HOST: s-mysql-zipkin
      MYSQL_DB: db_zipkin
      MYSQL_USER: zipkin
      MYSQL_PASS: zipkin
    networks:
      - microservices-project-net
    depends_on:
      s-mysql-zipkin:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:9411/health" ]
      interval: 10s
      timeout: 5s
      retries: 5

  s-product-service:
    build:
      context: ./business-domain/product-service
    image: product-service
    container_name: c-product-service
    restart: unless-stopped
    networks:
      - microservices-project-net
    depends_on:
      s-mysql:
        condition: service_healthy
      s-discovery-server:
        condition: service_healthy
      s-zipkin:
        condition: service_healthy

  s-item-service:
    build:
      context: ./business-domain/item-service
    image: item-service
    container_name: c-item-service
    restart: unless-stopped
    ports:
      - '8085:8085'
    networks:
      - microservices-project-net
    depends_on:
      s-config-server:
        condition: service_healthy
      s-discovery-server:
        condition: service_healthy
      s-zipkin:
        condition: service_healthy
      s-product-service:
        condition: service_started
````

En la configuración anterior he agregado al `s-config-server` y al `s-discovery-server` su `healthcheck` para que los
servicios que dependen de estos puedan saber que la aplicación se está ejecutando con buena salud.

Tomemos como ejemplo el `healthcheck` del `s-config-server`.

````yml
services:
  s-config-server:
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:8888/actuator/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
````

El valor definido dentro de `test`, significa que va a verificar silenciosamente si el endpoint
`http://localhost:8888/actuator/health` está accesible dentro del contenedor. Si lo está, el contenedor se considera
saludable.

**En detalle**

- `CMD`: Le dice a `Docker` que ejecute el siguiente comando en el contenedor.

- `wget`: Es una herramienta de línea de comandos que se usa para hacer solicitudes HTTP (como un navegador, pero en la
  terminal). Aquí se usa para consultar una URL.

- `--spider`: Le indica a `wget` que no descargue el contenido de la URL, sino que solo verifique si está disponible o
  responde. Es como "tocar la puerta" sin entrar.

- `-q`: Significa `quiet` (silencioso). Oculta toda la salida del comando, salvo los errores. Esto evita que el log del
  contenedor se llene de mensajes innecesarios.

- `http://localhost:8888/actuator/health`: Es la URL que `wget` va a verificar. Si responde con éxito (código HTTP 200),
  el `healthcheck` se considera exitoso. El endpoint `/actuator/health` es un típico endpoint de salud de
  `Spring Boot Actuator`.

**Nota**
> El endpoint `/actuator/health` se habilita automáticamente en `Spring Boot` cuando agregamos la dependencia
> `spring-boot-starter-actuator` al `pom.xml`.
>
> Por defecto, `Spring Boot` expone el endpoint `/actuator/health` sin necesidad de configuración adicional. Este
> endpoint no requiere autenticación (a menos que nosotros lo configuremos).
>
> Incluso sin definir nada en `application.yml`, el `/actuator/health` ya está disponible con un contenido básico (por
> ejemplo: `{"status": "UP"}`).

Ahora observemos las configuraciones del `s-item-service`. En esta configuración podemos observar que este servicio
depende de la salud de otros servicios como `s-config-server`, `s-discovery-server` y `s-zipkin-server`.

````yml
services:
  s-item-service:
    build:
      context: ./business-domain/item-service
    image: item-service
    container_name: c-item-service
    restart: unless-stopped
    ports:
      - '8085:8085'
    networks:
      - microservices-project-net
    depends_on:
      s-config-server:
        condition: service_healthy
      s-discovery-server:
        condition: service_healthy
      s-zipkin:
        condition: service_healthy
      s-product-service:
        condition: service_started
````

Con respecto a la dependencia del servicio `s-product-service` no podemos colocar la condición `service_healthy` como a
los demás, dado que el servicio `s-product-service` no tiene un puerto definido, es decir el puerto es aleatorio, eso lo
podemos saber dado que colocamos la configuración `server.port=0`. Entonces, al no saber previamente el puerto en el
que correrá la aplicación al interior del contenedor no podemos definir su `healthcheck` de la manera como hemos
definido los otros servicios como por ejemplo en el `s-config-server`, definimos en el `test` la siguiente url
al que le hacíamos petición para saber si está bien `http://localhost:8888/actuator/health`, esto lo hacíamos porque
de antemano sabíamos que su puerto sería el `8888`, pero en el caso del `s-product-service` el puerto se define de
manera dinámica.

Por lo tanto, como el `s-item-service` depende del `s-product-service`, lo único que haré será colocar en la condición
el valor `service_started`. Esto significa que `Docker Compose` solo iniciará `s-item-service` después de que
`s-product-service` haya sido iniciado, sin importar si está saludable o no. Es decir, no espera que el servicio
esté en un estado saludable, solo que el contenedor esté ejecutándose.

- Para `s-config-server`, `s-discovery-server`, y `s-zipkin`, estamos indicando que `s-item-service` solo debe iniciarse
  cuando esos servicios estén `saludables` (es decir, cuando los endpoints de salud de esos servicios respondan
  positivamente).

- Para `s-product-service`, solo estamos especificando que debe estar iniciado, es decir, el contenedor debe estar en
  ejecución, pero no necesariamente en un estado saludable.

Si ejecutamos nuestros servicios con todo lo que tenemos hasta el momento, veremos que todos se ejecutan sin problemas.

````bash
D:\programming\spring\01.udemy\02.andres_guzman\06.microservices_netflix_eureka\microservices-project (feature/section-12)
$ docker compose up -d                                                                                                    
[+] Running 8/8                                                                                                           
 ✔ Container c-mysql-zipkin      Healthy                                                                                  
 ✔ Container c-config-server     Healthy                                                                                  
 ✔ Container c-mysql             Healthy                                                                                  
 ✔ Container c-postgres          Running                                                                                  
 ✔ Container c-discovery-server  Healthy                                                                                  
 ✔ Container c-zipkin            Healthy                                                                                  
 ✔ Container c-product-service   Running                                                                                  
 ✔ Container c-item-service      Running                                                                                  
````

Listamos las imágenes generadas hasta ahora.

````bash
$ docker image ls
REPOSITORY          TAG             IMAGE ID       CREATED          SIZE
item-service        latest          5b48268f1a1e   41 minutes ago   378MB
product-service     latest          c091937f65eb   42 minutes ago   432MB
config-server       latest          a356852f4624   24 hours ago     366MB
discovery-server    latest          5b83d852c066   24 hours ago     391MB
openzipkin/zipkin   3.5.1           bb570eb45c29   4 days ago       379MB
postgres            17-alpine       7062a2109c4b   2 months ago     398MB
mysql               8.0.41-debian   b2252987e0ec   3 months ago     812MB
````

Listamos todos los contenedores generados hasta ahora.

````bash
$ docker container ls -a
CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS                    PORTS                               NAMES
26748b570f3f   item-service              "/__cacert_entrypoin…"   40 minutes ago   Up 39 minutes             0.0.0.0:8085->8085/tcp              c-item-service
5c6f3181c8c3   product-service           "/__cacert_entrypoin…"   40 minutes ago   Up 40 minutes                                                 c-product-service
f2a29335dbc2   config-server             "/__cacert_entrypoin…"   40 minutes ago   Up 40 minutes (healthy)   0.0.0.0:8888->8888/tcp              c-config-server
e32ed7f3153f   openzipkin/zipkin:3.5.1   "start-zipkin"           40 minutes ago   Up 40 minutes (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp    c-zipkin
de0a43d2c1e5   discovery-server          "/__cacert_entrypoin…"   40 minutes ago   Up 40 minutes (healthy)   0.0.0.0:8761->8761/tcp              c-discovery-server
a921eb5de50c   postgres:17-alpine        "docker-entrypoint.s…"   18 hours ago     Up 2 hours (healthy)      0.0.0.0:5433->5432/tcp              c-postgres
d0b8174eec7f   mysql:8.0.41-debian       "docker-entrypoint.s…"   18 hours ago     Up 2 hours (healthy)      33060/tcp, 0.0.0.0:3308->3306/tcp   c-mysql-zipkin
0974babfc235   mysql:8.0.41-debian       "docker-entrypoint.s…"   18 hours ago     Up 2 hours (healthy)      33060/tcp, 0.0.0.0:3307->3306/tcp   c-mysql
````

Verificamos que nuestro contenedor de items y productos estén funcionando.

````bash
$ curl -v http://localhost:8085/api/v1/items | jq
>
< HTTP/1.1 200
< Content-Type: application/json
< Transfer-Encoding: chunked
< Date: Fri, 02 May 2025 16:43:39 GMT
<
[
  {
    "product": {
      "id": 1,
      "name": "Panasonic",
      "price": 800.00,
      "createAt": "2025-05-02T21:04:40",
      "port": 45833
    },
    "quantity": 1,
    "total": 800.00
  },
  {...}
]
````
